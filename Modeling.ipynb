{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/v-artur/Golden_Oreos/blob/main/Modeling.ipynb)"
      ],
      "metadata": {
        "id": "-YwF9Hzt2euf"
      },
      "id": "-YwF9Hzt2euf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import files from drive"
      ],
      "metadata": {
        "id": "gVCNOaaoDSbO"
      },
      "id": "gVCNOaaoDSbO"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a2005cbc",
      "metadata": {
        "id": "a2005cbc",
        "outputId": "2053fee4-f1bc-406c-b14b-db8a9f18bd87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spectrogram reconstruction "
      ],
      "metadata": {
        "id": "QUch8ZTsGET3"
      },
      "id": "QUch8ZTsGET3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.) One person model"
      ],
      "metadata": {
        "id": "SFlhVaHrGjo-"
      },
      "id": "SFlhVaHrGjo-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for modeling and predicting"
      ],
      "metadata": {
        "id": "HPnNLUrkzd3J"
      },
      "id": "HPnNLUrkzd3J"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM, GRU, Conv1D, MaxPooling1D, Flatten\n",
        "import numpy as np\n",
        "\n",
        "# Bottleneck FC-DNN\n",
        "\n",
        "def create_ae_model(inputsize, outputsize):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(inputsize)))\n",
        "    model.add(tf.keras.layers.Dense(256, activation=\"relu\", kernel_initializer='HeNormal'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(64, activation=\"relu\", kernel_initializer='HeNormal'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(16, activation=\"relu\", kernel_initializer='HeNormal'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(4, activation=\"relu\", kernel_initializer='HeNormal'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(16, activation=\"relu\", kernel_initializer='HeNormal'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(outputsize))\n",
        "    return model\n",
        "\n",
        "\n",
        "# LSTM Network\n",
        "\n",
        "def create_LSTM_model(inputsize, outputsize):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(LSTM(units=100, dropout=0.2, input_shape=(inputsize,1)))\n",
        "  #model.add(LSTM(units=25, dropout=0.2, input_shape=(inputsize,1)))\n",
        "  model.add(Dense(outputsize))\n",
        "  return(model)\n",
        "\n",
        "# CNN network\n",
        "\n",
        "def create_cnn_model(inputsize, outputsize, rows_per_feature):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(Conv1D(10, 8, activation=\"relu\", input_shape=(rows_per_feature, inputsize)))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Conv1D(20, 8, activation=\"relu\"))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units=outputsize, activation='linear'))\n",
        "  return(model)\n",
        "\n",
        "# to make a 2D object out of the EEG featurevector with sliding window\n",
        "# windowsize is minimum 10 and multiple of 5.\n",
        "def cnnize(data, window):\n",
        "  data = np.asarray(data)\n",
        "  #the new array, the first coordinate is the number of samples,\n",
        "  #the second and the third are the new features\n",
        "  X = np.zeros((data.shape[0], int((data.shape[1]-window)/5)+1 , window))\n",
        "  for index, elem in enumerate(data):\n",
        "    # creating the feature-parts\n",
        "    new_elems = np.array([elem[i:i+window] for i in range(0, data.shape[1] - window + 1, 5)])\n",
        "    X[index] = new_elems\n",
        "  return X\n"
      ],
      "metadata": {
        "id": "Wk9elaV4zgv3"
      },
      "id": "Wk9elaV4zgv3",
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting seed\n",
        "tf.keras.utils.set_random_seed(1234)"
      ],
      "metadata": {
        "id": "xdpHUVGY8wv-"
      },
      "id": "xdpHUVGY8wv-",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizing the FC-DNN structure for one person\n",
        "\n"
      ],
      "metadata": {
        "id": "wZ5bO6U_yM5h"
      },
      "id": "wZ5bO6U_yM5h"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "data = np.load(r'/content/drive/MyDrive/DeepLearning/features/sub-01_feat.npy')\n",
        "spectrogram = np.load(r'/content/drive/MyDrive/DeepLearning/features/sub-01_spec.npy')\n",
        "\n",
        "#Inital parameters\n",
        "nfolds = 10\n",
        "kf = KFold(nfolds,shuffle=False)\n",
        "pca = PCA()\n",
        "numComps = 300\n",
        "explainedVariance = np.zeros(nfolds)\n",
        "val_split = 0.2\n",
        "\n",
        "\n",
        "#Initialize an empty spectrogram to save the reconstruction to\n",
        "rec_spec = np.zeros(spectrogram.shape)\n",
        "#Save the correlation coefficients for each fold\n",
        "rs = np.zeros((nfolds,spectrogram.shape[1]))\n",
        "for k,(train, test) in enumerate(kf.split(data)):\n",
        "          \n",
        "    #Train, validation and test data\n",
        "    X_train_temp = data[train,:]\n",
        "    y_train_temp = spectrogram[train,:]\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=val_split, random_state=0)\n",
        "    X_test = data[test,:]\n",
        "    y_test = spectrogram[test,:] # this one might not be needed\n",
        "    \n",
        "    #Normalization\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train[:] = scaler.transform(X_train)\n",
        "    X_val[:] = scaler.transform(X_val)\n",
        "    X_test[:] = scaler.transform(X_test)\n",
        "\n",
        "    #Fit PCA to training data\n",
        "    pca.fit(X_train)\n",
        "    #Get percentage of explained variance by selected components\n",
        "    explainedVariance[k] = np.sum(pca.explained_variance_ratio_[:numComps])\n",
        "    #Tranform data \n",
        "    X_train = np.dot(X_train, pca.components_[:numComps,:].T)\n",
        "    X_val = np.dot(X_val, pca.components_[:numComps,:].T)\n",
        "    X_test = np.dot(X_test, pca.components_[:numComps,:].T)\n",
        "\n",
        "    # Bottleneck model\n",
        "    early_stopping=EarlyStopping(patience=25, verbose=1, min_delta=1e-5)\n",
        "    checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "    model = create_ae_model(numComps, spectrogram.shape[1])\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
        "    model.fit(X_train, y_train, batch_size=64, \n",
        "              epochs=500, verbose=0, validation_data=(X_val, y_val), shuffle=True,\n",
        "              callbacks=[checkpointer, early_stopping])\n",
        "            \n",
        "    #predict with the Autoencoder\n",
        "    model.load_weights('weights1.hdf5')\n",
        "    rec_spec[test, :] = model.predict(X_test, verbose=0)\n",
        "\n",
        "    #Evaluate reconstruction of this fold\n",
        "    for specBin in range(spectrogram.shape[1]):\n",
        "         r, p = pearsonr(spectrogram[test, specBin], rec_spec[test, specBin])\n",
        "         rs[k,specBin] = r\n",
        "\n",
        "\n",
        "#Show evaluation result\n",
        "print('mean correlation', np.mean(rs))"
      ],
      "metadata": {
        "id": "ZHp3r8AIyUfI",
        "outputId": "a195712e-bd99-4709-ba03-377f574d8a54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "ZHp3r8AIyUfI",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 5.08743, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss improved from 5.08743 to 3.29884, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 3: val_loss improved from 3.29884 to 2.55712, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 4: val_loss improved from 2.55712 to 2.20429, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 5: val_loss improved from 2.20429 to 2.04845, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 6: val_loss improved from 2.04845 to 1.91544, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 7: val_loss improved from 1.91544 to 1.71705, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 8: val_loss improved from 1.71705 to 1.50765, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 9: val_loss improved from 1.50765 to 1.50368, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.50368\n",
            "\n",
            "Epoch 11: val_loss did not improve from 1.50368\n",
            "\n",
            "Epoch 12: val_loss improved from 1.50368 to 1.47402, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 13: val_loss did not improve from 1.47402\n",
            "\n",
            "Epoch 14: val_loss improved from 1.47402 to 1.46418, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 15: val_loss improved from 1.46418 to 1.38421, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 16: val_loss did not improve from 1.38421\n",
            "\n",
            "Epoch 17: val_loss did not improve from 1.38421\n",
            "\n",
            "Epoch 18: val_loss improved from 1.38421 to 1.19393, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 19: val_loss did not improve from 1.19393\n",
            "\n",
            "Epoch 20: val_loss did not improve from 1.19393\n",
            "\n",
            "Epoch 21: val_loss did not improve from 1.19393\n",
            "\n",
            "Epoch 22: val_loss improved from 1.19393 to 1.18906, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 23: val_loss did not improve from 1.18906\n",
            "\n",
            "Epoch 24: val_loss did not improve from 1.18906\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ab1b2aa19223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     model.fit(X_train, y_train, batch_size=64, \n\u001b[1;32m     57\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m               callbacks=[checkpointer, early_stopping])\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m#predict with the Autoencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizing the LSTM structure for one person"
      ],
      "metadata": {
        "id": "YuDJaqHQoNR3"
      },
      "id": "YuDJaqHQoNR3"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "data = np.load(r'/content/drive/MyDrive/DeepLearning/features/sub-01_feat.npy')\n",
        "spectrogram = np.load(r'/content/drive/MyDrive/DeepLearning/features/sub-01_spec.npy')\n",
        "\n",
        "#Inital parameters\n",
        "nfolds = 10\n",
        "kf = KFold(nfolds,shuffle=False)\n",
        "pca = PCA()\n",
        "numComps = 100\n",
        "explainedVariance = np.zeros(nfolds)\n",
        "val_split = 0.2\n",
        "\n",
        "\n",
        "#Initialize an empty spectrogram to save the reconstruction to\n",
        "rec_spec = np.zeros(spectrogram.shape)\n",
        "#Save the correlation coefficients for each fold\n",
        "rs = np.zeros((nfolds,spectrogram.shape[1]))\n",
        "for k,(train, test) in enumerate(kf.split(data)):         \n",
        "    #Train, validation and test data\n",
        "    X_train_temp = data[train,:]\n",
        "    y_train_temp = spectrogram[train,:]\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=val_split, random_state=0, shuffle=False)\n",
        "    X_test = data[test,:]\n",
        "    y_test = spectrogram[test,:] # this one might not be needed\n",
        "    \n",
        "    #Normalization\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train[:] = scaler.transform(X_train)\n",
        "    X_val[:] = scaler.transform(X_val)\n",
        "    X_test[:] = scaler.transform(X_test)\n",
        "\n",
        "    #Fit PCA to training data\n",
        "    pca.fit(X_train)\n",
        "    #Get percentage of explained variance by selected components\n",
        "    explainedVariance[k] = np.sum(pca.explained_variance_ratio_[:numComps])\n",
        "    #Tranform data \n",
        "    X_train = np.dot(X_train, pca.components_[:numComps,:].T)\n",
        "    X_val = np.dot(X_val, pca.components_[:numComps,:].T)\n",
        "    X_test = np.dot(X_test, pca.components_[:numComps,:].T)\n",
        "\n",
        "    # LSTM model\n",
        "    early_stopping=EarlyStopping(patience=25, verbose=1, min_delta=1e-5)\n",
        "    checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "    model = create_LSTM_model(numComps, spectrogram.shape[1])\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
        "    model.fit(X_train, y_train, batch_size=64, \n",
        "              epochs=150, verbose=0, validation_data=(X_val, y_val), shuffle=True,\n",
        "              callbacks=[checkpointer, early_stopping])\n",
        "            \n",
        "    #predict with the Autoencoder\n",
        "    model.load_weights('weights1.hdf5')\n",
        "    rec_spec[test, :] = model.predict(X_test, verbose=0)\n",
        "\n",
        "    #Evaluate reconstruction of this fold\n",
        "    for specBin in range(spectrogram.shape[1]):\n",
        "         r, p = pearsonr(spectrogram[test, specBin], rec_spec[test, specBin])\n",
        "         rs[k,specBin] = r\n",
        "\n",
        "\n",
        "#Show evaluation result\n",
        "print('mean correlation', np.mean(rs))"
      ],
      "metadata": {
        "id": "ifhJLOucoM3d",
        "outputId": "d715e9a2-5995-4f58-a1a4-307d446f68ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "ifhJLOucoM3d",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 3.52521, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss did not improve from 3.52521\n",
            "\n",
            "Epoch 3: val_loss improved from 3.52521 to 3.52369, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 4: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 5: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 6: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 7: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 8: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 9: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 10: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 11: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 12: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 13: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 14: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 15: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 16: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 17: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 18: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 19: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 20: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 21: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 22: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 23: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 24: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 25: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 26: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 27: val_loss did not improve from 3.52369\n",
            "\n",
            "Epoch 28: val_loss improved from 3.52369 to 3.51752, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 29: val_loss did not improve from 3.51752\n",
            "\n",
            "Epoch 30: val_loss did not improve from 3.51752\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c1e1ebfc9e84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m     model.fit(X_train, y_train, batch_size=64, \n\u001b[1;32m     56\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m               callbacks=[checkpointer, early_stopping])\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m#predict with the Autoencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizing the CNN structure for one person"
      ],
      "metadata": {
        "id": "JASkuUZGO6Di"
      },
      "id": "JASkuUZGO6Di"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "data = np.load(r'/content/drive/MyDrive/DeepLearning/features/sub-01_feat.npy')\n",
        "spectrogram = np.load(r'/content/drive/MyDrive/DeepLearning/features/sub-01_spec.npy')\n",
        "\n",
        "#Inital parameters\n",
        "nfolds = 10\n",
        "kf = KFold(nfolds,shuffle=False)\n",
        "pca = PCA()\n",
        "numComps = 100\n",
        "explainedVariance = np.zeros(nfolds)\n",
        "val_split = 0.2\n",
        "window = 10\n",
        "\n",
        "\n",
        "#Initialize an empty spectrogram to save the reconstruction to\n",
        "rec_spec = np.zeros(spectrogram.shape)\n",
        "#Save the correlation coefficients for each fold\n",
        "rs = np.zeros((nfolds,spectrogram.shape[1]))\n",
        "for k,(train, test) in enumerate(kf.split(data)):         \n",
        "    #Train, validation and test data\n",
        "    X_train_temp = data[train,:]\n",
        "    y_train_temp = spectrogram[train,:]\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=val_split, random_state=0, shuffle=False)\n",
        "    X_test = data[test,:]\n",
        "    y_test = spectrogram[test,:] # this one might not be needed\n",
        "    \n",
        "    #Normalization\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train[:] = scaler.transform(X_train)\n",
        "    X_val[:] = scaler.transform(X_val)\n",
        "    X_test[:] = scaler.transform(X_test)\n",
        "\n",
        "    #Fit PCA to training data\n",
        "    pca.fit(X_train)\n",
        "    #Get percentage of explained variance by selected components\n",
        "    explainedVariance[k] = np.sum(pca.explained_variance_ratio_[:numComps])\n",
        "    #Tranform data \n",
        "    X_train = np.dot(X_train, pca.components_[:numComps,:].T)\n",
        "    X_val = np.dot(X_val, pca.components_[:numComps,:].T)\n",
        "    X_test = np.dot(X_test, pca.components_[:numComps,:].T)\n",
        "\n",
        "    #reshaping the data\n",
        "    X_train = cnnize(X_train, window)\n",
        "    X_val = cnnize(X_val, window)\n",
        "    X_test = cnnize(X_test, window)\n",
        "\n",
        "    # CNN model\n",
        "    early_stopping=EarlyStopping(patience=25, verbose=1, min_delta=1e-5)\n",
        "    checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "    model = create_cnn_model(window, spectrogram.shape[1], int((numComps-window)/5)+1)\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
        "    model.fit(X_train, y_train, batch_size=64, \n",
        "              epochs=150, verbose=0, validation_data=(X_val, y_val), shuffle=True,\n",
        "              callbacks=[checkpointer, early_stopping])\n",
        "            \n",
        "    #predict with the Autoencoder\n",
        "    model.load_weights('weights1.hdf5')\n",
        "    rec_spec[test, :] = model.predict(X_test, verbose=0)\n",
        "\n",
        "    #Evaluate reconstruction of this fold\n",
        "    for specBin in range(spectrogram.shape[1]):\n",
        "         r, p = pearsonr(spectrogram[test, specBin], rec_spec[test, specBin])\n",
        "         rs[k,specBin] = r\n",
        "\n",
        "\n",
        "#Show evaluation result\n",
        "print('mean correlation', np.mean(rs))"
      ],
      "metadata": {
        "id": "JmcptaXXO1Sj",
        "outputId": "f4a13c7e-53fe-4fe4-da54-36a0044b3dfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "JmcptaXXO1Sj",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 3.97438, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss improved from 3.97438 to 3.24613, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 3: val_loss improved from 3.24613 to 2.91479, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 4: val_loss improved from 2.91479 to 2.73820, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 5: val_loss improved from 2.73820 to 2.68556, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 6: val_loss improved from 2.68556 to 2.61870, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 7: val_loss improved from 2.61870 to 2.61583, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 8: val_loss improved from 2.61583 to 2.56091, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 9: val_loss improved from 2.56091 to 2.54416, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.54416\n",
            "\n",
            "Epoch 11: val_loss improved from 2.54416 to 2.52061, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.52061\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.52061\n",
            "\n",
            "Epoch 14: val_loss improved from 2.52061 to 2.49894, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 27: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 28: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 29: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 30: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 31: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 32: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 33: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 34: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 35: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 36: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 37: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 38: val_loss did not improve from 2.49894\n",
            "\n",
            "Epoch 39: val_loss did not improve from 2.49894\n",
            "Epoch 39: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 4.15216, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss improved from 4.15216 to 3.35939, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 3: val_loss improved from 3.35939 to 3.05858, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 4: val_loss improved from 3.05858 to 2.91299, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 5: val_loss improved from 2.91299 to 2.82520, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 6: val_loss improved from 2.82520 to 2.79089, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 7: val_loss improved from 2.79089 to 2.73052, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 8: val_loss improved from 2.73052 to 2.68348, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 9: val_loss improved from 2.68348 to 2.66743, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.66743\n",
            "\n",
            "Epoch 11: val_loss improved from 2.66743 to 2.60995, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.60995\n",
            "\n",
            "Epoch 13: val_loss improved from 2.60995 to 2.58350, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 14: val_loss improved from 2.58350 to 2.57593, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 27: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 28: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 29: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 30: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 31: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 32: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 33: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 34: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 35: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 36: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 37: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 38: val_loss did not improve from 2.57593\n",
            "\n",
            "Epoch 39: val_loss did not improve from 2.57593\n",
            "Epoch 39: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 3.98229, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss improved from 3.98229 to 3.15815, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 3: val_loss improved from 3.15815 to 2.91448, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 4: val_loss improved from 2.91448 to 2.73677, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 5: val_loss improved from 2.73677 to 2.65214, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 6: val_loss did not improve from 2.65214\n",
            "\n",
            "Epoch 7: val_loss improved from 2.65214 to 2.62971, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 8: val_loss improved from 2.62971 to 2.56996, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 9: val_loss improved from 2.56996 to 2.55278, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.55278\n",
            "\n",
            "Epoch 11: val_loss improved from 2.55278 to 2.54986, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.54986\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.54986\n",
            "\n",
            "Epoch 14: val_loss improved from 2.54986 to 2.51773, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.51773\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.51773\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.51773\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.51773\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.51773\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.51773\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.51773\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.51773\n",
            "\n",
            "Epoch 23: val_loss improved from 2.51773 to 2.46863, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.46863\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.46863\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.46863\n",
            "\n",
            "Epoch 27: val_loss did not improve from 2.46863\n",
            "\n",
            "Epoch 28: val_loss improved from 2.46863 to 2.45564, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 29: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 30: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 31: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 32: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 33: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 34: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 35: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 36: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 37: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 38: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 39: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 40: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 41: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 42: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 43: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 44: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 45: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 46: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 47: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 48: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 49: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 50: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 51: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 52: val_loss did not improve from 2.45564\n",
            "\n",
            "Epoch 53: val_loss did not improve from 2.45564\n",
            "Epoch 53: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 3.88863, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss improved from 3.88863 to 3.24286, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 3: val_loss improved from 3.24286 to 3.07221, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 4: val_loss improved from 3.07221 to 2.91018, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 5: val_loss improved from 2.91018 to 2.88229, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 6: val_loss improved from 2.88229 to 2.81413, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 7: val_loss improved from 2.81413 to 2.71787, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.71787\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.71787\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.71787\n",
            "\n",
            "Epoch 11: val_loss did not improve from 2.71787\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.71787\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.71787\n",
            "\n",
            "Epoch 14: val_loss improved from 2.71787 to 2.66279, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 27: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 28: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 29: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 30: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 31: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 32: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 33: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 34: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 35: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 36: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 37: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 38: val_loss did not improve from 2.66279\n",
            "\n",
            "Epoch 39: val_loss did not improve from 2.66279\n",
            "Epoch 39: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 4.22505, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss improved from 4.22505 to 3.37011, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 3: val_loss improved from 3.37011 to 3.07946, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 4: val_loss improved from 3.07946 to 2.87086, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 5: val_loss improved from 2.87086 to 2.80716, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 6: val_loss improved from 2.80716 to 2.74117, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 7: val_loss improved from 2.74117 to 2.70147, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 8: val_loss improved from 2.70147 to 2.60794, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.60794\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.60794\n",
            "\n",
            "Epoch 11: val_loss did not improve from 2.60794\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.60794\n",
            "\n",
            "Epoch 13: val_loss improved from 2.60794 to 2.56331, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 14: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 27: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 28: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 29: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 30: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 31: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 32: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 33: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 34: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 35: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 36: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 37: val_loss did not improve from 2.56331\n",
            "\n",
            "Epoch 38: val_loss did not improve from 2.56331\n",
            "Epoch 38: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 3.75616, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss improved from 3.75616 to 3.10904, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 3: val_loss improved from 3.10904 to 2.91525, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 4: val_loss improved from 2.91525 to 2.73988, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 5: val_loss improved from 2.73988 to 2.69308, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 6: val_loss improved from 2.69308 to 2.64839, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 7: val_loss improved from 2.64839 to 2.63443, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 8: val_loss improved from 2.63443 to 2.58959, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.58959\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.58959\n",
            "\n",
            "Epoch 11: val_loss improved from 2.58959 to 2.58815, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 14: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 27: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 28: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 29: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 30: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 31: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 32: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 33: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 34: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 35: val_loss did not improve from 2.58815\n",
            "\n",
            "Epoch 36: val_loss did not improve from 2.58815\n",
            "Epoch 36: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 3.88190, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss improved from 3.88190 to 3.14012, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 3: val_loss improved from 3.14012 to 2.83914, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 4: val_loss improved from 2.83914 to 2.60024, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 5: val_loss improved from 2.60024 to 2.56546, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 6: val_loss improved from 2.56546 to 2.48519, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 7: val_loss improved from 2.48519 to 2.47108, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 8: val_loss improved from 2.47108 to 2.46691, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 9: val_loss improved from 2.46691 to 2.38786, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.38786\n",
            "\n",
            "Epoch 11: val_loss improved from 2.38786 to 2.38235, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 12: val_loss improved from 2.38235 to 2.32045, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 14: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 27: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 28: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 29: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 30: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 31: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 32: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 33: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 34: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 35: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 36: val_loss did not improve from 2.32045\n",
            "\n",
            "Epoch 37: val_loss did not improve from 2.32045\n",
            "Epoch 37: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 4.31478, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss improved from 4.31478 to 3.49203, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 3: val_loss improved from 3.49203 to 3.20313, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 4: val_loss improved from 3.20313 to 2.93356, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 5: val_loss improved from 2.93356 to 2.81403, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 6: val_loss improved from 2.81403 to 2.75159, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 7: val_loss improved from 2.75159 to 2.73542, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 8: val_loss improved from 2.73542 to 2.67143, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 9: val_loss improved from 2.67143 to 2.60515, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.60515\n",
            "\n",
            "Epoch 11: val_loss did not improve from 2.60515\n",
            "\n",
            "Epoch 12: val_loss improved from 2.60515 to 2.59701, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.59701\n",
            "\n",
            "Epoch 14: val_loss improved from 2.59701 to 2.58237, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 15: val_loss improved from 2.58237 to 2.57578, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.57578\n",
            "\n",
            "Epoch 17: val_loss improved from 2.57578 to 2.57061, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.57061\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.57061\n",
            "\n",
            "Epoch 20: val_loss improved from 2.57061 to 2.52110, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 27: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 28: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 29: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 30: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 31: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 32: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 33: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 34: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 35: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 36: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 37: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 38: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 39: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 40: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 41: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 42: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 43: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 44: val_loss did not improve from 2.52110\n",
            "\n",
            "Epoch 45: val_loss did not improve from 2.52110\n",
            "Epoch 45: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 4.57467, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss improved from 4.57467 to 3.69867, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 3: val_loss improved from 3.69867 to 3.40082, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 4: val_loss improved from 3.40082 to 3.20258, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 5: val_loss improved from 3.20258 to 3.08269, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 6: val_loss improved from 3.08269 to 2.97091, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 7: val_loss improved from 2.97091 to 2.94794, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 8: val_loss improved from 2.94794 to 2.89542, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 9: val_loss improved from 2.89542 to 2.87011, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.87011\n",
            "\n",
            "Epoch 11: val_loss did not improve from 2.87011\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.87011\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.87011\n",
            "\n",
            "Epoch 14: val_loss did not improve from 2.87011\n",
            "\n",
            "Epoch 15: val_loss improved from 2.87011 to 2.86579, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 27: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 28: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 29: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 30: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 31: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 32: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 33: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 34: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 35: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 36: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 37: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 38: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 39: val_loss did not improve from 2.86579\n",
            "\n",
            "Epoch 40: val_loss did not improve from 2.86579\n",
            "Epoch 40: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 4.09072, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss improved from 4.09072 to 3.24230, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 3: val_loss improved from 3.24230 to 2.93671, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 4: val_loss improved from 2.93671 to 2.73928, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 5: val_loss improved from 2.73928 to 2.59718, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 6: val_loss improved from 2.59718 to 2.53176, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 7: val_loss improved from 2.53176 to 2.46834, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 8: val_loss improved from 2.46834 to 2.41503, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.41503\n",
            "\n",
            "Epoch 10: val_loss improved from 2.41503 to 2.37471, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 11: val_loss improved from 2.37471 to 2.37046, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.37046\n",
            "\n",
            "Epoch 13: val_loss improved from 2.37046 to 2.36302, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 14: val_loss improved from 2.36302 to 2.35301, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.35301\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.35301\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.35301\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.35301\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.35301\n",
            "\n",
            "Epoch 20: val_loss improved from 2.35301 to 2.33220, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.33220\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.33220\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.33220\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.33220\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.33220\n",
            "\n",
            "Epoch 26: val_loss improved from 2.33220 to 2.33194, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 27: val_loss did not improve from 2.33194\n",
            "\n",
            "Epoch 28: val_loss did not improve from 2.33194\n",
            "\n",
            "Epoch 29: val_loss did not improve from 2.33194\n",
            "\n",
            "Epoch 30: val_loss improved from 2.33194 to 2.31788, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 31: val_loss did not improve from 2.31788\n",
            "\n",
            "Epoch 32: val_loss did not improve from 2.31788\n",
            "\n",
            "Epoch 33: val_loss did not improve from 2.31788\n",
            "\n",
            "Epoch 34: val_loss did not improve from 2.31788\n",
            "\n",
            "Epoch 35: val_loss did not improve from 2.31788\n",
            "\n",
            "Epoch 36: val_loss did not improve from 2.31788\n",
            "\n",
            "Epoch 37: val_loss improved from 2.31788 to 2.31421, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 38: val_loss did not improve from 2.31421\n",
            "\n",
            "Epoch 39: val_loss did not improve from 2.31421\n",
            "\n",
            "Epoch 40: val_loss did not improve from 2.31421\n",
            "\n",
            "Epoch 41: val_loss did not improve from 2.31421\n",
            "\n",
            "Epoch 42: val_loss did not improve from 2.31421\n",
            "\n",
            "Epoch 43: val_loss did not improve from 2.31421\n",
            "\n",
            "Epoch 44: val_loss improved from 2.31421 to 2.30122, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 45: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 46: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 47: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 48: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 49: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 50: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 51: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 52: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 53: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 54: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 55: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 56: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 57: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 58: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 59: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 60: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 61: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 62: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 63: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 64: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 65: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 66: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 67: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 68: val_loss did not improve from 2.30122\n",
            "\n",
            "Epoch 69: val_loss did not improve from 2.30122\n",
            "Epoch 69: early stopping\n",
            "mean correlation 0.48785714439235534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.) Trying it out for every subject"
      ],
      "metadata": {
        "id": "8puXSrNvPks0"
      },
      "id": "8puXSrNvPks0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c0796a",
      "metadata": {
        "id": "31c0796a",
        "outputId": "fe1e224e-354d-4f60-ed10-dfef7396dffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a44b39bb2b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m             model.fit(trainData, spectrogram[train, :], batch_size=64, \n\u001b[1;32m     93\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                       callbacks=[checkpointer, early_stopping])\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m#predict with the Autoencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wavfile\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#import reconstructWave as rW\n",
        "#import MelFilterBank as mel\n",
        "\n",
        "# #Optional function\n",
        "# def createAudio(spectrogram, audiosr=16000, winLength=0.05, frameshift=0.01):\n",
        "#     mfb = mel.MelFilterBank(int((audiosr*winLength)/2+1), spectrogram.shape[1], audiosr)\n",
        "#     nfolds = 10\n",
        "#     hop = int(spectrogram.shape[0]/nfolds)\n",
        "#     rec_audio = np.array([])\n",
        "#     for_reconstruction = mfb.fromLogMels(spectrogram)\n",
        "#     for w in range(0,spectrogram.shape[0],hop):\n",
        "#         spec = for_reconstruction[w:min(w+hop,for_reconstruction.shape[0]),:]\n",
        "#         rec = rW.reconstructWavFromSpectrogram(spec,spec.shape[0]*spec.shape[1],fftsize=int(audiosr*winLength),overlap=int(winLength/frameshift))\n",
        "#         rec_audio = np.append(rec_audio,rec)\n",
        "#     scaled = np.int16(rec_audio/np.max(np.abs(rec_audio)) * 32767)\n",
        "#     return scaled\n",
        "\n",
        "\n",
        "##### MODELING ########\n",
        "\n",
        "model = True\n",
        "\n",
        "scores= []\n",
        "\n",
        "if model == True:\n",
        "    feat_path = r'/content/drive/MyDrive/DeepLearning/features'\n",
        "    result_path = r'/content/drive/MyDrive/DeepLearning/features'\n",
        "    pts = ['sub-%02d'%i for i in range(1,11)]\n",
        "\n",
        "    winLength = 0.05\n",
        "    frameshift = 0.01\n",
        "    audiosr = 16000\n",
        "\n",
        "    nfolds = 15\n",
        "    kf = KFold(nfolds,shuffle=False)\n",
        "    pca = PCA()\n",
        "    numComps = 400\n",
        "    \n",
        "    #Initialize empty matrices for correlation results, randomized controls and amount of explained variance\n",
        "    allRes = np.zeros((len(pts),nfolds,23))\n",
        "    explainedVariance = np.zeros((len(pts),nfolds))\n",
        "    numRands = 1000\n",
        "    randomControl = np.zeros((len(pts),numRands, 23))\n",
        "\n",
        "    for pNr, pt in enumerate(pts):\n",
        "        \n",
        "        \n",
        "        #Load the data\n",
        "        #Dimensions of these data vary depending on the subject\n",
        "        spectrogram = np.load(os.path.join(feat_path,f'{pt}_spec.npy'))  \n",
        "        data = np.load(os.path.join(feat_path,f'{pt}_feat.npy'))\n",
        "        labels = np.load(os.path.join(feat_path,f'{pt}_procWords.npy'))\n",
        "        featName = np.load(os.path.join(feat_path,f'{pt}_feat_names.npy'))\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        #Initialize an empty spectrogram to save the reconstruction to\n",
        "        rec_spec = np.zeros(spectrogram.shape)\n",
        "        #Save the correlation coefficients for each fold\n",
        "        rs = np.zeros((nfolds,spectrogram.shape[1]))\n",
        "        for k,(train, test) in enumerate(kf.split(data)):\n",
        "          \n",
        "            #Normalization\n",
        "            mu=np.mean(data[train,:],axis=0)\n",
        "            std=np.std(data[train,:],axis=0)\n",
        "            trainData=(data[train,:]-mu)/std\n",
        "            testData=(data[test,:]-mu)/std\n",
        "\n",
        "            #Fit PCA to training data\n",
        "            pca.fit(trainData)\n",
        "            #Get percentage of explained variance by selected components\n",
        "            explainedVariance[pNr,k] = np.sum(pca.explained_variance_ratio_[:numComps])\n",
        "            #Tranform data into component space\n",
        "            trainData = np.dot(trainData, pca.components_[:numComps,:].T)\n",
        "            testData = np.dot(testData, pca.components_[:numComps,:].T)\n",
        "\n",
        "            early_stopping=EarlyStopping(patience=25, verbose=0, min_delta=1e-5)\n",
        "            checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=0)\n",
        "\n",
        "            #Autoencoder\n",
        "            model = create_ae_model(trainData.shape[1], spectrogram.shape[1])\n",
        "            model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
        "            model.fit(trainData, spectrogram[train, :], batch_size=64, \n",
        "                      epochs=500, verbose=0, validation_split=1/9, shuffle=True,\n",
        "                      callbacks=[checkpointer, early_stopping])\n",
        "            \n",
        "            #predict with the Autoencoder\n",
        "            rec_spec[test, :] = model.predict(testData, verbose=0)\n",
        "\n",
        "            #Evaluate reconstruction of this fold\n",
        "            for specBin in range(spectrogram.shape[1]):\n",
        "                if np.any(np.isnan(rec_spec)):\n",
        "                    print('%s has %d broken samples in reconstruction' % (pt, np.sum(np.isnan(rec_spec))))\n",
        "                r, p = pearsonr(spectrogram[test, specBin], rec_spec[test, specBin])\n",
        "                rs[k,specBin] = r\n",
        "\n",
        "        #Show evaluation result\n",
        "        print('%s has mean correlation of %f' % (pt, np.mean(rs)))\n",
        "        allRes[pNr,:,:]=rs\n",
        "        scores.append(np.mean(rs))\n",
        "\n",
        "#         #Estimate random baseline\n",
        "#         for randRound in range(numRands):\n",
        "#             #Choose a random splitting point at least 10% of the dataset size away\n",
        "#             splitPoint = np.random.choice(np.arange(int(spectrogram.shape[0]*0.1),int(spectrogram.shape[0]*0.9)))\n",
        "#             #Swap the dataset on the splitting point \n",
        "#             shuffled = np.concatenate((spectrogram[splitPoint:,:],spectrogram[:splitPoint,:]))\n",
        "#             #Calculate the correlations\n",
        "#             for specBin in range(spectrogram.shape[1]):\n",
        "#                 if np.any(np.isnan(rec_spec)):\n",
        "#                     print('%s has %d broken samples in reconstruction' % (pt, np.sum(np.isnan(rec_spec))))\n",
        "#                 r, p = pearsonr(spectrogram[:,specBin], shuffled[:,specBin])\n",
        "#                 randomControl[pNr, randRound,specBin]=r\n",
        "\n",
        "\n",
        "#         #Save reconstructed spectrogram\n",
        "#         os.makedirs(os.path.join(result_path), exist_ok=True)\n",
        "#         np.save(os.path.join(result_path,f'{pt}_predicted_spec.npy'), rec_spec)\n",
        "        \n",
        "#         #Synthesize waveform from spectrogram using Griffin-Lim\n",
        "#         reconstructedWav = createAudio(rec_spec,audiosr=audiosr,winLength=winLength,frameshift=frameshift)\n",
        "#         wavfile.write(os.path.join(result_path,f'{pt}_predicted.wav'),int(audiosr),reconstructedWav)\n",
        "\n",
        "#         #For comparison synthesize the original spectrogram with Griffin-Lim\n",
        "#         origWav = createAudio(spectrogram,audiosr=audiosr,winLength=winLength,frameshift=frameshift)\n",
        "#         wavfile.write(os.path.join(result_path,f'{pt}_orig_synthesized.wav'),int(audiosr),origWav)\n",
        "\n",
        "#     #Save results in numpy arrays          \n",
        "#     np.save(os.path.join(result_path,'linearResults.npy'),allRes)\n",
        "#     np.save(os.path.join(result_path,'randomResults.npy'),randomControl)\n",
        "#     np.save(os.path.join(result_path,'explainedVariance.npy'),explainedVariance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a6e6b40",
      "metadata": {
        "id": "0a6e6b40",
        "outputId": "4e3b10f6-a443-47ac-e2b6-c6eba396472d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5629875535958715,\n",
              " 0.615317710223286,\n",
              " 0.8574981748428402,\n",
              " 0.816490556136459,\n",
              " 0.5712122543230624,\n",
              " 0.8819938942481294,\n",
              " 0.7771201475020989,\n",
              " 0.6878816278290169,\n",
              " 0.7386449014724298,\n",
              " 0.7146618900721041]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R6FcMJWfW_99"
      },
      "id": "R6FcMJWfW_99",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}