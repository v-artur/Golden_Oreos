{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOQsB4UBUV6GWW8e1IotOOv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v-artur/Golden_Oreos/blob/main/speaker_indep_dim_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtaining the data"
      ],
      "metadata": {
        "id": "QRznZ-paPxRN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3F36vS9-11r",
        "outputId": "95f7bdc9-c2ee-466d-a97f-83ba8ed01e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-09 23:19:16--  https://docs.google.com/uc?export=download&confirm=t&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.79.138, 173.194.79.139, 173.194.79.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.79.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/d1dp7p4k7u5vc6o49g18ces9nrl0bppm/1670627925000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=fdc33b18-48bf-4c73-9a2b-5926e754534b [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-09 23:19:17--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/d1dp7p4k7u5vc6o49g18ces9nrl0bppm/1670627925000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=fdc33b18-48bf-4c73-9a2b-5926e754534b\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 142.251.18.132, 2a00:1450:4013:c18::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|142.251.18.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2141405093 (2.0G) [application/x-zip-compressed]\n",
            "Saving to: ‘features.zip’\n",
            "\n",
            "features.zip        100%[===================>]   1.99G   237MB/s    in 7.9s    \n",
            "\n",
            "2022-12-09 23:19:25 (259 MB/s) - ‘features.zip’ saved [2141405093/2141405093]\n",
            "\n",
            "--2022-12-09 23:19:26--  https://docs.google.com/uc?export=download&confirm=&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.79.138, 173.194.79.139, 173.194.79.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.79.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0li0atm65qska9btq368hvhvtmmn6j5k/1670627925000/17895932938140350971/*/1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80?e=download&uuid=3d50f60e-913e-46d1-9b70-42e01f37baf0 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-09 23:19:27--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0li0atm65qska9btq368hvhvtmmn6j5k/1670627925000/17895932938140350971/*/1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80?e=download&uuid=3d50f60e-913e-46d1-9b70-42e01f37baf0\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 142.251.18.132, 2a00:1450:4013:c18::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|142.251.18.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9238 (9.0K) [application/x-zip-compressed]\n",
            "Saving to: ‘subject_channels.zip’\n",
            "\n",
            "subject_channels.zi 100%[===================>]   9.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-09 23:19:27 (95.5 MB/s) - ‘subject_channels.zip’ saved [9238/9238]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#features\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\" -O features.zip && rm -rf /tmp/cookies.txt\n",
        "#original electrode names\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80\" -O subject_channels.zip && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# Data extraction\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/features.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/features\")\n",
        "zip_ref.close()\n",
        "\n",
        "# Electrode name extraction\n",
        "zip_ref = zipfile.ZipFile(\"/content/subject_channels.zip\", 'r')\n",
        "zip_ref.extractall(\"/content\")\n",
        "zip_ref.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Preparations and needed functions</h3>\n",
        "\n"
      ],
      "metadata": {
        "id": "QdxGeq3EP2hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our plan is to transform every feature vector into a larger dimensional feature vector,  which contains all the different electrode names across all the subjects."
      ],
      "metadata": {
        "id": "WgUblU51bfZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Getting the different electrode names\n",
        "all_electrodes = set()\n",
        "\n",
        "feat_path = \"/content/features\"\n",
        "\n",
        "for subject in ['01','02','03','04','05','06','07','08','09','10']:\n",
        "  elecs = np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy'))\n",
        "  elecs = set(elecs)\n",
        "  all_electrodes = all_electrodes.union(elecs)\n",
        "\n",
        "# We will use this list's indexes to correspond to the feature matrices gained from the subjects\n",
        "all_electrodes = list(all_electrodes) \n",
        "\n",
        "print('Number of different electrodes:', len(all_electrodes))"
      ],
      "metadata": {
        "id": "annt8G29nDNS",
        "outputId": "43ef45d5-48f0-4e43-bb25-3bf536596301",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of different electrodes: 4860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for making every feature vector into a 4860 dimensional vector\n",
        "\n",
        "def dim_adjust(data, feature_names):\n",
        "  #create a new matrix with zeros, insert the values into the columns\n",
        "  #which correspond to the subject's feature names, then concatenate them\n",
        "\n",
        "  new_matrix = np.zeros((data.shape[0],len(all_electrodes)))\n",
        "  for column in range(data.shape[1]):\n",
        "    insert_index = all_electrodes.index(feature_names[column])\n",
        "    new_matrix[:,insert_index] = data[:,column]\n",
        "    \n",
        "  return new_matrix\n"
      ],
      "metadata": {
        "id": "d8o2edSn_CkY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making the iterated test, validation and test sets"
      ],
      "metadata": {
        "id": "bwJu3UnzP8ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate the train, val and test features and mel spectrograms\n",
        "\n",
        "def generate_features_and_spec():\n",
        "  #Base arrays\n",
        "  train_feat = np.empty((0, len(all_electrodes)))\n",
        "  val_feat = np.empty((0, len(all_electrodes)))\n",
        "  test_feat = np.empty((0, len(all_electrodes)))\n",
        "\n",
        "  train_spec = np.empty((0, 23))\n",
        "  val_spec = np.empty((0, 23))\n",
        "  test_spec = np.empty((0, 23))\n",
        "\n",
        "  for index, subject in enumerate(['01','02','03','04','05','06','07','08','09','10']):\n",
        "    #loading the features, feature names and mel spectrogram of the subject\n",
        "    data = np.load(os.path.join(feat_path,f'sub-{subject}_feat.npy'))\n",
        "    feature_names = np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy'))\n",
        "    spectrogram = np.load(os.path.join(feat_path,f'sub-{subject}_spec.npy'))\n",
        "\n",
        "    #splittig the features and the labels into 5 parts\n",
        "    feat_splits = np.array_split(data, 5)\n",
        "    spec_splits = np.array_split(spectrogram, 5)\n",
        "\n",
        "    #making the train, val or test arrays for the subject using a 60-20-20 ratio\n",
        "    #because of the \"index\" changing, the splitting position will iterate with each subject as well\n",
        "    subject_train_feat = np.vstack((feat_splits[index % 5],feat_splits[(index+1) % 5],feat_splits[(index+2) % 5]))\n",
        "    subject_train_spec = np.vstack((spec_splits[index % 5],spec_splits[(index+1) % 5],spec_splits[(index+2) % 5]))\n",
        "\n",
        "    subject_val_feat = feat_splits[(index+3) % 5]\n",
        "    subject_val_spec = spec_splits[(index+3) % 5]\n",
        "\n",
        "    subject_test_feat = feat_splits[(index+4) % 5]\n",
        "    subject_test_spec = spec_splits[(index+4) % 5]\n",
        "\n",
        "    #concatenating the dimensionality-adjusted features with the pre-existing feature set\n",
        "    train_feat = np.concatenate((train_feat, dim_adjust(subject_train_feat, feature_names)))\n",
        "    val_feat = np.concatenate((val_feat, dim_adjust(subject_val_feat, feature_names)))\n",
        "    test_feat = np.concatenate((test_feat, dim_adjust(subject_test_feat, feature_names)))\n",
        "\n",
        "    #concatenating the appropriate arrays with the pre-existing train, val or test labels\n",
        "    train_spec = np.concatenate((train_spec, subject_train_spec))\n",
        "    val_spec = np.concatenate((val_spec, subject_val_spec))\n",
        "    test_spec = np.concatenate((test_spec, subject_test_spec))\n",
        "\n",
        "  return train_feat, train_spec, val_feat, val_spec, test_feat, test_spec \n",
        "\n",
        "# Generating the data\n",
        "train_feat, train_spec, val_feat, val_spec, test_feat, test_spec = generate_features_and_spec()"
      ],
      "metadata": {
        "id": "3sBGVYAoc7LA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Scaling the data \n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_feat)\n",
        "train_feat = scaler.transform(train_feat)\n",
        "val_feat = scaler.transform(val_feat)\n",
        "test_feat = scaler.transform(test_feat)"
      ],
      "metadata": {
        "id": "JC2LF6YOfflo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality reduction"
      ],
      "metadata": {
        "id": "MRV6pzVbmNWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Option 1: Tuned AutoEncoder</h3>"
      ],
      "metadata": {
        "id": "NV5SeqERySxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "dVwyhAYq-4O9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the data generator\n",
        "from tensorflow.keras.utils import Sequence, set_random_seed\n",
        "\n",
        "set_random_seed(1234)\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    # Initialization\n",
        "    def __init__(self, data, batch_size=32, dim=len(all_electrodes), shuffle=True):\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.data = data\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Shows the number of batches per epoch\n",
        "        return int(np.floor(self.data.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate one batch of data\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        batch = np.empty((self.batch_size, self.dim))\n",
        "        for index1, elem in enumerate(indexes):\n",
        "          batch[index1] = self.data[index1]\n",
        "\n",
        "        return batch, batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updating the index after each epoch\n",
        "        self.indexes = np.arange(self.data.shape[0])\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "# Creating the generators for training\n",
        "# According to our early experiments, batch sizes of 256 or 512 proved to be the best,\n",
        "# but to shorten the amount time for hyperparameter tuning, we set it for 512\n",
        "train_gen = DataGenerator(train_feat, 512)\n",
        "val_gen = DataGenerator(val_feat, 512)\n",
        "test_gen = DataGenerator(test_feat, 512)"
      ],
      "metadata": {
        "id": "xQ8U-ZLCCYTL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "import keras_tuner as kt\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# Creating the hyperparameter tuner function\n",
        "# The AE has 5 Dense layers besides the output, and the 3rd one is the bottleneck layer\n",
        "def create_ae_optimal(hp):\n",
        "  input = Input(shape=(4860))\n",
        "  \n",
        "  # Layer 1\n",
        "  hp_units_1 = hp.Int('units_1', min_value=1500, max_value=2500, step=250)\n",
        "  encoded = Dense(units=hp_units_1, activation=\"relu\", kernel_initializer='HeNormal')(input)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_1', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(encoded)\n",
        "\n",
        "  # Layer 2\n",
        "  hp_units_2 = hp.Int('units_2', min_value=750, max_value=1250, step=125)\n",
        "  encoded = Dense(units=hp_units_2, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_2', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(encoded)\n",
        "\n",
        "  # Layer 3\n",
        "  hp_units_3 = hp.Int('units_3', min_value=350, max_value=650, step=50)\n",
        "  encoded = Dense(units=hp_units_3, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_3', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(encoded)\n",
        "\n",
        "  # Layer 4\n",
        "  hp_units_4 = hp.Int('units_4', min_value=750, max_value=1250, step=125)\n",
        "  decoded = Dense(units=hp_units_4, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  decoded = Dropout(rate=hp.Float('dropout_4', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(decoded)\n",
        "\n",
        "  # Layer 5\n",
        "  hp_units_5 = hp.Int('units_5', min_value=1500, max_value=2500, step=250)\n",
        "  decoded = Dense(units=hp_units_5, activation=\"relu\", kernel_initializer='HeNormal')(decoded)\n",
        "  decoded = Dropout(rate=hp.Float('dropout_5', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(decoded)\n",
        "\n",
        "  # Output layer\n",
        "  output = Dense(4860, activation='linear')(decoded)\n",
        "\n",
        "  # Optimizer parameters\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model = Model(input, output)\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss='mse',\n",
        "                metrics=['mse'])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "R_ZSI31I-4zT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the tuner\n",
        "tuner = kt.Hyperband(create_ae_optimal,\n",
        "                     objective='val_loss',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='/content/ae_opt',\n",
        "                     project_name='ae_opt1')"
      ],
      "metadata": {
        "id": "kdUDep51B7_T"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "#note: takes about 20 minutes to optimize\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=10)\n",
        "tuner.search(train_gen, epochs=100, validation_data=val_gen, verbose=0, shuffle=True, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "ISWZG5BNCJ7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4cc8b9-c5e1-4a65-dedf-5a110ef398d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0177s vs `on_train_batch_end` time: 0.0179s). Check your callbacks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting and retraining the model with the best params\n",
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "early_stopping=EarlyStopping(patience=10, verbose=1, min_delta=1e-5)\n",
        "checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "hypermodel.fit(train_gen, epochs=100, verbose=1, validation_data=val_gen, callbacks=[checkpointer, early_stopping])"
      ],
      "metadata": {
        "id": "_lpNSFOpDDJj",
        "outputId": "7ba899b1-9fd0-4a1f-eea6-424a2ac8cbca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.5572 - mse: 0.5572\n",
            "Epoch 1: val_loss improved from inf to 0.46640, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.5566 - mse: 0.5566 - val_loss: 0.4664 - val_mse: 0.4664\n",
            "Epoch 2/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2705 - mse: 0.2705\n",
            "Epoch 2: val_loss improved from 0.46640 to 0.30801, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2703 - mse: 0.2703 - val_loss: 0.3080 - val_mse: 0.3080\n",
            "Epoch 3/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1834 - mse: 0.1834\n",
            "Epoch 3: val_loss improved from 0.30801 to 0.28483, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.1834 - mse: 0.1834 - val_loss: 0.2848 - val_mse: 0.2848\n",
            "Epoch 4/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1244 - mse: 0.1244\n",
            "Epoch 4: val_loss improved from 0.28483 to 0.27579, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.1244 - mse: 0.1244 - val_loss: 0.2758 - val_mse: 0.2758\n",
            "Epoch 5/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0950 - mse: 0.0950\n",
            "Epoch 5: val_loss improved from 0.27579 to 0.26971, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.0950 - mse: 0.0950 - val_loss: 0.2697 - val_mse: 0.2697\n",
            "Epoch 6/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0792 - mse: 0.0792\n",
            "Epoch 6: val_loss did not improve from 0.26971\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.0792 - mse: 0.0792 - val_loss: 0.2754 - val_mse: 0.2754\n",
            "Epoch 7/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0696 - mse: 0.0696\n",
            "Epoch 7: val_loss did not improve from 0.26971\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.0696 - mse: 0.0696 - val_loss: 0.2747 - val_mse: 0.2747\n",
            "Epoch 8/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0623 - mse: 0.0623\n",
            "Epoch 8: val_loss did not improve from 0.26971\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.0623 - mse: 0.0623 - val_loss: 0.2860 - val_mse: 0.2860\n",
            "Epoch 9/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0568 - mse: 0.0568\n",
            "Epoch 9: val_loss did not improve from 0.26971\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0568 - mse: 0.0568 - val_loss: 0.2908 - val_mse: 0.2908\n",
            "Epoch 10/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0523 - mse: 0.0523\n",
            "Epoch 10: val_loss did not improve from 0.26971\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.0523 - mse: 0.0523 - val_loss: 0.3031 - val_mse: 0.3031\n",
            "Epoch 11/100\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0483 - mse: 0.0483\n",
            "Epoch 11: val_loss did not improve from 0.26971\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.0483 - mse: 0.0483 - val_loss: 0.3050 - val_mse: 0.3050\n",
            "Epoch 12/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0454 - mse: 0.0454\n",
            "Epoch 12: val_loss did not improve from 0.26971\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.0454 - mse: 0.0454 - val_loss: 0.3107 - val_mse: 0.3107\n",
            "Epoch 13/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 13: val_loss did not improve from 0.26971\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.3238 - val_mse: 0.3238\n",
            "Epoch 14/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0416 - mse: 0.0416\n",
            "Epoch 14: val_loss did not improve from 0.26971\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.3160 - val_mse: 0.3160\n",
            "Epoch 15/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 15: val_loss did not improve from 0.26971\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.3794 - val_mse: 0.3794\n",
            "Epoch 15: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb490079040>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the performance on the test set\n",
        "hypermodel.load_weights('weights1.hdf5')\n",
        "hypermodel.evaluate(test_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOq0ZdtfBjCD",
        "outputId": "36a1862c-fce1-424e-d01b-6106b29b43d1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "116/116 [==============================] - 2s 15ms/step - loss: 0.2760 - mse: 0.2760\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.276035338640213, 0.276035338640213]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading back the best weights and checking the layers\n",
        "hypermodel.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QWuY1lchrS4",
        "outputId": "0d9dc897-c64c-4d6f-9179-3302722e682d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4860)]            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 2000)              9722000   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 2000)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1000)              2001000   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 550)               550550    \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 550)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1250)              688750    \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 1250)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 2000)              2502000   \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 2000)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 4860)              9724860   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,189,160\n",
            "Trainable params: 25,189,160\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keeping only the encoder\n",
        "model2 = Model(inputs=hypermodel.input, outputs=hypermodel.layers[-7].output)\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrTwZhiyhv8Q",
        "outputId": "8a503799-e2f4-4861-90ec-bda0af932127"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4860)]            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 2000)              9722000   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 2000)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1000)              2001000   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 550)               550550    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,273,550\n",
            "Trainable params: 12,273,550\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sizes of the new arrays\n",
        "print('Train:', train_feat.shape) \n",
        "print('Validation:', val_feat.shape)\n",
        "print('Test:', test_feat.shape)"
      ],
      "metadata": {
        "id": "OYLjwKunQi-J",
        "outputId": "b331f388-009b-4ef9-cc69-e77819636df2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (179011, 4860)\n",
            "Validation: (59672, 4860)\n",
            "Test: (59672, 4860)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to redefine the generators for the predictions in order to retain every feature vector in every set, otherwise some of them would be left out because of the batch size.\n",
        "\n",
        "Since the memory can't fit all the data, and using 1 as batch size would be slow, we need to use prime factorization to determine the appropriate batch sizes.\n",
        "\n",
        "The factorizations are:\n",
        "- 179011 = 7 x 107 x 239\n",
        "- 59672 = 2 * 2 * 2 * 7459  "
      ],
      "metadata": {
        "id": "j25X21GObqbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = DataGenerator(train_feat, 107*239)\n",
        "val_gen = DataGenerator(val_feat, 7459)\n",
        "test_gen = DataGenerator(test_feat, 7459)\n",
        "\n",
        "# Generating the new data using the output of the bottleneck layer\n",
        "train_new = model2.predict(train_gen)\n",
        "val_new = model2.predict(val_gen)\n",
        "test_new = model2.predict(test_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqrNk9pKiiHa",
        "outputId": "1023fbf8-be12-4cd1-8b58-02c73ca7f02c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 5s 760ms/step\n",
            "8/8 [==============================] - 2s 197ms/step\n",
            "8/8 [==============================] - 2s 192ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if the new sets really has the same amount of vectors\n",
        "print(train_new.shape[0] == train_feat.shape[0])\n",
        "print(val_new.shape[0] == val_feat.shape[0])\n",
        "print(test_new.shape[0] == test_feat.shape[0])"
      ],
      "metadata": {
        "id": "ODtUOd0KSBE0",
        "outputId": "22afc0dd-64fe-4274-b22e-6374bd6291ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the feature-label set pairs as pickle files onto Google Drive\n",
        "import pickle\n",
        "\n",
        "with open('train.pkl', 'wb') as f:  \n",
        "    pickle.dump([train_new, train_spec], f)\n",
        "\n",
        "with open('val.pkl', 'wb') as f:  \n",
        "    pickle.dump([val_new, val_spec], f)\n",
        "\n",
        "with open('test.pkl', 'wb') as f:  \n",
        "    pickle.dump([test_new, test_spec], f)"
      ],
      "metadata": {
        "id": "UWTKZceejPgS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp train.pkl drive/MyDrive/DeepLearning/train.pkl\n",
        "!cp val.pkl drive/MyDrive/DeepLearning/val.pkl\n",
        "!cp test.pkl drive/MyDrive/DeepLearning/test.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lhk7ec0pyg_",
        "outputId": "0d09cde6-907c-46c9-d0fd-18d2eae21ec2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Option 2: Incremental PCA</h3>"
      ],
      "metadata": {
        "id": "q9cD1KX0ajBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "n_comp = 250\n",
        "\n",
        "# note: takes about 10 minutes to run\n",
        "pca = IncrementalPCA(n_components=n_comp, batch_size=1024)\n",
        "pca.fit(train_feat)\n",
        "train_feat = pca.transform(train_feat)\n",
        "val_feat = pca.transform(val_feat)\n",
        "test_feat = pca.transform(test_feat)"
      ],
      "metadata": {
        "id": "AARr0wkEaqfR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the new data\n",
        "\n",
        "with open('train_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([train_feat, train_spec], f)\n",
        "\n",
        "with open('val_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([val_feat, val_spec], f)\n",
        "\n",
        "with open('test_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([test_feat, test_spec], f)\n",
        "\n",
        "!cp train_v2.pkl drive/MyDrive/DeepLearning/train_v2.pkl\n",
        "!cp val_v2.pkl drive/MyDrive/DeepLearning/val_v2.pkl\n",
        "!cp test_v2.pkl drive/MyDrive/DeepLearning/test_v2.pkl"
      ],
      "metadata": {
        "id": "QbzPyjaBdZeu"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}