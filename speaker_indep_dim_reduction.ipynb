{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMC7oc56Ub3/eDWgoMZbtdB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v-artur/Golden_Oreos/blob/main/speaker_indep_dim_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtaining the data"
      ],
      "metadata": {
        "id": "QRznZ-paPxRN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3F36vS9-11r",
        "outputId": "99fd3a3b-9dcd-4680-a202-5079c968441a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-09 22:32:56--  https://docs.google.com/uc?export=download&confirm=t&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.204.113, 74.125.204.102, 74.125.204.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.204.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/pghd24vvobh47r6e5tfiagrhq00gqocu/1670625150000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=aa55481b-3989-440c-b8b8-c3ef448cccf7 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-09 22:32:57--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/pghd24vvobh47r6e5tfiagrhq00gqocu/1670625150000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=aa55481b-3989-440c-b8b8-c3ef448cccf7\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 64.233.189.132, 2404:6800:4008:c07::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|64.233.189.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2141405093 (2.0G) [application/x-zip-compressed]\n",
            "Saving to: ‘features.zip’\n",
            "\n",
            "features.zip        100%[===================>]   1.99G   152MB/s    in 14s     \n",
            "\n",
            "2022-12-09 22:33:11 (144 MB/s) - ‘features.zip’ saved [2141405093/2141405093]\n",
            "\n",
            "--2022-12-09 22:33:12--  https://docs.google.com/uc?export=download&confirm=&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.204.113, 74.125.204.102, 74.125.204.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.204.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/veq7q6t4v3hgqoo5aslh65du09490lf9/1670625150000/17895932938140350971/*/1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80?e=download&uuid=251690db-6986-47e7-a5be-cd2237be866a [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-09 22:33:12--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/veq7q6t4v3hgqoo5aslh65du09490lf9/1670625150000/17895932938140350971/*/1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80?e=download&uuid=251690db-6986-47e7-a5be-cd2237be866a\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 64.233.189.132, 2404:6800:4008:c07::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|64.233.189.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9238 (9.0K) [application/x-zip-compressed]\n",
            "Saving to: ‘subject_channels.zip’\n",
            "\n",
            "subject_channels.zi 100%[===================>]   9.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-09 22:33:13 (76.3 MB/s) - ‘subject_channels.zip’ saved [9238/9238]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#features\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\" -O features.zip && rm -rf /tmp/cookies.txt\n",
        "#original electrode names\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80\" -O subject_channels.zip && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# Data extraction\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/features.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/features\")\n",
        "zip_ref.close()\n",
        "\n",
        "# Electrode name extraction\n",
        "zip_ref = zipfile.ZipFile(\"/content/subject_channels.zip\", 'r')\n",
        "zip_ref.extractall(\"/content\")\n",
        "zip_ref.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Preparations and needed functions</h3>\n",
        "\n"
      ],
      "metadata": {
        "id": "QdxGeq3EP2hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our plan is to transform every feature vector into a larger dimensional feature vector,  which contains all the different electrode names across all the subjects."
      ],
      "metadata": {
        "id": "WgUblU51bfZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Getting the different electrode names\n",
        "all_electrodes = set()\n",
        "\n",
        "feat_path = \"/content/features\"\n",
        "\n",
        "for subject in ['01','02','03','04','05','06','07','08','09','10']:\n",
        "  elecs = np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy'))\n",
        "  elecs = set(elecs)\n",
        "  all_electrodes = all_electrodes.union(elecs)\n",
        "\n",
        "# We will use this list's indexes to correspond to the feature matrices gained from the subjects\n",
        "all_electrodes = list(all_electrodes) \n",
        "\n",
        "print('Number of different electrodes:', len(all_electrodes))"
      ],
      "metadata": {
        "id": "annt8G29nDNS",
        "outputId": "65e3620f-03b2-4110-a6a0-0379a154ab6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of different electrodes: 4860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for making every feature vector into a 4860 dimensional vector\n",
        "\n",
        "def dim_adjust(data, feature_names):\n",
        "  #create a new matrix with zeros, insert the values into the columns\n",
        "  #which correspond to the subject's feature names, then concatenate them\n",
        "\n",
        "  new_matrix = np.zeros((data.shape[0],len(all_electrodes)))\n",
        "  for column in range(data.shape[1]):\n",
        "    insert_index = all_electrodes.index(feature_names[column])\n",
        "    new_matrix[:,insert_index] = data[:,column]\n",
        "    \n",
        "  return new_matrix\n"
      ],
      "metadata": {
        "id": "d8o2edSn_CkY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making the iterated test, validation and test sets"
      ],
      "metadata": {
        "id": "bwJu3UnzP8ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate the train, val and test features and mel spectrograms\n",
        "\n",
        "def generate_features_and_spec():\n",
        "  #Base arrays\n",
        "  train_feat = np.empty((0, len(all_electrodes)))\n",
        "  val_feat = np.empty((0, len(all_electrodes)))\n",
        "  test_feat = np.empty((0, len(all_electrodes)))\n",
        "\n",
        "  train_spec = np.empty((0, 23))\n",
        "  val_spec = np.empty((0, 23))\n",
        "  test_spec = np.empty((0, 23))\n",
        "\n",
        "  for index, subject in enumerate(['01','02','03','04','05','06','07','08','09','10']):\n",
        "    #loading the features, feature names and mel spectrogram of the subject\n",
        "    data = np.load(os.path.join(feat_path,f'sub-{subject}_feat.npy'))\n",
        "    feature_names = np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy'))\n",
        "    spectrogram = np.load(os.path.join(feat_path,f'sub-{subject}_spec.npy'))\n",
        "\n",
        "    #splittig the features and the labels into 5 parts\n",
        "    feat_splits = np.array_split(data, 5)\n",
        "    spec_splits = np.array_split(spectrogram, 5)\n",
        "\n",
        "    #making the train, val or test arrays for the subject using a 60-20-20 ratio\n",
        "    #because of the \"index\" changing, the splitting position will iterate with each subject as well\n",
        "    subject_train_feat = np.vstack((feat_splits[index % 5],feat_splits[(index+1) % 5],feat_splits[(index+2) % 5]))\n",
        "    subject_train_spec = np.vstack((spec_splits[index % 5],spec_splits[(index+1) % 5],spec_splits[(index+2) % 5]))\n",
        "\n",
        "    subject_val_feat = feat_splits[(index+3) % 5]\n",
        "    subject_val_spec = spec_splits[(index+3) % 5]\n",
        "\n",
        "    subject_test_feat = feat_splits[(index+4) % 5]\n",
        "    subject_test_spec = spec_splits[(index+4) % 5]\n",
        "\n",
        "    #concatenating the dimensionality-adjusted features with the pre-existing feature set\n",
        "    train_feat = np.concatenate((train_feat, dim_adjust(subject_train_feat, feature_names)))\n",
        "    val_feat = np.concatenate((val_feat, dim_adjust(subject_val_feat, feature_names)))\n",
        "    test_feat = np.concatenate((test_feat, dim_adjust(subject_test_feat, feature_names)))\n",
        "\n",
        "    #concatenating the appropriate arrays with the pre-existing train, val or test labels\n",
        "    train_spec = np.concatenate((train_spec, subject_train_spec))\n",
        "    val_spec = np.concatenate((val_spec, subject_val_spec))\n",
        "    test_spec = np.concatenate((test_spec, subject_test_spec))\n",
        "\n",
        "  return train_feat, train_spec, val_feat, val_spec, test_feat, test_spec \n",
        "\n",
        "# Generating the data\n",
        "train_feat, train_spec, val_feat, val_spec, test_feat, test_spec = generate_features_and_spec()"
      ],
      "metadata": {
        "id": "3sBGVYAoc7LA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Scaling the data \n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_feat)\n",
        "train_feat = scaler.transform(train_feat)\n",
        "val_feat = scaler.transform(val_feat)\n",
        "test_feat = scaler.transform(test_feat)"
      ],
      "metadata": {
        "id": "JC2LF6YOfflo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality reduction"
      ],
      "metadata": {
        "id": "MRV6pzVbmNWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Option 1: Tuned AutoEncoder</h3>"
      ],
      "metadata": {
        "id": "NV5SeqERySxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "dVwyhAYq-4O9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the data generator\n",
        "from tensorflow.keras.utils import Sequence, set_random_seed\n",
        "\n",
        "set_random_seed(1234)\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    # Initialization\n",
        "    def __init__(self, data, batch_size=32, dim=len(all_electrodes), shuffle=True):\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.data = data\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Shows the number of batches per epoch\n",
        "        return int(np.floor(self.data.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate one batch of data\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        batch = np.empty((self.batch_size, self.dim))\n",
        "        for index1, elem in enumerate(indexes):\n",
        "          batch[index1] = self.data[index1]\n",
        "\n",
        "        return batch, batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updating the index after each epoch\n",
        "        self.indexes = np.arange(self.data.shape[0])\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "# Creating the generators for training\n",
        "# According to our early experiments, batch sizes of 256 or 512 proved to be the best,\n",
        "# but to shorten the amount time for hyperparameter tuning, we set it for 512\n",
        "train_gen = DataGenerator(train_feat, 512)\n",
        "val_gen = DataGenerator(val_feat, 512)\n",
        "test_gen = DataGenerator(test_feat, 512)"
      ],
      "metadata": {
        "id": "xQ8U-ZLCCYTL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "import keras_tuner as kt\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# Creating the hyperparameter tuner function\n",
        "# The AE has 5 Dense layers besides the output, and the 3rd one is the bottleneck layer\n",
        "def create_ae_optimal(hp):\n",
        "  input = Input(shape=(4860))\n",
        "  \n",
        "  # Layer 1\n",
        "  hp_units_1 = hp.Int('units_1', min_value=1500, max_value=2500, step=250)\n",
        "  encoded = Dense(units=hp_units_1, activation=\"relu\", kernel_initializer='HeNormal')(input)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_1', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(encoded)\n",
        "\n",
        "  # Layer 2\n",
        "  hp_units_2 = hp.Int('units_2', min_value=750, max_value=1250, step=125)\n",
        "  encoded = Dense(units=hp_units_2, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_2', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(encoded)\n",
        "\n",
        "  # Layer 3\n",
        "  hp_units_3 = hp.Int('units_3', min_value=350, max_value=650, step=50)\n",
        "  encoded = Dense(units=hp_units_3, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_3', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(encoded)\n",
        "\n",
        "  # Layer 4\n",
        "  hp_units_4 = hp.Int('units_4', min_value=750, max_value=1250, step=125)\n",
        "  decoded = Dense(units=hp_units_4, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  decoded = Dropout(rate=hp.Float('dropout_4', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(decoded)\n",
        "\n",
        "  # Layer 5\n",
        "  hp_units_5 = hp.Int('units_5', min_value=1500, max_value=2500, step=250)\n",
        "  decoded = Dense(units=hp_units_5, activation=\"relu\", kernel_initializer='HeNormal')(decoded)\n",
        "  decoded = Dropout(rate=hp.Float('dropout_5', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(decoded)\n",
        "\n",
        "  # Output layer\n",
        "  output = Dense(4860, activation='linear')(decoded)\n",
        "\n",
        "  # Optimizer parameters\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  hp_momentum = hp.Choice('momentum', values=[0.9, 0.95])\n",
        "\n",
        "  model = Model(input, output)\n",
        "  # Our early experiments showed that SGD preforms slightly better here than ADAM\n",
        "  model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=hp_learning_rate, momentum=hp_momentum),\n",
        "                loss='mse',\n",
        "                metrics=['mse'])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "R_ZSI31I-4zT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the tuner\n",
        "tuner = kt.Hyperband(create_ae_optimal,\n",
        "                     objective='val_loss',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='/content/ae_opt',\n",
        "                     project_name='ae_opt1')"
      ],
      "metadata": {
        "id": "kdUDep51B7_T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "#note: takes about 20 minutes to optimize\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=10)\n",
        "tuner.search(train_gen, epochs=100, validation_data=val_gen, verbose=0, shuffle=True, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "ISWZG5BNCJ7d"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting and retraining the model with the best params\n",
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "early_stopping=EarlyStopping(patience=10, verbose=1, min_delta=1e-5)\n",
        "checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "hypermodel.fit(train_gen, epochs=100, verbose=1, validation_data=val_gen, callbacks=[checkpointer, early_stopping])"
      ],
      "metadata": {
        "id": "_lpNSFOpDDJj",
        "outputId": "46e86119-5498-4f95-dc96-822b089328d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.6928 - mse: 0.6928\n",
            "Epoch 1: val_loss improved from inf to 0.69168, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.6928 - mse: 0.6928 - val_loss: 0.6917 - val_mse: 0.6917\n",
            "Epoch 2/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.5614 - mse: 0.5614\n",
            "Epoch 2: val_loss improved from 0.69168 to 0.67581, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.5614 - mse: 0.5614 - val_loss: 0.6758 - val_mse: 0.6758\n",
            "Epoch 3/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.5451 - mse: 0.5451\n",
            "Epoch 3: val_loss improved from 0.67581 to 0.64288, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.5450 - mse: 0.5450 - val_loss: 0.6429 - val_mse: 0.6429\n",
            "Epoch 4/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.5196 - mse: 0.5196\n",
            "Epoch 4: val_loss improved from 0.64288 to 0.59481, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.5195 - mse: 0.5195 - val_loss: 0.5948 - val_mse: 0.5948\n",
            "Epoch 5/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.4819 - mse: 0.4819\n",
            "Epoch 5: val_loss improved from 0.59481 to 0.54610, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.4819 - mse: 0.4819 - val_loss: 0.5461 - val_mse: 0.5461\n",
            "Epoch 6/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.4494 - mse: 0.4494\n",
            "Epoch 6: val_loss improved from 0.54610 to 0.51222, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.4494 - mse: 0.4494 - val_loss: 0.5122 - val_mse: 0.5122\n",
            "Epoch 7/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.4207 - mse: 0.4207\n",
            "Epoch 7: val_loss improved from 0.51222 to 0.48177, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.4207 - mse: 0.4207 - val_loss: 0.4818 - val_mse: 0.4818\n",
            "Epoch 8/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.3899 - mse: 0.3899\n",
            "Epoch 8: val_loss improved from 0.48177 to 0.45153, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.3899 - mse: 0.3899 - val_loss: 0.4515 - val_mse: 0.4515\n",
            "Epoch 9/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.3631 - mse: 0.3631\n",
            "Epoch 9: val_loss improved from 0.45153 to 0.41879, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.3631 - mse: 0.3631 - val_loss: 0.4188 - val_mse: 0.4188\n",
            "Epoch 10/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.3424 - mse: 0.3424\n",
            "Epoch 10: val_loss improved from 0.41879 to 0.39150, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.3424 - mse: 0.3424 - val_loss: 0.3915 - val_mse: 0.3915\n",
            "Epoch 11/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.3251 - mse: 0.3251\n",
            "Epoch 11: val_loss improved from 0.39150 to 0.37263, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.3251 - mse: 0.3251 - val_loss: 0.3726 - val_mse: 0.3726\n",
            "Epoch 12/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.3099 - mse: 0.3099\n",
            "Epoch 12: val_loss improved from 0.37263 to 0.35173, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.3099 - mse: 0.3099 - val_loss: 0.3517 - val_mse: 0.3517\n",
            "Epoch 13/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2971 - mse: 0.2971\n",
            "Epoch 13: val_loss improved from 0.35173 to 0.33601, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2971 - mse: 0.2971 - val_loss: 0.3360 - val_mse: 0.3360\n",
            "Epoch 14/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2870 - mse: 0.2870\n",
            "Epoch 14: val_loss improved from 0.33601 to 0.32114, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2870 - mse: 0.2870 - val_loss: 0.3211 - val_mse: 0.3211\n",
            "Epoch 15/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2791 - mse: 0.2791\n",
            "Epoch 15: val_loss improved from 0.32114 to 0.31050, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.3105 - val_mse: 0.3105\n",
            "Epoch 16/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2728 - mse: 0.2728\n",
            "Epoch 16: val_loss improved from 0.31050 to 0.30097, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2728 - mse: 0.2728 - val_loss: 0.3010 - val_mse: 0.3010\n",
            "Epoch 17/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2679 - mse: 0.2679\n",
            "Epoch 17: val_loss improved from 0.30097 to 0.29445, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2679 - mse: 0.2679 - val_loss: 0.2945 - val_mse: 0.2945\n",
            "Epoch 18/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2639 - mse: 0.2639\n",
            "Epoch 18: val_loss improved from 0.29445 to 0.28997, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2639 - mse: 0.2639 - val_loss: 0.2900 - val_mse: 0.2900\n",
            "Epoch 19/100\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.2606 - mse: 0.2606\n",
            "Epoch 19: val_loss improved from 0.28997 to 0.28616, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2606 - mse: 0.2606 - val_loss: 0.2862 - val_mse: 0.2862\n",
            "Epoch 20/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2575 - mse: 0.2575\n",
            "Epoch 20: val_loss improved from 0.28616 to 0.28247, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2575 - mse: 0.2575 - val_loss: 0.2825 - val_mse: 0.2825\n",
            "Epoch 21/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2550 - mse: 0.2550\n",
            "Epoch 21: val_loss improved from 0.28247 to 0.28084, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2550 - mse: 0.2550 - val_loss: 0.2808 - val_mse: 0.2808\n",
            "Epoch 22/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2526 - mse: 0.2526\n",
            "Epoch 22: val_loss improved from 0.28084 to 0.27940, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2526 - mse: 0.2526 - val_loss: 0.2794 - val_mse: 0.2794\n",
            "Epoch 23/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2506 - mse: 0.2506\n",
            "Epoch 23: val_loss improved from 0.27940 to 0.27819, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2506 - mse: 0.2506 - val_loss: 0.2782 - val_mse: 0.2782\n",
            "Epoch 24/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2487 - mse: 0.2487\n",
            "Epoch 24: val_loss improved from 0.27819 to 0.27610, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2487 - mse: 0.2487 - val_loss: 0.2761 - val_mse: 0.2761\n",
            "Epoch 25/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2469 - mse: 0.2469\n",
            "Epoch 25: val_loss improved from 0.27610 to 0.27567, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2469 - mse: 0.2469 - val_loss: 0.2757 - val_mse: 0.2757\n",
            "Epoch 26/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2453 - mse: 0.2453\n",
            "Epoch 26: val_loss improved from 0.27567 to 0.27501, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 33ms/step - loss: 0.2453 - mse: 0.2453 - val_loss: 0.2750 - val_mse: 0.2750\n",
            "Epoch 27/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2437 - mse: 0.2437\n",
            "Epoch 27: val_loss improved from 0.27501 to 0.27445, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 35ms/step - loss: 0.2437 - mse: 0.2437 - val_loss: 0.2745 - val_mse: 0.2745\n",
            "Epoch 28/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2422 - mse: 0.2422\n",
            "Epoch 28: val_loss improved from 0.27445 to 0.27371, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 33ms/step - loss: 0.2422 - mse: 0.2422 - val_loss: 0.2737 - val_mse: 0.2737\n",
            "Epoch 29/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2408 - mse: 0.2408\n",
            "Epoch 29: val_loss improved from 0.27371 to 0.27365, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2737 - val_mse: 0.2737\n",
            "Epoch 30/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2394 - mse: 0.2394\n",
            "Epoch 30: val_loss improved from 0.27365 to 0.27324, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 33ms/step - loss: 0.2394 - mse: 0.2394 - val_loss: 0.2732 - val_mse: 0.2732\n",
            "Epoch 31/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2381 - mse: 0.2381\n",
            "Epoch 31: val_loss improved from 0.27324 to 0.27275, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2381 - mse: 0.2381 - val_loss: 0.2727 - val_mse: 0.2727\n",
            "Epoch 32/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2366 - mse: 0.2366\n",
            "Epoch 32: val_loss improved from 0.27275 to 0.27199, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2366 - mse: 0.2366 - val_loss: 0.2720 - val_mse: 0.2720\n",
            "Epoch 33/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2353 - mse: 0.2353\n",
            "Epoch 33: val_loss did not improve from 0.27199\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.2353 - mse: 0.2353 - val_loss: 0.2726 - val_mse: 0.2726\n",
            "Epoch 34/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 34: val_loss improved from 0.27199 to 0.27141, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2341 - mse: 0.2341 - val_loss: 0.2714 - val_mse: 0.2714\n",
            "Epoch 35/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2327 - mse: 0.2327\n",
            "Epoch 35: val_loss did not improve from 0.27141\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.2327 - mse: 0.2327 - val_loss: 0.2715 - val_mse: 0.2715\n",
            "Epoch 36/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2315 - mse: 0.2315\n",
            "Epoch 36: val_loss improved from 0.27141 to 0.27126, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2315 - mse: 0.2315 - val_loss: 0.2713 - val_mse: 0.2713\n",
            "Epoch 37/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2301 - mse: 0.2301\n",
            "Epoch 37: val_loss did not improve from 0.27126\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.2301 - mse: 0.2301 - val_loss: 0.2716 - val_mse: 0.2716\n",
            "Epoch 38/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2288 - mse: 0.2288\n",
            "Epoch 38: val_loss improved from 0.27126 to 0.27074, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2288 - mse: 0.2288 - val_loss: 0.2707 - val_mse: 0.2707\n",
            "Epoch 39/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2274 - mse: 0.2274\n",
            "Epoch 39: val_loss did not improve from 0.27074\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.2274 - mse: 0.2274 - val_loss: 0.2710 - val_mse: 0.2710\n",
            "Epoch 40/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2260 - mse: 0.2260\n",
            "Epoch 40: val_loss did not improve from 0.27074\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.2260 - mse: 0.2260 - val_loss: 0.2711 - val_mse: 0.2711\n",
            "Epoch 41/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2246 - mse: 0.2246\n",
            "Epoch 41: val_loss did not improve from 0.27074\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.2246 - mse: 0.2246 - val_loss: 0.2713 - val_mse: 0.2713\n",
            "Epoch 42/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2231 - mse: 0.2231\n",
            "Epoch 42: val_loss did not improve from 0.27074\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.2231 - mse: 0.2231 - val_loss: 0.2714 - val_mse: 0.2714\n",
            "Epoch 43/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2215 - mse: 0.2215\n",
            "Epoch 43: val_loss did not improve from 0.27074\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.2215 - mse: 0.2215 - val_loss: 0.2709 - val_mse: 0.2709\n",
            "Epoch 44/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2199 - mse: 0.2199\n",
            "Epoch 44: val_loss improved from 0.27074 to 0.27070, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2199 - mse: 0.2199 - val_loss: 0.2707 - val_mse: 0.2707\n",
            "Epoch 45/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2182 - mse: 0.2182\n",
            "Epoch 45: val_loss did not improve from 0.27070\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.2182 - mse: 0.2182 - val_loss: 0.2719 - val_mse: 0.2719\n",
            "Epoch 46/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2165 - mse: 0.2165\n",
            "Epoch 46: val_loss did not improve from 0.27070\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.2165 - mse: 0.2165 - val_loss: 0.2713 - val_mse: 0.2713\n",
            "Epoch 47/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2148 - mse: 0.2148\n",
            "Epoch 47: val_loss did not improve from 0.27070\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.2148 - mse: 0.2148 - val_loss: 0.2707 - val_mse: 0.2707\n",
            "Epoch 48/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2130 - mse: 0.2130\n",
            "Epoch 48: val_loss improved from 0.27070 to 0.27034, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2130 - mse: 0.2130 - val_loss: 0.2703 - val_mse: 0.2703\n",
            "Epoch 49/100\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.2112 - mse: 0.2112\n",
            "Epoch 49: val_loss did not improve from 0.27034\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.2112 - mse: 0.2112 - val_loss: 0.2708 - val_mse: 0.2708\n",
            "Epoch 50/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2093 - mse: 0.2093\n",
            "Epoch 50: val_loss improved from 0.27034 to 0.27021, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2093 - mse: 0.2093 - val_loss: 0.2702 - val_mse: 0.2702\n",
            "Epoch 51/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2074 - mse: 0.2074\n",
            "Epoch 51: val_loss did not improve from 0.27021\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.2074 - mse: 0.2074 - val_loss: 0.2708 - val_mse: 0.2708\n",
            "Epoch 52/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2056 - mse: 0.2056\n",
            "Epoch 52: val_loss improved from 0.27021 to 0.26983, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2056 - mse: 0.2056 - val_loss: 0.2698 - val_mse: 0.2698\n",
            "Epoch 53/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2037 - mse: 0.2037\n",
            "Epoch 53: val_loss improved from 0.26983 to 0.26961, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 35ms/step - loss: 0.2037 - mse: 0.2037 - val_loss: 0.2696 - val_mse: 0.2696\n",
            "Epoch 54/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2017 - mse: 0.2017\n",
            "Epoch 54: val_loss improved from 0.26961 to 0.26950, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.2017 - mse: 0.2017 - val_loss: 0.2695 - val_mse: 0.2695\n",
            "Epoch 55/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1999 - mse: 0.1999\n",
            "Epoch 55: val_loss improved from 0.26950 to 0.26921, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.1999 - mse: 0.1999 - val_loss: 0.2692 - val_mse: 0.2692\n",
            "Epoch 56/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1981 - mse: 0.1981\n",
            "Epoch 56: val_loss did not improve from 0.26921\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1981 - mse: 0.1981 - val_loss: 0.2694 - val_mse: 0.2694\n",
            "Epoch 57/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1962 - mse: 0.1962\n",
            "Epoch 57: val_loss did not improve from 0.26921\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1962 - mse: 0.1962 - val_loss: 0.2701 - val_mse: 0.2701\n",
            "Epoch 58/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1944 - mse: 0.1944\n",
            "Epoch 58: val_loss did not improve from 0.26921\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1944 - mse: 0.1944 - val_loss: 0.2692 - val_mse: 0.2692\n",
            "Epoch 59/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1926 - mse: 0.1926\n",
            "Epoch 59: val_loss improved from 0.26921 to 0.26849, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.1926 - mse: 0.1926 - val_loss: 0.2685 - val_mse: 0.2685\n",
            "Epoch 60/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1908 - mse: 0.1908\n",
            "Epoch 60: val_loss did not improve from 0.26849\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1908 - mse: 0.1908 - val_loss: 0.2690 - val_mse: 0.2690\n",
            "Epoch 61/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1892 - mse: 0.1892\n",
            "Epoch 61: val_loss improved from 0.26849 to 0.26842, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.1892 - mse: 0.1892 - val_loss: 0.2684 - val_mse: 0.2684\n",
            "Epoch 62/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1873 - mse: 0.1873\n",
            "Epoch 62: val_loss improved from 0.26842 to 0.26831, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.1873 - mse: 0.1873 - val_loss: 0.2683 - val_mse: 0.2683\n",
            "Epoch 63/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1857 - mse: 0.1857\n",
            "Epoch 63: val_loss improved from 0.26831 to 0.26677, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.1857 - mse: 0.1857 - val_loss: 0.2668 - val_mse: 0.2668\n",
            "Epoch 64/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1840 - mse: 0.1840\n",
            "Epoch 64: val_loss did not improve from 0.26677\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1840 - mse: 0.1840 - val_loss: 0.2682 - val_mse: 0.2682\n",
            "Epoch 65/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1825 - mse: 0.1825\n",
            "Epoch 65: val_loss did not improve from 0.26677\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1825 - mse: 0.1825 - val_loss: 0.2683 - val_mse: 0.2683\n",
            "Epoch 66/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1808 - mse: 0.1808\n",
            "Epoch 66: val_loss did not improve from 0.26677\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1808 - mse: 0.1808 - val_loss: 0.2675 - val_mse: 0.2675\n",
            "Epoch 67/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1791 - mse: 0.1791\n",
            "Epoch 67: val_loss did not improve from 0.26677\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1791 - mse: 0.1791 - val_loss: 0.2680 - val_mse: 0.2680\n",
            "Epoch 68/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1776 - mse: 0.1776\n",
            "Epoch 68: val_loss did not improve from 0.26677\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1776 - mse: 0.1776 - val_loss: 0.2668 - val_mse: 0.2668\n",
            "Epoch 69/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1761 - mse: 0.1761\n",
            "Epoch 69: val_loss did not improve from 0.26677\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1761 - mse: 0.1761 - val_loss: 0.2678 - val_mse: 0.2678\n",
            "Epoch 70/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1745 - mse: 0.1745\n",
            "Epoch 70: val_loss did not improve from 0.26677\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1745 - mse: 0.1745 - val_loss: 0.2672 - val_mse: 0.2672\n",
            "Epoch 71/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1730 - mse: 0.1730\n",
            "Epoch 71: val_loss did not improve from 0.26677\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.1730 - mse: 0.1730 - val_loss: 0.2676 - val_mse: 0.2676\n",
            "Epoch 72/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1715 - mse: 0.1715\n",
            "Epoch 72: val_loss did not improve from 0.26677\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.1715 - mse: 0.1715 - val_loss: 0.2673 - val_mse: 0.2673\n",
            "Epoch 73/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1700 - mse: 0.1700\n",
            "Epoch 73: val_loss improved from 0.26677 to 0.26642, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.1700 - mse: 0.1700 - val_loss: 0.2664 - val_mse: 0.2664\n",
            "Epoch 74/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1685 - mse: 0.1685\n",
            "Epoch 74: val_loss improved from 0.26642 to 0.26610, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.1685 - mse: 0.1685 - val_loss: 0.2661 - val_mse: 0.2661\n",
            "Epoch 75/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1672 - mse: 0.1672\n",
            "Epoch 75: val_loss did not improve from 0.26610\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1672 - mse: 0.1672 - val_loss: 0.2665 - val_mse: 0.2665\n",
            "Epoch 76/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1657 - mse: 0.1657\n",
            "Epoch 76: val_loss did not improve from 0.26610\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1657 - mse: 0.1657 - val_loss: 0.2664 - val_mse: 0.2664\n",
            "Epoch 77/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1643 - mse: 0.1643\n",
            "Epoch 77: val_loss did not improve from 0.26610\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1643 - mse: 0.1643 - val_loss: 0.2661 - val_mse: 0.2661\n",
            "Epoch 78/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1630 - mse: 0.1630\n",
            "Epoch 78: val_loss improved from 0.26610 to 0.26573, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.1630 - mse: 0.1630 - val_loss: 0.2657 - val_mse: 0.2657\n",
            "Epoch 79/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1616 - mse: 0.1616\n",
            "Epoch 79: val_loss improved from 0.26573 to 0.26481, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.1616 - mse: 0.1616 - val_loss: 0.2648 - val_mse: 0.2648\n",
            "Epoch 80/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1602 - mse: 0.1602\n",
            "Epoch 80: val_loss did not improve from 0.26481\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1602 - mse: 0.1602 - val_loss: 0.2651 - val_mse: 0.2651\n",
            "Epoch 81/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1590 - mse: 0.1590\n",
            "Epoch 81: val_loss did not improve from 0.26481\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1590 - mse: 0.1590 - val_loss: 0.2652 - val_mse: 0.2652\n",
            "Epoch 82/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1577 - mse: 0.1577\n",
            "Epoch 82: val_loss did not improve from 0.26481\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1577 - mse: 0.1577 - val_loss: 0.2653 - val_mse: 0.2653\n",
            "Epoch 83/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1564 - mse: 0.1564\n",
            "Epoch 83: val_loss did not improve from 0.26481\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1564 - mse: 0.1564 - val_loss: 0.2651 - val_mse: 0.2651\n",
            "Epoch 84/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1551 - mse: 0.1551\n",
            "Epoch 84: val_loss did not improve from 0.26481\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.1551 - mse: 0.1551 - val_loss: 0.2652 - val_mse: 0.2652\n",
            "Epoch 85/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1539 - mse: 0.1539\n",
            "Epoch 85: val_loss did not improve from 0.26481\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1539 - mse: 0.1539 - val_loss: 0.2659 - val_mse: 0.2659\n",
            "Epoch 86/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1527 - mse: 0.1527\n",
            "Epoch 86: val_loss improved from 0.26481 to 0.26458, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.1527 - mse: 0.1527 - val_loss: 0.2646 - val_mse: 0.2646\n",
            "Epoch 87/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1515 - mse: 0.1515\n",
            "Epoch 87: val_loss did not improve from 0.26458\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1515 - mse: 0.1515 - val_loss: 0.2653 - val_mse: 0.2653\n",
            "Epoch 88/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1503 - mse: 0.1503\n",
            "Epoch 88: val_loss improved from 0.26458 to 0.26452, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.1503 - mse: 0.1503 - val_loss: 0.2645 - val_mse: 0.2645\n",
            "Epoch 89/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1492 - mse: 0.1492\n",
            "Epoch 89: val_loss improved from 0.26452 to 0.26445, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.1492 - mse: 0.1492 - val_loss: 0.2644 - val_mse: 0.2644\n",
            "Epoch 90/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1480 - mse: 0.1480\n",
            "Epoch 90: val_loss improved from 0.26445 to 0.26427, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 33ms/step - loss: 0.1480 - mse: 0.1480 - val_loss: 0.2643 - val_mse: 0.2643\n",
            "Epoch 91/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1469 - mse: 0.1469\n",
            "Epoch 91: val_loss improved from 0.26427 to 0.26408, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.1469 - mse: 0.1469 - val_loss: 0.2641 - val_mse: 0.2641\n",
            "Epoch 92/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1457 - mse: 0.1457\n",
            "Epoch 92: val_loss did not improve from 0.26408\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1457 - mse: 0.1457 - val_loss: 0.2648 - val_mse: 0.2648\n",
            "Epoch 93/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1445 - mse: 0.1445\n",
            "Epoch 93: val_loss improved from 0.26408 to 0.26407, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 33ms/step - loss: 0.1445 - mse: 0.1445 - val_loss: 0.2641 - val_mse: 0.2641\n",
            "Epoch 94/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1435 - mse: 0.1435\n",
            "Epoch 94: val_loss did not improve from 0.26407\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1435 - mse: 0.1435 - val_loss: 0.2651 - val_mse: 0.2651\n",
            "Epoch 95/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1424 - mse: 0.1424\n",
            "Epoch 95: val_loss did not improve from 0.26407\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1424 - mse: 0.1424 - val_loss: 0.2642 - val_mse: 0.2642\n",
            "Epoch 96/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1412 - mse: 0.1412\n",
            "Epoch 96: val_loss did not improve from 0.26407\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1412 - mse: 0.1412 - val_loss: 0.2645 - val_mse: 0.2645\n",
            "Epoch 97/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1403 - mse: 0.1403\n",
            "Epoch 97: val_loss did not improve from 0.26407\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1403 - mse: 0.1403 - val_loss: 0.2648 - val_mse: 0.2648\n",
            "Epoch 98/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1393 - mse: 0.1393\n",
            "Epoch 98: val_loss did not improve from 0.26407\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1393 - mse: 0.1393 - val_loss: 0.2643 - val_mse: 0.2643\n",
            "Epoch 99/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1382 - mse: 0.1382\n",
            "Epoch 99: val_loss did not improve from 0.26407\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1382 - mse: 0.1382 - val_loss: 0.2650 - val_mse: 0.2650\n",
            "Epoch 100/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1371 - mse: 0.1371\n",
            "Epoch 100: val_loss did not improve from 0.26407\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.1371 - mse: 0.1371 - val_loss: 0.2641 - val_mse: 0.2641\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f641a5cfdf0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the performance on the test set\n",
        "hypermodel.load_weights('weights1.hdf5')\n",
        "hypermodel.evaluate(test_gen)"
      ],
      "metadata": {
        "id": "bOq0ZdtfBjCD",
        "outputId": "1b5eb341-7847-47be-862b-3e08dc684779",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "116/116 [==============================] - 2s 15ms/step - loss: 0.2780 - mse: 0.2780\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2780494689941406, 0.2780494689941406]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading back the best weights and checking the layers\n",
        "hypermodel.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QWuY1lchrS4",
        "outputId": "ad324a8a-b46d-4d96-81f7-e78875c60880"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4860)]            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1750)              8506750   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 1750)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 750)               1313250   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 750)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 400)               300400    \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1000)              401000    \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 2500)              2502500   \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 2500)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 4860)              12154860  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,178,760\n",
            "Trainable params: 25,178,760\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keeping only the encoder\n",
        "model2 = Model(inputs=hypermodel.input, outputs=hypermodel.layers[-7].output)\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrTwZhiyhv8Q",
        "outputId": "8c9dfb53-b165-4171-e5ef-0a1b61dbf504"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4860)]            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1750)              8506750   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 1750)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 750)               1313250   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 750)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 400)               300400    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,120,400\n",
            "Trainable params: 10,120,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sizes of the new arrays\n",
        "print('Train:', train_feat.shape) \n",
        "print('Validation:', val_feat.shape)\n",
        "print('Test:', test_feat.shape)"
      ],
      "metadata": {
        "id": "OYLjwKunQi-J",
        "outputId": "35ee16e1-f140-416c-b9a3-192ad9f421b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (179011, 4860)\n",
            "Validation: (59672, 4860)\n",
            "Test: (59672, 4860)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to redefine the generators for the predictions in order to retain every feature vector in every set, otherwise some of them would be left out because of the batch size.\n",
        "\n",
        "Since the memory can't fit all the data, and using 1 as batch size would be slow, we need to use prime factorization to determine the appropriate batch sizes.\n",
        "\n",
        "The factorizations are:\n",
        "- 179011 = 7 x 107 x 239\n",
        "- 59672 = 8 * 7459  "
      ],
      "metadata": {
        "id": "j25X21GObqbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = DataGenerator(train_feat, 107*239)\n",
        "val_gen = DataGenerator(val_feat, 7459)\n",
        "test_gen = DataGenerator(test_feat, 7459)\n",
        "\n",
        "# Generating the new data using the output of the bottleneck layer\n",
        "train_new = model2.predict(train_gen)\n",
        "val_new = model2.predict(val_gen)\n",
        "test_new = model2.predict(test_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqrNk9pKiiHa",
        "outputId": "791cdb6d-128d-4b63-d80e-59b1ed380858"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 5s 774ms/step\n",
            "8/8 [==============================] - 2s 199ms/step\n",
            "8/8 [==============================] - 2s 198ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if the new sets really has the same amount of vectors\n",
        "print(train_new.shape[0] == train_feat.shape[0])\n",
        "print(val_new.shape[0] == val_feat.shape[0])\n",
        "print(test_new.shape[0] == test_feat.shape[0])"
      ],
      "metadata": {
        "id": "ODtUOd0KSBE0",
        "outputId": "056d365a-2ac4-4d1d-b38f-adfe6c90fe75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the feature-label set pairs as pickle files onto Google Drive\n",
        "import pickle\n",
        "\n",
        "with open('train.pkl', 'wb') as f:  \n",
        "    pickle.dump([train_new, train_spec], f)\n",
        "\n",
        "with open('val.pkl', 'wb') as f:  \n",
        "    pickle.dump([val_new, val_spec], f)\n",
        "\n",
        "with open('test.pkl', 'wb') as f:  \n",
        "    pickle.dump([test_new, test_spec], f)"
      ],
      "metadata": {
        "id": "UWTKZceejPgS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp train.pkl drive/MyDrive/DeepLearning/train.pkl\n",
        "!cp val.pkl drive/MyDrive/DeepLearning/val.pkl\n",
        "!cp test.pkl drive/MyDrive/DeepLearning/test.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lhk7ec0pyg_",
        "outputId": "c07115ff-141b-46fb-a5cb-6332c4ef9bc5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Option 2: Incremental PCA</h3>"
      ],
      "metadata": {
        "id": "q9cD1KX0ajBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "n_comp = 250\n",
        "\n",
        "# note: takes about 10 minutes to run\n",
        "pca = IncrementalPCA(n_components=n_comp, batch_size=1024)\n",
        "pca.fit(train_feat)\n",
        "train_feat = pca.transform(train_feat)\n",
        "val_feat = pca.transform(val_feat)\n",
        "test_feat = pca.transform(test_feat)"
      ],
      "metadata": {
        "id": "AARr0wkEaqfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the new data\n",
        "\n",
        "with open('train_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([train_feat, train_spec], f)\n",
        "\n",
        "with open('val_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([val_feat, val_spec], f)\n",
        "\n",
        "with open('test_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([test_feat, test_spec], f)\n",
        "\n",
        "!cp train_v2.pkl drive/MyDrive/DeepLearning/train_v2.pkl\n",
        "!cp val_v2.pkl drive/MyDrive/DeepLearning/val_v2.pkl\n",
        "!cp test_v2.pkl drive/MyDrive/DeepLearning/test_v2.pkl"
      ],
      "metadata": {
        "id": "QbzPyjaBdZeu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}