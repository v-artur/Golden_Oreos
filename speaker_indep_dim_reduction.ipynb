{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPn2HUEfGUmWXY9loCfq2US",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v-artur/Golden_Oreos/blob/main/speaker_indep_dim_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtaining the data"
      ],
      "metadata": {
        "id": "QRznZ-paPxRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>IMPORTANT</b>: In order to access our files stored in Google Drive, you need to visit the following link: https://drive.google.com/drive/folders/1Qfr8TNZSlrhpKgYx0LrTxve9ljIFwqRq?usp=sharing\n",
        "\n",
        "Then, click on the \"DeepLearning\" folder just beneath the search bar, then select \"Add shortcut to Drive\", then select \"My Drive\" and create a shortcut. After that, you should be able to see our folder and the files within when you are mounting your drive. The paths to the files in the code should work as inteded, but we can't cross out the possibility that you might need to change some filepaths (it worked for us and we also tested it with our other 3rd party accounts). If you have any questions or something does not work, please contact us."
      ],
      "metadata": {
        "id": "wdt0MSBQi3Cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Data extraction\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/DeepLearning/features.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/features\")\n",
        "zip_ref.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "LkwKAzaRiiZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "457d2a9c-9b8d-4824-9ee3-6c26fddaafb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3F36vS9-11r"
      },
      "outputs": [],
      "source": [
        "# In case the google drive method doesn't work, you can try getting the files with wget\n",
        "# And in the case this throws a \"429 ERROR\", you can download our files from Google Drive, then upload them here (but then you might need to change some filepaths)\n",
        "\"\"\"\n",
        "#features\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\" -O features.zip && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# Data extraction\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/features.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/features\")\n",
        "zip_ref.close()\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Preparations and needed functions</h3>\n",
        "\n"
      ],
      "metadata": {
        "id": "QdxGeq3EP2hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our plan is to transform every feature vector into a larger dimensional feature vector,  which contains all the different electrode names across all the subjects."
      ],
      "metadata": {
        "id": "WgUblU51bfZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Getting the different electrode names\n",
        "all_electrodes = set()\n",
        "\n",
        "feat_path = \"/content/features\"\n",
        "\n",
        "for subject in ['01','02','03','04','05','06','07','08','09','10']:\n",
        "  elecs = np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy'))\n",
        "  elecs = set(elecs)\n",
        "  all_electrodes = all_electrodes.union(elecs)\n",
        "\n",
        "# We will use this list's indexes to correspond to the feature matrices gained from the subjects\n",
        "all_electrodes = list(all_electrodes) \n",
        "\n",
        "print('Number of different electrodes:', len(all_electrodes))"
      ],
      "metadata": {
        "id": "annt8G29nDNS",
        "outputId": "1fc8a268-66f6-40b1-e183-a31a69c5bf98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of different electrodes: 4860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for making every feature vector into a 4860 dimensional vector\n",
        "\n",
        "def dim_adjust(data, feature_names):\n",
        "  #create a new matrix with zeros, insert the values into the columns\n",
        "  #which correspond to the subject's feature names, then concatenate them\n",
        "\n",
        "  new_matrix = np.zeros((data.shape[0],len(all_electrodes)))\n",
        "  for column in range(data.shape[1]):\n",
        "    insert_index = all_electrodes.index(feature_names[column])\n",
        "    new_matrix[:,insert_index] = data[:,column]\n",
        "    \n",
        "  return new_matrix\n"
      ],
      "metadata": {
        "id": "d8o2edSn_CkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making the iterated test, validation and test sets"
      ],
      "metadata": {
        "id": "bwJu3UnzP8ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate the train, val and test features and mel spectrograms\n",
        "\n",
        "def generate_features_and_spec():\n",
        "  #Base arrays\n",
        "  train_feat = np.empty((0, len(all_electrodes)))\n",
        "  val_feat = np.empty((0, len(all_electrodes)))\n",
        "  test_feat = np.empty((0, len(all_electrodes)))\n",
        "\n",
        "  train_spec = np.empty((0, 23))\n",
        "  val_spec = np.empty((0, 23))\n",
        "  test_spec = np.empty((0, 23))\n",
        "\n",
        "\n",
        "  for index, subject in enumerate(['01','02','03','04','05','06','07','08','09','10']):\n",
        "    #loading the features, feature names and mel spectrogram of the subject\n",
        "    data = np.load(os.path.join(feat_path,f'sub-{subject}_feat.npy'))\n",
        "    feature_names = np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy'))\n",
        "    spectrogram = np.load(os.path.join(feat_path,f'sub-{subject}_spec.npy'))\n",
        "\n",
        "    #splittig the features and the labels into 5 parts\n",
        "    feat_splits = np.array_split(data, 5)\n",
        "    spec_splits = np.array_split(spectrogram, 5)\n",
        "\n",
        "    #making the train, val or test arrays for the subject using a 60-20-20 ratio\n",
        "    #because of the \"index\" changing, the splitting position will iterate with each subject as well\n",
        "    subject_train_feat = np.vstack((feat_splits[index % 5],feat_splits[(index+1) % 5],feat_splits[(index+2) % 5]))\n",
        "    subject_train_spec = np.vstack((spec_splits[index % 5],spec_splits[(index+1) % 5],spec_splits[(index+2) % 5]))\n",
        "\n",
        "    subject_val_feat = feat_splits[(index+3) % 5]\n",
        "    subject_val_spec = spec_splits[(index+3) % 5]\n",
        "\n",
        "    subject_test_feat = feat_splits[(index+4) % 5]\n",
        "    subject_test_spec = spec_splits[(index+4) % 5]\n",
        "\n",
        "\n",
        "    #concatenating the dimensionality-adjusted features with the pre-existing feature set\n",
        "    train_feat = np.concatenate((train_feat, dim_adjust(subject_train_feat, feature_names)))\n",
        "    val_feat = np.concatenate((val_feat, dim_adjust(subject_val_feat, feature_names)))\n",
        "    test_feat = np.concatenate((test_feat, dim_adjust(subject_test_feat, feature_names)))\n",
        "\n",
        "    #concatenating the appropriate arrays with the pre-existing train, val or test labels\n",
        "    train_spec = np.concatenate((train_spec, subject_train_spec))\n",
        "    val_spec = np.concatenate((val_spec, subject_val_spec))\n",
        "    test_spec = np.concatenate((test_spec, subject_test_spec))\n",
        "\n",
        "  return train_feat, train_spec, val_feat, val_spec, test_feat, test_spec\n",
        "\n",
        "# Generating the data\n",
        "train_feat, train_spec, val_feat, val_spec, test_feat, test_spec = generate_features_and_spec()"
      ],
      "metadata": {
        "id": "3sBGVYAoc7LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Scaling the data \n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_feat)\n",
        "train_feat = scaler.transform(train_feat)\n",
        "val_feat = scaler.transform(val_feat)\n",
        "test_feat = scaler.transform(test_feat)"
      ],
      "metadata": {
        "id": "JC2LF6YOfflo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality reduction"
      ],
      "metadata": {
        "id": "MRV6pzVbmNWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Option 1: Tuned AutoEncoder</h3>"
      ],
      "metadata": {
        "id": "NV5SeqERySxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "dVwyhAYq-4O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the data generator\n",
        "from tensorflow.keras.utils import Sequence, set_random_seed\n",
        "\n",
        "# setting seeds\n",
        "set_random_seed(1234)\n",
        "np.random.seed(42)\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    # Initialization\n",
        "    def __init__(self, data, batch_size=32, dim=len(all_electrodes), shuffle=True):\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.data = data\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Shows the number of batches per epoch\n",
        "        return int(np.floor(self.data.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate one batch of data\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        batch = np.empty((self.batch_size, self.dim))\n",
        "        for index1, elem in enumerate(indexes):\n",
        "          batch[index1] = self.data[elem]\n",
        "\n",
        "        return batch, batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updating the index after each epoch\n",
        "        self.indexes = np.arange(self.data.shape[0])\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "# Creating the generators for training\n",
        "# According to our early experiments, batch sizes of 256 or 512 proved to be the best,\n",
        "# but to shorten the amount time for hyperparameter tuning, we set it for 512\n",
        "train_gen = DataGenerator(train_feat, 512)\n",
        "val_gen = DataGenerator(val_feat, 512)\n",
        "test_gen = DataGenerator(test_feat, 512)"
      ],
      "metadata": {
        "id": "xQ8U-ZLCCYTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "import keras_tuner as kt\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# Creating the hyperparameter tuner function\n",
        "# The AE has 5 Dense layers besides the output, and the 3rd one is the bottleneck layer\n",
        "def create_ae_optimal(hp):\n",
        "  input = Input(shape=(4860))\n",
        "  \n",
        "  # Layer 1\n",
        "  hp_units_1 = hp.Int('units_1', min_value=1500, max_value=2500, step=250)\n",
        "  encoded = Dense(units=hp_units_1, activation=\"relu\", kernel_initializer='HeNormal')(input)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_1', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(encoded)\n",
        "\n",
        "  # Layer 2\n",
        "  hp_units_2 = hp.Int('units_2', min_value=750, max_value=1250, step=125)\n",
        "  encoded = Dense(units=hp_units_2, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_2', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(encoded)\n",
        "\n",
        "  # Layer 3\n",
        "  hp_units_3 = hp.Int('units_3', min_value=350, max_value=650, step=50)\n",
        "  encoded = Dense(units=hp_units_3, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_3', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(encoded)\n",
        "\n",
        "  # Layer 4\n",
        "  hp_units_4 = hp.Int('units_4', min_value=750, max_value=1250, step=125)\n",
        "  decoded = Dense(units=hp_units_4, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  decoded = Dropout(rate=hp.Float('dropout_4', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(decoded)\n",
        "\n",
        "  # Layer 5\n",
        "  hp_units_5 = hp.Int('units_5', min_value=1500, max_value=2500, step=250)\n",
        "  decoded = Dense(units=hp_units_5, activation=\"relu\", kernel_initializer='HeNormal')(decoded)\n",
        "  decoded = Dropout(rate=hp.Float('dropout_5', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(decoded)\n",
        "\n",
        "  # Output layer\n",
        "  output = Dense(4860, activation='linear')(decoded)\n",
        "\n",
        "  # Optimizer parameters\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model = Model(input, output)\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss='mse',\n",
        "                metrics=['mse'])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "R_ZSI31I-4zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the tuner\n",
        "tuner = kt.Hyperband(create_ae_optimal,\n",
        "                     objective='val_loss',\n",
        "                     max_epochs=15,\n",
        "                     factor=3,\n",
        "                     seed = 0,\n",
        "                     directory='/content/ae_opt',\n",
        "                     project_name='ae_opt1')"
      ],
      "metadata": {
        "id": "kdUDep51B7_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "#note: takes about 30 minutes to optimize\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=8)\n",
        "tuner.search(train_gen, epochs=100, validation_data=val_gen, verbose=0, shuffle=True, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters (Might throw some warnings regarding the speed of the on_epoch_end method)\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "ISWZG5BNCJ7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting and retraining the model with the best params\n",
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "early_stopping=EarlyStopping(patience=8, verbose=1, min_delta=1e-5)\n",
        "checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "hypermodel.fit(train_gen, epochs=100, verbose=1, validation_data=val_gen, callbacks=[checkpointer, early_stopping])"
      ],
      "metadata": {
        "id": "_lpNSFOpDDJj",
        "outputId": "6758f338-a954-468b-cc13-356c81ef1700",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2781 - mse: 0.2781\n",
            "Epoch 1: val_loss improved from inf to 0.14455, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 13s 34ms/step - loss: 0.2781 - mse: 0.2781 - val_loss: 0.1446 - val_mse: 0.1446\n",
            "Epoch 2/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1315 - mse: 0.1315\n",
            "Epoch 2: val_loss improved from 0.14455 to 0.12123, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.1315 - mse: 0.1315 - val_loss: 0.1212 - val_mse: 0.1212\n",
            "Epoch 3/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1195 - mse: 0.1195\n",
            "Epoch 3: val_loss improved from 0.12123 to 0.11638, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 35ms/step - loss: 0.1195 - mse: 0.1195 - val_loss: 0.1164 - val_mse: 0.1164\n",
            "Epoch 4/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1145 - mse: 0.1145\n",
            "Epoch 4: val_loss improved from 0.11638 to 0.11298, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 13s 36ms/step - loss: 0.1145 - mse: 0.1145 - val_loss: 0.1130 - val_mse: 0.1130\n",
            "Epoch 5/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1106 - mse: 0.1106\n",
            "Epoch 5: val_loss improved from 0.11298 to 0.10974, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.1106 - mse: 0.1106 - val_loss: 0.1097 - val_mse: 0.1097\n",
            "Epoch 6/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1076 - mse: 0.1076\n",
            "Epoch 6: val_loss improved from 0.10974 to 0.10595, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.1076 - mse: 0.1076 - val_loss: 0.1060 - val_mse: 0.1060\n",
            "Epoch 7/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1047 - mse: 0.1047\n",
            "Epoch 7: val_loss improved from 0.10595 to 0.10280, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.1047 - mse: 0.1047 - val_loss: 0.1028 - val_mse: 0.1028\n",
            "Epoch 8/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1017 - mse: 0.1017\n",
            "Epoch 8: val_loss improved from 0.10280 to 0.10038, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.1017 - mse: 0.1017 - val_loss: 0.1004 - val_mse: 0.1004\n",
            "Epoch 9/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0990 - mse: 0.0990\n",
            "Epoch 9: val_loss improved from 0.10038 to 0.09680, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0990 - mse: 0.0990 - val_loss: 0.0968 - val_mse: 0.0968\n",
            "Epoch 10/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0967 - mse: 0.0967\n",
            "Epoch 10: val_loss improved from 0.09680 to 0.09472, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0967 - mse: 0.0967 - val_loss: 0.0947 - val_mse: 0.0947\n",
            "Epoch 11/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0947 - mse: 0.0947\n",
            "Epoch 11: val_loss improved from 0.09472 to 0.09222, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 35ms/step - loss: 0.0947 - mse: 0.0947 - val_loss: 0.0922 - val_mse: 0.0922\n",
            "Epoch 12/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0928 - mse: 0.0928\n",
            "Epoch 12: val_loss improved from 0.09222 to 0.09047, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0905 - val_mse: 0.0905\n",
            "Epoch 13/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0912 - mse: 0.0912\n",
            "Epoch 13: val_loss improved from 0.09047 to 0.08833, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0912 - mse: 0.0912 - val_loss: 0.0883 - val_mse: 0.0883\n",
            "Epoch 14/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0898 - mse: 0.0898\n",
            "Epoch 14: val_loss improved from 0.08833 to 0.08756, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 35ms/step - loss: 0.0898 - mse: 0.0898 - val_loss: 0.0876 - val_mse: 0.0876\n",
            "Epoch 15/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0885 - mse: 0.0885\n",
            "Epoch 15: val_loss improved from 0.08756 to 0.08603, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.0860 - val_mse: 0.0860\n",
            "Epoch 16/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0872 - mse: 0.0872\n",
            "Epoch 16: val_loss improved from 0.08603 to 0.08592, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0872 - mse: 0.0872 - val_loss: 0.0859 - val_mse: 0.0859\n",
            "Epoch 17/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0860 - mse: 0.0860\n",
            "Epoch 17: val_loss improved from 0.08592 to 0.08357, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 35ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 0.0836 - val_mse: 0.0836\n",
            "Epoch 18/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0849 - mse: 0.0849\n",
            "Epoch 18: val_loss improved from 0.08357 to 0.08268, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0849 - mse: 0.0849 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "Epoch 19/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0839 - mse: 0.0839\n",
            "Epoch 19: val_loss improved from 0.08268 to 0.08230, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0839 - mse: 0.0839 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "Epoch 20/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0828 - mse: 0.0828\n",
            "Epoch 20: val_loss improved from 0.08230 to 0.08110, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0828 - mse: 0.0828 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "Epoch 21/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0819 - mse: 0.0819\n",
            "Epoch 21: val_loss improved from 0.08110 to 0.08000, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0819 - mse: 0.0819 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "Epoch 22/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0810 - mse: 0.0810\n",
            "Epoch 22: val_loss improved from 0.08000 to 0.07944, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0810 - mse: 0.0810 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 23/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0803 - mse: 0.0803\n",
            "Epoch 23: val_loss improved from 0.07944 to 0.07917, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0803 - mse: 0.0803 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "Epoch 24/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0796 - mse: 0.0796\n",
            "Epoch 24: val_loss improved from 0.07917 to 0.07814, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0796 - mse: 0.0796 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "Epoch 25/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0789 - mse: 0.0789\n",
            "Epoch 25: val_loss improved from 0.07814 to 0.07781, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0789 - mse: 0.0789 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "Epoch 26/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0783 - mse: 0.0783\n",
            "Epoch 26: val_loss improved from 0.07781 to 0.07743, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0783 - mse: 0.0783 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "Epoch 27/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0777 - mse: 0.0777\n",
            "Epoch 27: val_loss improved from 0.07743 to 0.07664, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0777 - mse: 0.0777 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "Epoch 28/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0772 - mse: 0.0772\n",
            "Epoch 28: val_loss improved from 0.07664 to 0.07634, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0772 - mse: 0.0772 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "Epoch 29/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0767 - mse: 0.0767\n",
            "Epoch 29: val_loss improved from 0.07634 to 0.07600, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0767 - mse: 0.0767 - val_loss: 0.0760 - val_mse: 0.0760\n",
            "Epoch 30/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0762 - mse: 0.0762\n",
            "Epoch 30: val_loss improved from 0.07600 to 0.07581, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0762 - mse: 0.0762 - val_loss: 0.0758 - val_mse: 0.0758\n",
            "Epoch 31/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0757 - mse: 0.0757\n",
            "Epoch 31: val_loss did not improve from 0.07581\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0757 - mse: 0.0757 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "Epoch 32/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0753 - mse: 0.0753\n",
            "Epoch 32: val_loss improved from 0.07581 to 0.07558, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0753 - mse: 0.0753 - val_loss: 0.0756 - val_mse: 0.0756\n",
            "Epoch 33/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0750 - mse: 0.0750\n",
            "Epoch 33: val_loss improved from 0.07558 to 0.07510, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0750 - mse: 0.0750 - val_loss: 0.0751 - val_mse: 0.0751\n",
            "Epoch 34/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0747 - mse: 0.0747\n",
            "Epoch 34: val_loss did not improve from 0.07510\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0747 - mse: 0.0747 - val_loss: 0.0751 - val_mse: 0.0751\n",
            "Epoch 35/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0743 - mse: 0.0743\n",
            "Epoch 35: val_loss improved from 0.07510 to 0.07470, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0743 - mse: 0.0743 - val_loss: 0.0747 - val_mse: 0.0747\n",
            "Epoch 36/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0740 - mse: 0.0740\n",
            "Epoch 36: val_loss improved from 0.07470 to 0.07439, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0740 - mse: 0.0740 - val_loss: 0.0744 - val_mse: 0.0744\n",
            "Epoch 37/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0737 - mse: 0.0737\n",
            "Epoch 37: val_loss improved from 0.07439 to 0.07424, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0737 - mse: 0.0737 - val_loss: 0.0742 - val_mse: 0.0742\n",
            "Epoch 38/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0734 - mse: 0.0734\n",
            "Epoch 38: val_loss did not improve from 0.07424\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0734 - mse: 0.0734 - val_loss: 0.0743 - val_mse: 0.0743\n",
            "Epoch 39/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0731 - mse: 0.0731\n",
            "Epoch 39: val_loss did not improve from 0.07424\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0731 - mse: 0.0731 - val_loss: 0.0743 - val_mse: 0.0743\n",
            "Epoch 40/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0729 - mse: 0.0729\n",
            "Epoch 40: val_loss improved from 0.07424 to 0.07389, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 35ms/step - loss: 0.0729 - mse: 0.0729 - val_loss: 0.0739 - val_mse: 0.0739\n",
            "Epoch 41/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0726 - mse: 0.0726\n",
            "Epoch 41: val_loss did not improve from 0.07389\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0726 - mse: 0.0726 - val_loss: 0.0741 - val_mse: 0.0741\n",
            "Epoch 42/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0725 - mse: 0.0725\n",
            "Epoch 42: val_loss improved from 0.07389 to 0.07387, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0725 - mse: 0.0725 - val_loss: 0.0739 - val_mse: 0.0739\n",
            "Epoch 43/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0722 - mse: 0.0722\n",
            "Epoch 43: val_loss improved from 0.07387 to 0.07364, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0722 - mse: 0.0722 - val_loss: 0.0736 - val_mse: 0.0736\n",
            "Epoch 44/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0719 - mse: 0.0719\n",
            "Epoch 44: val_loss did not improve from 0.07364\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0737 - val_mse: 0.0737\n",
            "Epoch 45/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0718 - mse: 0.0718\n",
            "Epoch 45: val_loss did not improve from 0.07364\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0718 - mse: 0.0718 - val_loss: 0.0738 - val_mse: 0.0738\n",
            "Epoch 46/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0716 - mse: 0.0716\n",
            "Epoch 46: val_loss did not improve from 0.07364\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0716 - mse: 0.0716 - val_loss: 0.0737 - val_mse: 0.0737\n",
            "Epoch 47/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0714 - mse: 0.0714\n",
            "Epoch 47: val_loss improved from 0.07364 to 0.07343, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0714 - mse: 0.0714 - val_loss: 0.0734 - val_mse: 0.0734\n",
            "Epoch 48/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0712 - mse: 0.0712\n",
            "Epoch 48: val_loss did not improve from 0.07343\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0712 - mse: 0.0712 - val_loss: 0.0735 - val_mse: 0.0735\n",
            "Epoch 49/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0710 - mse: 0.0710\n",
            "Epoch 49: val_loss improved from 0.07343 to 0.07323, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0710 - mse: 0.0710 - val_loss: 0.0732 - val_mse: 0.0732\n",
            "Epoch 50/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0709 - mse: 0.0709\n",
            "Epoch 50: val_loss did not improve from 0.07323\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0709 - mse: 0.0709 - val_loss: 0.0732 - val_mse: 0.0732\n",
            "Epoch 51/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0706 - mse: 0.0706\n",
            "Epoch 51: val_loss did not improve from 0.07323\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0706 - mse: 0.0706 - val_loss: 0.0734 - val_mse: 0.0734\n",
            "Epoch 52/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0705 - mse: 0.0705\n",
            "Epoch 52: val_loss improved from 0.07323 to 0.07316, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0705 - mse: 0.0705 - val_loss: 0.0732 - val_mse: 0.0732\n",
            "Epoch 53/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0703 - mse: 0.0703\n",
            "Epoch 53: val_loss did not improve from 0.07316\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0703 - mse: 0.0703 - val_loss: 0.0732 - val_mse: 0.0732\n",
            "Epoch 54/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0702 - mse: 0.0702\n",
            "Epoch 54: val_loss did not improve from 0.07316\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0702 - mse: 0.0702 - val_loss: 0.0734 - val_mse: 0.0734\n",
            "Epoch 55/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0700 - mse: 0.0700\n",
            "Epoch 55: val_loss did not improve from 0.07316\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0700 - mse: 0.0700 - val_loss: 0.0733 - val_mse: 0.0733\n",
            "Epoch 56/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0698 - mse: 0.0698\n",
            "Epoch 56: val_loss improved from 0.07316 to 0.07310, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0698 - mse: 0.0698 - val_loss: 0.0731 - val_mse: 0.0731\n",
            "Epoch 57/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0697 - mse: 0.0697\n",
            "Epoch 57: val_loss improved from 0.07310 to 0.07287, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0697 - mse: 0.0697 - val_loss: 0.0729 - val_mse: 0.0729\n",
            "Epoch 58/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0695 - mse: 0.0695\n",
            "Epoch 58: val_loss improved from 0.07287 to 0.07287, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0695 - mse: 0.0695 - val_loss: 0.0729 - val_mse: 0.0729\n",
            "Epoch 59/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0694 - mse: 0.0694\n",
            "Epoch 59: val_loss did not improve from 0.07287\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0694 - mse: 0.0694 - val_loss: 0.0730 - val_mse: 0.0730\n",
            "Epoch 60/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0692 - mse: 0.0692\n",
            "Epoch 60: val_loss improved from 0.07287 to 0.07285, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0692 - mse: 0.0692 - val_loss: 0.0728 - val_mse: 0.0728\n",
            "Epoch 61/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0691 - mse: 0.0691\n",
            "Epoch 61: val_loss improved from 0.07285 to 0.07281, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0691 - mse: 0.0691 - val_loss: 0.0728 - val_mse: 0.0728\n",
            "Epoch 62/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0690 - mse: 0.0690\n",
            "Epoch 62: val_loss did not improve from 0.07281\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0690 - mse: 0.0690 - val_loss: 0.0729 - val_mse: 0.0729\n",
            "Epoch 63/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0689 - mse: 0.0689\n",
            "Epoch 63: val_loss improved from 0.07281 to 0.07273, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 35ms/step - loss: 0.0689 - mse: 0.0689 - val_loss: 0.0727 - val_mse: 0.0727\n",
            "Epoch 64/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0687 - mse: 0.0687\n",
            "Epoch 64: val_loss did not improve from 0.07273\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0687 - mse: 0.0687 - val_loss: 0.0728 - val_mse: 0.0728\n",
            "Epoch 65/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0686 - mse: 0.0686\n",
            "Epoch 65: val_loss improved from 0.07273 to 0.07267, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0686 - mse: 0.0686 - val_loss: 0.0727 - val_mse: 0.0727\n",
            "Epoch 66/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0685 - mse: 0.0685\n",
            "Epoch 66: val_loss did not improve from 0.07267\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0685 - mse: 0.0685 - val_loss: 0.0727 - val_mse: 0.0727\n",
            "Epoch 67/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0684 - mse: 0.0684\n",
            "Epoch 67: val_loss improved from 0.07267 to 0.07265, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0684 - mse: 0.0684 - val_loss: 0.0727 - val_mse: 0.0727\n",
            "Epoch 68/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0682 - mse: 0.0682\n",
            "Epoch 68: val_loss improved from 0.07265 to 0.07256, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0682 - mse: 0.0682 - val_loss: 0.0726 - val_mse: 0.0726\n",
            "Epoch 69/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0681 - mse: 0.0681\n",
            "Epoch 69: val_loss did not improve from 0.07256\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0681 - mse: 0.0681 - val_loss: 0.0726 - val_mse: 0.0726\n",
            "Epoch 70/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0680 - mse: 0.0680\n",
            "Epoch 70: val_loss did not improve from 0.07256\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0680 - mse: 0.0680 - val_loss: 0.0726 - val_mse: 0.0726\n",
            "Epoch 71/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0679 - mse: 0.0679\n",
            "Epoch 71: val_loss improved from 0.07256 to 0.07255, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0679 - mse: 0.0679 - val_loss: 0.0725 - val_mse: 0.0725\n",
            "Epoch 72/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0678 - mse: 0.0678\n",
            "Epoch 72: val_loss did not improve from 0.07255\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0678 - mse: 0.0678 - val_loss: 0.0726 - val_mse: 0.0726\n",
            "Epoch 73/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0677 - mse: 0.0677\n",
            "Epoch 73: val_loss did not improve from 0.07255\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0677 - mse: 0.0677 - val_loss: 0.0726 - val_mse: 0.0726\n",
            "Epoch 74/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0676 - mse: 0.0676\n",
            "Epoch 74: val_loss did not improve from 0.07255\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0676 - mse: 0.0676 - val_loss: 0.0726 - val_mse: 0.0726\n",
            "Epoch 75/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0675 - mse: 0.0675\n",
            "Epoch 75: val_loss did not improve from 0.07255\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0675 - mse: 0.0675 - val_loss: 0.0727 - val_mse: 0.0727\n",
            "Epoch 76/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0674 - mse: 0.0674\n",
            "Epoch 76: val_loss did not improve from 0.07255\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0674 - mse: 0.0674 - val_loss: 0.0726 - val_mse: 0.0726\n",
            "Epoch 77/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0673 - mse: 0.0673\n",
            "Epoch 77: val_loss did not improve from 0.07255\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0673 - mse: 0.0673 - val_loss: 0.0726 - val_mse: 0.0726\n",
            "Epoch 78/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0672 - mse: 0.0672\n",
            "Epoch 78: val_loss improved from 0.07255 to 0.07253, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0672 - mse: 0.0672 - val_loss: 0.0725 - val_mse: 0.0725\n",
            "Epoch 79/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0671 - mse: 0.0671\n",
            "Epoch 79: val_loss did not improve from 0.07253\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0671 - mse: 0.0671 - val_loss: 0.0728 - val_mse: 0.0728\n",
            "Epoch 80/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0670 - mse: 0.0670\n",
            "Epoch 80: val_loss improved from 0.07253 to 0.07241, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0670 - mse: 0.0670 - val_loss: 0.0724 - val_mse: 0.0724\n",
            "Epoch 81/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0669 - mse: 0.0669\n",
            "Epoch 81: val_loss did not improve from 0.07241\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0669 - mse: 0.0669 - val_loss: 0.0726 - val_mse: 0.0726\n",
            "Epoch 82/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0668 - mse: 0.0668\n",
            "Epoch 82: val_loss did not improve from 0.07241\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0668 - mse: 0.0668 - val_loss: 0.0724 - val_mse: 0.0724\n",
            "Epoch 83/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0668 - mse: 0.0668\n",
            "Epoch 83: val_loss did not improve from 0.07241\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0668 - mse: 0.0668 - val_loss: 0.0727 - val_mse: 0.0727\n",
            "Epoch 84/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0667 - mse: 0.0667\n",
            "Epoch 84: val_loss did not improve from 0.07241\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0667 - mse: 0.0667 - val_loss: 0.0725 - val_mse: 0.0725\n",
            "Epoch 85/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0666 - mse: 0.0666\n",
            "Epoch 85: val_loss did not improve from 0.07241\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0666 - mse: 0.0666 - val_loss: 0.0725 - val_mse: 0.0725\n",
            "Epoch 86/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0665 - mse: 0.0665\n",
            "Epoch 86: val_loss did not improve from 0.07241\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0665 - mse: 0.0665 - val_loss: 0.0725 - val_mse: 0.0725\n",
            "Epoch 87/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0664 - mse: 0.0664\n",
            "Epoch 87: val_loss improved from 0.07241 to 0.07237, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0664 - mse: 0.0664 - val_loss: 0.0724 - val_mse: 0.0724\n",
            "Epoch 88/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0663 - mse: 0.0663\n",
            "Epoch 88: val_loss did not improve from 0.07237\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0663 - mse: 0.0663 - val_loss: 0.0725 - val_mse: 0.0725\n",
            "Epoch 89/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0662 - mse: 0.0662\n",
            "Epoch 89: val_loss improved from 0.07237 to 0.07236, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0662 - mse: 0.0662 - val_loss: 0.0724 - val_mse: 0.0724\n",
            "Epoch 90/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0662 - mse: 0.0662\n",
            "Epoch 90: val_loss did not improve from 0.07236\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0662 - mse: 0.0662 - val_loss: 0.0726 - val_mse: 0.0726\n",
            "Epoch 91/100\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0661 - mse: 0.0661\n",
            "Epoch 91: val_loss did not improve from 0.07236\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0661 - mse: 0.0661 - val_loss: 0.0725 - val_mse: 0.0725\n",
            "Epoch 92/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0660 - mse: 0.0660\n",
            "Epoch 92: val_loss improved from 0.07236 to 0.07234, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.0660 - mse: 0.0660 - val_loss: 0.0723 - val_mse: 0.0723\n",
            "Epoch 93/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0659 - mse: 0.0659\n",
            "Epoch 93: val_loss did not improve from 0.07234\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0659 - mse: 0.0659 - val_loss: 0.0724 - val_mse: 0.0724\n",
            "Epoch 94/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0658 - mse: 0.0658\n",
            "Epoch 94: val_loss did not improve from 0.07234\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0658 - mse: 0.0658 - val_loss: 0.0724 - val_mse: 0.0724\n",
            "Epoch 95/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0658 - mse: 0.0658\n",
            "Epoch 95: val_loss did not improve from 0.07234\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0658 - mse: 0.0658 - val_loss: 0.0725 - val_mse: 0.0725\n",
            "Epoch 96/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0656 - mse: 0.0656\n",
            "Epoch 96: val_loss did not improve from 0.07234\n",
            "349/349 [==============================] - 11s 33ms/step - loss: 0.0656 - mse: 0.0656 - val_loss: 0.0724 - val_mse: 0.0724\n",
            "Epoch 97/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0656 - mse: 0.0656\n",
            "Epoch 97: val_loss did not improve from 0.07234\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0656 - mse: 0.0656 - val_loss: 0.0725 - val_mse: 0.0725\n",
            "Epoch 98/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0655 - mse: 0.0655\n",
            "Epoch 98: val_loss did not improve from 0.07234\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0655 - mse: 0.0655 - val_loss: 0.0724 - val_mse: 0.0724\n",
            "Epoch 99/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0654 - mse: 0.0654\n",
            "Epoch 99: val_loss did not improve from 0.07234\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0654 - mse: 0.0654 - val_loss: 0.0724 - val_mse: 0.0724\n",
            "Epoch 100/100\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0653 - mse: 0.0653\n",
            "Epoch 100: val_loss did not improve from 0.07234\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.0653 - mse: 0.0653 - val_loss: 0.0724 - val_mse: 0.0724\n",
            "Epoch 100: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9e0e425820>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the performance on the test set\n",
        "hypermodel.load_weights('weights1.hdf5')\n",
        "hypermodel.evaluate(test_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOq0ZdtfBjCD",
        "outputId": "4899d389-6916-43a5-97b2-0e5e34924288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "116/116 [==============================] - 2s 16ms/step - loss: 0.0711 - mse: 0.0711\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07114315778017044, 0.07114315778017044]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the optimal hyperparameters\n",
        "for i in range(1,6):\n",
        "  print(f'Units on the {i}. layer:', best_hps.get(f'units_{i}'))\n",
        "  print(f'Dropout on the {i}. layer:', best_hps.get(f'dropout_{i}'))\n",
        "  print(\"\")\n",
        "\n",
        "print('Learning rate:', best_hps.get('learning_rate'))"
      ],
      "metadata": {
        "id": "Lt4OCxTUxb1v",
        "outputId": "27ca5648-60f3-4f3d-9ac4-52560aa9132f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Units on the 1. layer: 2500\n",
            "Dropout on the 1. layer: 0.2\n",
            "\n",
            "Units on the 2. layer: 1125\n",
            "Dropout on the 2. layer: 0.30000000000000004\n",
            "\n",
            "Units on the 3. layer: 350\n",
            "Dropout on the 3. layer: 0.0\n",
            "\n",
            "Units on the 4. layer: 1125\n",
            "Dropout on the 4. layer: 0.1\n",
            "\n",
            "Units on the 5. layer: 1750\n",
            "Dropout on the 5. layer: 0.0\n",
            "\n",
            "Learning rate: 0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the layers\n",
        "hypermodel.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QWuY1lchrS4",
        "outputId": "b5a21142-a426-45f5-cebb-01982df1e2e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4860)]            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 2500)              12152500  \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 2500)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1125)              2813625   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 1125)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 350)               394100    \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 350)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1125)              394875    \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 1125)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1750)              1970500   \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 1750)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 4860)              8509860   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 26,235,460\n",
            "Trainable params: 26,235,460\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keeping only the encoder\n",
        "model2 = Model(inputs=hypermodel.input, outputs=hypermodel.layers[-7].output)\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrTwZhiyhv8Q",
        "outputId": "e50267ea-dda8-45b9-b351-6aac7ae701a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4860)]            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 2500)              12152500  \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 2500)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1125)              2813625   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 1125)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 350)               394100    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,360,225\n",
            "Trainable params: 15,360,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sizes of the new arrays\n",
        "print('Train:', train_feat.shape) \n",
        "print('Validation:', val_feat.shape)\n",
        "print('Test:', test_feat.shape)"
      ],
      "metadata": {
        "id": "OYLjwKunQi-J",
        "outputId": "3844f333-d220-4aaa-edf1-934f5360d616",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (179011, 4860)\n",
            "Validation: (59672, 4860)\n",
            "Test: (59672, 4860)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to redefine the generators for the predictions in order to retain every feature vector in every set, otherwise some of them would be left out because of the batch size.\n",
        "\n",
        "Since the memory can't fit all the data, and using 1 as batch size would be slow, we need to use prime factorization to determine the appropriate batch sizes.\n",
        "\n",
        "The factorizations are:\n",
        "- 179011 = 7 x 107 x 239\n",
        "- 59672 = 2 * 2 * 2 * 7459  "
      ],
      "metadata": {
        "id": "j25X21GObqbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = DataGenerator(train_feat, 107*239)\n",
        "val_gen = DataGenerator(val_feat, 7459)\n",
        "test_gen = DataGenerator(test_feat, 7459)\n",
        "\n",
        "# Generating the new data using the output of the bottleneck layer\n",
        "train_new = model2.predict(train_gen)\n",
        "val_new = model2.predict(val_gen)\n",
        "test_new = model2.predict(test_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqrNk9pKiiHa",
        "outputId": "3df6d12a-09de-4052-dad4-a080c1054605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 5s 651ms/step\n",
            "8/8 [==============================] - 2s 206ms/step\n",
            "8/8 [==============================] - 2s 196ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if the new sets really has the same amount of vectors\n",
        "print(train_new.shape[0] == train_feat.shape[0])\n",
        "print(val_new.shape[0] == val_feat.shape[0])\n",
        "print(test_new.shape[0] == test_feat.shape[0])"
      ],
      "metadata": {
        "id": "ODtUOd0KSBE0",
        "outputId": "006e8d6d-447f-4879-cd7a-afc977800523",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the feature-label set pairs as pickle files onto Google Drive\n",
        "import pickle\n",
        "\n",
        "with open('train.pkl', 'wb') as f:  \n",
        "    pickle.dump([train_new, train_spec], f)\n",
        "\n",
        "with open('val.pkl', 'wb') as f:  \n",
        "    pickle.dump([val_new, val_spec], f)\n",
        "\n",
        "with open('test.pkl', 'wb') as f:  \n",
        "    pickle.dump([test_new, test_spec], f)"
      ],
      "metadata": {
        "id": "UWTKZceejPgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp train.pkl drive/MyDrive/DeepLearning/train.pkl\n",
        "!cp val.pkl drive/MyDrive/DeepLearning/val.pkl\n",
        "!cp test.pkl drive/MyDrive/DeepLearning/test.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lhk7ec0pyg_",
        "outputId": "32cd445d-8708-4ef1-e620-ff5a78e86481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Option 2: Incremental PCA</h3>"
      ],
      "metadata": {
        "id": "q9cD1KX0ajBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "n_comp = 250\n",
        "\n",
        "# note: takes about 10 minutes to run\n",
        "pca = IncrementalPCA(n_components=n_comp, batch_size=1024)\n",
        "pca.fit(train_feat)\n",
        "train_feat = pca.transform(train_feat)\n",
        "val_feat = pca.transform(val_feat)\n",
        "test_feat = pca.transform(test_feat)"
      ],
      "metadata": {
        "id": "AARr0wkEaqfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the new data\n",
        "\n",
        "with open('train_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([train_feat, train_spec], f)\n",
        "\n",
        "with open('val_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([val_feat, val_spec], f)\n",
        "\n",
        "with open('test_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([test_feat, test_spec], f)\n",
        "\n",
        "!cp train_v2.pkl drive/MyDrive/DeepLearning/train_v2.pkl\n",
        "!cp val_v2.pkl drive/MyDrive/DeepLearning/val_v2.pkl\n",
        "!cp test_v2.pkl drive/MyDrive/DeepLearning/test_v2.pkl"
      ],
      "metadata": {
        "id": "QbzPyjaBdZeu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}