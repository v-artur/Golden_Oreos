{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMxBAS/TAgaWg2j581pX7+H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v-artur/Golden_Oreos/blob/main/speaker_indep_dim_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtaining the data"
      ],
      "metadata": {
        "id": "QRznZ-paPxRN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3F36vS9-11r",
        "outputId": "629ac85f-f004-45c5-b709-d0fd2ff5715a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-09 19:19:18--  https://docs.google.com/uc?export=download&confirm=t&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.194.102, 172.217.194.101, 172.217.194.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.194.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ekldknnmoi77aju6kkcvupjd6dds9qvs/1670613525000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=46180031-facb-473b-9576-36c30d949f2a [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-09 19:19:18--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ekldknnmoi77aju6kkcvupjd6dds9qvs/1670613525000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=46180031-facb-473b-9576-36c30d949f2a\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 74.125.24.132, 2404:6800:4003:c03::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|74.125.24.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2141405093 (2.0G) [application/x-zip-compressed]\n",
            "Saving to: ‘features.zip’\n",
            "\n",
            "features.zip        100%[===================>]   1.99G  59.8MB/s    in 40s     \n",
            "\n",
            "2022-12-09 19:19:59 (51.5 MB/s) - ‘features.zip’ saved [2141405093/2141405093]\n",
            "\n",
            "--2022-12-09 19:20:01--  https://docs.google.com/uc?export=download&confirm=&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.12.101, 142.251.12.138, 142.251.12.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.12.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/asftctr7lp78c7ofmtl15g3a3q6eocb5/1670613600000/17895932938140350971/*/1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80?e=download&uuid=6e72837c-05fd-462a-87c5-7948626ca3e1 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-09 19:20:02--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/asftctr7lp78c7ofmtl15g3a3q6eocb5/1670613600000/17895932938140350971/*/1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80?e=download&uuid=6e72837c-05fd-462a-87c5-7948626ca3e1\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 74.125.24.132, 2404:6800:4003:c03::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|74.125.24.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9238 (9.0K) [application/x-zip-compressed]\n",
            "Saving to: ‘subject_channels.zip’\n",
            "\n",
            "subject_channels.zi 100%[===================>]   9.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-09 19:20:02 (111 MB/s) - ‘subject_channels.zip’ saved [9238/9238]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#features\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\" -O features.zip && rm -rf /tmp/cookies.txt\n",
        "#original electrode names\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80\" -O subject_channels.zip && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# Data extraction\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/features.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/features\")\n",
        "zip_ref.close()\n",
        "\n",
        "# Electrode name extraction\n",
        "zip_ref = zipfile.ZipFile(\"/content/subject_channels.zip\", 'r')\n",
        "zip_ref.extractall(\"/content\")\n",
        "zip_ref.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Preparations and needed functions</h3>\n",
        "\n"
      ],
      "metadata": {
        "id": "QdxGeq3EP2hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our plan is to transform every feature vector into a larger dimensional feature vector,  which contains all the different electrode names across all the subjects."
      ],
      "metadata": {
        "id": "WgUblU51bfZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# \n",
        "\n",
        "# Getting the different electrode names\n",
        "all_electrodes = set()\n",
        "\n",
        "feat_path = \"/content/features\"\n",
        "\n",
        "for subject in ['01','02','03','04','05','06','07','08','09','10']:\n",
        "  elecs = np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy'))\n",
        "  elecs = set(elecs)\n",
        "  all_electrodes = all_electrodes.union(elecs)\n",
        "\n",
        "# We will use this list's indexes to correspond to the feature matrices gained from the subjects\n",
        "all_electrodes = list(all_electrodes) \n",
        "\n",
        "print('Number of different electrodes:', len(all_electrodes))"
      ],
      "metadata": {
        "id": "annt8G29nDNS",
        "outputId": "06a23a92-f9ea-4f6f-b6f2-0260d6271c91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of different electrodes: 4860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for making every feature vector into a 4860 dimensional vector\n",
        "\n",
        "def dim_adjust(data, feature_names):\n",
        "  #create a new matrix with zeros, insert the values into the columns\n",
        "  #which correspond to the subject's feature names, then concatenate them\n",
        "\n",
        "  new_matrix = np.zeros((data.shape[0],len(all_electrodes)))\n",
        "  for column in range(data.shape[1]):\n",
        "    insert_index = all_electrodes.index(feature_names[column])\n",
        "    new_matrix[:,insert_index] = data[:,column]\n",
        "    \n",
        "  return new_matrix\n"
      ],
      "metadata": {
        "id": "d8o2edSn_CkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making the iterated test, validation and test sets"
      ],
      "metadata": {
        "id": "bwJu3UnzP8ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate the train, val and test features and mel spectrograms\n",
        "\n",
        "def generate_features_and_spec():\n",
        "  #Base arrays\n",
        "  train_feat = np.empty((0, len(all_electrodes)))\n",
        "  val_feat = np.empty((0, len(all_electrodes)))\n",
        "  test_feat = np.empty((0, len(all_electrodes)))\n",
        "\n",
        "  train_spec = np.empty((0, 23))\n",
        "  val_spec = np.empty((0, 23))\n",
        "  test_spec = np.empty((0, 23))\n",
        "\n",
        "  for index, subject in enumerate(['01','02','03','04','05','06','07','08','09','10']):\n",
        "    #loading the features, feature names and mel spectrogram of the subject\n",
        "    data = np.load(os.path.join(feat_path,f'sub-{subject}_feat.npy'))\n",
        "    feature_names = np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy'))\n",
        "    spectrogram = np.load(os.path.join(feat_path,f'sub-{subject}_spec.npy'))\n",
        "\n",
        "    #splittig the features and the labels into 5 parts\n",
        "    feat_splits = np.array_split(data, 5)\n",
        "    spec_splits = np.array_split(spectrogram, 5)\n",
        "\n",
        "    #making the train, val or test arrays for the subject using a 60-20-20 ratio\n",
        "    #because of the \"index\" changing, the splitting position will iterate with each subject as well\n",
        "    subject_train_feat = np.vstack((feat_splits[index % 5],feat_splits[(index+1) % 5],feat_splits[(index+2) % 5]))\n",
        "    subject_train_spec = np.vstack((spec_splits[index % 5],spec_splits[(index+1) % 5],spec_splits[(index+2) % 5]))\n",
        "\n",
        "    subject_val_feat = feat_splits[(index+3) % 5]\n",
        "    subject_val_spec = spec_splits[(index+3) % 5]\n",
        "\n",
        "    subject_test_feat = feat_splits[(index+4) % 5]\n",
        "    subject_test_spec = spec_splits[(index+4) % 5]\n",
        "\n",
        "    #concatenating the dimensionality-adjusted features with the pre-existing feature set\n",
        "    train_feat = np.concatenate((train_feat, dim_adjust(subject_train_feat, feature_names)))\n",
        "    val_feat = np.concatenate((val_feat, dim_adjust(subject_val_feat, feature_names)))\n",
        "    test_feat = np.concatenate((test_feat, dim_adjust(subject_test_feat, feature_names)))\n",
        "\n",
        "    #concatenating the appropriate arrays with the pre-existing train, val or test labels\n",
        "    train_spec = np.concatenate((train_spec, subject_train_spec))\n",
        "    val_spec = np.concatenate((val_spec, subject_val_spec))\n",
        "    test_spec = np.concatenate((test_spec, subject_test_spec))\n",
        "\n",
        "  return train_feat, train_spec, val_feat, val_spec, test_feat, test_spec \n",
        "\n",
        "# Generating the data\n",
        "train_feat, train_spec, val_feat, val_spec, test_feat, test_spec = generate_features_and_spec()"
      ],
      "metadata": {
        "id": "3sBGVYAoc7LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Scaling the data \n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_feat)\n",
        "train_feat = scaler.transform(train_feat)\n",
        "val_feat = scaler.transform(val_feat)\n",
        "test_feat = scaler.transform(test_feat)"
      ],
      "metadata": {
        "id": "JC2LF6YOfflo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality reduction"
      ],
      "metadata": {
        "id": "MRV6pzVbmNWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Option 1: Tuned AutoEncoder</h3>"
      ],
      "metadata": {
        "id": "NV5SeqERySxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "dVwyhAYq-4O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the data generator\n",
        "from tensorflow.keras.utils import Sequence, set_random_seed\n",
        "\n",
        "set_random_seed(1234)\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    # Initialization\n",
        "    def __init__(self, data, batch_size=32, dim=len(all_electrodes), shuffle=True):\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.data = data\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Shows the number of batches per epoch\n",
        "        return int(np.floor(self.data.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate one batch of data\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        batch = np.empty((self.batch_size, self.dim))\n",
        "        for index1, elem in enumerate(indexes):\n",
        "          batch[index1] = self.data[index1]\n",
        "\n",
        "        return batch, batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updating the index after each epoch\n",
        "        self.indexes = np.arange(self.data.shape[0])\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "# Creating the generators for training\n",
        "# According to our early experiments, batch sizes of 256 or 512 proved to be the best,\n",
        "# but to shorten the amount time for hyperparameter tuning, we set it for 512\n",
        "train_gen = DataGenerator(train_feat, 512)\n",
        "val_gen = DataGenerator(val_feat, 512)"
      ],
      "metadata": {
        "id": "xQ8U-ZLCCYTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "import keras_tuner as kt\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# Creating the hyperparameter tuner function\n",
        "# The AE has 5 Dense layers besides the output, and the 3rd one is the bottleneck layer\n",
        "def create_ae_optimal(hp):\n",
        "  input = Input(shape=(4860))\n",
        "  \n",
        "  hp_units_1 = hp.Int('units_1', min_value=1500, max_value=2500, step=250)\n",
        "  encoded = Dense(units=hp_units_1, activation=\"relu\", kernel_initializer='HeNormal')(input)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_1', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(encoded)\n",
        "\n",
        "  hp_units_2 = hp.Int('units_2', min_value=750, max_value=1250, step=125)\n",
        "  encoded = Dense(units=hp_units_2, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_2', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(encoded)\n",
        "\n",
        "  hp_units_3 = hp.Int('units_3', min_value=350, max_value=650, step=50)\n",
        "  encoded = Dense(units=hp_units_3, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_3', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(encoded)\n",
        "\n",
        "  hp_units_4 = hp.Int('units_4', min_value=750, max_value=1250, step=125)\n",
        "  decoded = Dense(units=hp_units_4, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  decoded = Dropout(rate=hp.Float('dropout_4', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(decoded)\n",
        "\n",
        "  hp_units_5 = hp.Int('units_5', min_value=1500, max_value=2500, step=250)\n",
        "  decoded = Dense(units=hp_units_5, activation=\"relu\", kernel_initializer='HeNormal')(decoded)\n",
        "  decoded = Dropout(rate=hp.Float('dropout_5', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1))(decoded)\n",
        "\n",
        "  output = Dense(4860, activation='linear')(decoded)\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  hp_momentum = hp.Choice('momentum', values=[0.9, 0.95])\n",
        "\n",
        "  model = Model(input, output)\n",
        "\n",
        "  # Our early experiments showed that SGD is slightly better here than ADAM\n",
        "  model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=hp_learning_rate, momentum=hp_momentum),\n",
        "                loss='mse',\n",
        "                metrics=['mse'])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "R_ZSI31I-4zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the tuner\n",
        "tuner = kt.Hyperband(create_ae_optimal,\n",
        "                     objective='val_loss',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='/content/ae_opt',\n",
        "                     project_name='ae_opt1')"
      ],
      "metadata": {
        "id": "kdUDep51B7_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "#note: takes about 20 minutes to optimize\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=10)\n",
        "tuner.search(train_gen, epochs=100, validation_data=val_gen, verbose=0, shuffle=True, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "ISWZG5BNCJ7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting and retraining the model with the best params\n",
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "early_stopping=EarlyStopping(patience=10, verbose=1, min_delta=1e-5)\n",
        "checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "hypermodel.fit(train_gen, epochs=100, verbose=1, validation_data=val_gen, callbacks=[checkpointer, early_stopping])"
      ],
      "metadata": {
        "id": "_lpNSFOpDDJj",
        "outputId": "eb8a5b2d-a3d2-4b25-eaba-295af169b44f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.7319 - mse: 0.7319\n",
            "Epoch 1: val_loss improved from inf to 0.58757, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 30ms/step - loss: 0.7311 - mse: 0.7311 - val_loss: 0.5876 - val_mse: 0.5876\n",
            "Epoch 2/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.5530 - mse: 0.5530\n",
            "Epoch 2: val_loss improved from 0.58757 to 0.52517, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.5529 - mse: 0.5529 - val_loss: 0.5252 - val_mse: 0.5252\n",
            "Epoch 3/200\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.4855 - mse: 0.4855\n",
            "Epoch 3: val_loss improved from 0.52517 to 0.45010, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 30ms/step - loss: 0.4855 - mse: 0.4855 - val_loss: 0.4501 - val_mse: 0.4501\n",
            "Epoch 4/200\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.4276 - mse: 0.4276\n",
            "Epoch 4: val_loss improved from 0.45010 to 0.39040, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 12s 34ms/step - loss: 0.4276 - mse: 0.4276 - val_loss: 0.3904 - val_mse: 0.3904\n",
            "Epoch 5/200\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.3848 - mse: 0.3848\n",
            "Epoch 5: val_loss improved from 0.39040 to 0.35126, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 32ms/step - loss: 0.3848 - mse: 0.3848 - val_loss: 0.3513 - val_mse: 0.3513\n",
            "Epoch 6/200\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.3536 - mse: 0.3536\n",
            "Epoch 6: val_loss improved from 0.35126 to 0.32216, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.3535 - mse: 0.3535 - val_loss: 0.3222 - val_mse: 0.3222\n",
            "Epoch 7/200\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.3308 - mse: 0.3308\n",
            "Epoch 7: val_loss improved from 0.32216 to 0.30219, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.3308 - mse: 0.3308 - val_loss: 0.3022 - val_mse: 0.3022\n",
            "Epoch 8/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.3145 - mse: 0.3145\n",
            "Epoch 8: val_loss improved from 0.30219 to 0.29323, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 30ms/step - loss: 0.3145 - mse: 0.3145 - val_loss: 0.2932 - val_mse: 0.2932\n",
            "Epoch 9/200\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.3024 - mse: 0.3024\n",
            "Epoch 9: val_loss improved from 0.29323 to 0.28118, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.3024 - mse: 0.3024 - val_loss: 0.2812 - val_mse: 0.2812\n",
            "Epoch 10/200\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.2931 - mse: 0.2931\n",
            "Epoch 10: val_loss improved from 0.28118 to 0.27639, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 30ms/step - loss: 0.2931 - mse: 0.2931 - val_loss: 0.2764 - val_mse: 0.2764\n",
            "Epoch 11/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2855 - mse: 0.2855\n",
            "Epoch 11: val_loss improved from 0.27639 to 0.27434, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.2855 - mse: 0.2855 - val_loss: 0.2743 - val_mse: 0.2743\n",
            "Epoch 12/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2789 - mse: 0.2789\n",
            "Epoch 12: val_loss improved from 0.27434 to 0.26946, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 10s 30ms/step - loss: 0.2789 - mse: 0.2789 - val_loss: 0.2695 - val_mse: 0.2695\n",
            "Epoch 13/200\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2730 - mse: 0.2730\n",
            "Epoch 13: val_loss improved from 0.26946 to 0.26932, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 30ms/step - loss: 0.2730 - mse: 0.2730 - val_loss: 0.2693 - val_mse: 0.2693\n",
            "Epoch 14/200\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2678 - mse: 0.2678\n",
            "Epoch 14: val_loss improved from 0.26932 to 0.26617, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 30ms/step - loss: 0.2678 - mse: 0.2678 - val_loss: 0.2662 - val_mse: 0.2662\n",
            "Epoch 15/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2629 - mse: 0.2629\n",
            "Epoch 15: val_loss did not improve from 0.26617\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.2629 - mse: 0.2629 - val_loss: 0.2668 - val_mse: 0.2668\n",
            "Epoch 16/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2582 - mse: 0.2582\n",
            "Epoch 16: val_loss improved from 0.26617 to 0.26377, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 30ms/step - loss: 0.2582 - mse: 0.2582 - val_loss: 0.2638 - val_mse: 0.2638\n",
            "Epoch 17/200\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2537 - mse: 0.2537\n",
            "Epoch 17: val_loss improved from 0.26377 to 0.26358, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 10s 30ms/step - loss: 0.2537 - mse: 0.2537 - val_loss: 0.2636 - val_mse: 0.2636\n",
            "Epoch 18/200\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2490 - mse: 0.2490\n",
            "Epoch 18: val_loss improved from 0.26358 to 0.26146, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 30ms/step - loss: 0.2490 - mse: 0.2490 - val_loss: 0.2615 - val_mse: 0.2615\n",
            "Epoch 19/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2446 - mse: 0.2446\n",
            "Epoch 19: val_loss did not improve from 0.26146\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.2446 - mse: 0.2446 - val_loss: 0.2621 - val_mse: 0.2621\n",
            "Epoch 20/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2401 - mse: 0.2401\n",
            "Epoch 20: val_loss improved from 0.26146 to 0.26014, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 10s 30ms/step - loss: 0.2401 - mse: 0.2401 - val_loss: 0.2601 - val_mse: 0.2601\n",
            "Epoch 21/200\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.2356 - mse: 0.2356\n",
            "Epoch 21: val_loss did not improve from 0.26014\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.2356 - mse: 0.2356 - val_loss: 0.2625 - val_mse: 0.2625\n",
            "Epoch 22/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2310 - mse: 0.2310\n",
            "Epoch 22: val_loss did not improve from 0.26014\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.2310 - mse: 0.2310 - val_loss: 0.2623 - val_mse: 0.2623\n",
            "Epoch 23/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2266 - mse: 0.2266\n",
            "Epoch 23: val_loss improved from 0.26014 to 0.26004, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 10s 30ms/step - loss: 0.2265 - mse: 0.2265 - val_loss: 0.2600 - val_mse: 0.2600\n",
            "Epoch 24/200\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2221 - mse: 0.2221\n",
            "Epoch 24: val_loss did not improve from 0.26004\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.2221 - mse: 0.2221 - val_loss: 0.2606 - val_mse: 0.2606\n",
            "Epoch 25/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.2179 - mse: 0.2179\n",
            "Epoch 25: val_loss did not improve from 0.26004\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.2179 - mse: 0.2179 - val_loss: 0.2615 - val_mse: 0.2615\n",
            "Epoch 26/200\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.2136 - mse: 0.2136\n",
            "Epoch 26: val_loss did not improve from 0.26004\n",
            "349/349 [==============================] - 10s 30ms/step - loss: 0.2136 - mse: 0.2136 - val_loss: 0.2610 - val_mse: 0.2610\n",
            "Epoch 27/200\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.2096 - mse: 0.2096\n",
            "Epoch 27: val_loss did not improve from 0.26004\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.2096 - mse: 0.2096 - val_loss: 0.2615 - val_mse: 0.2615\n",
            "Epoch 28/200\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.2057 - mse: 0.2057\n",
            "Epoch 28: val_loss did not improve from 0.26004\n",
            "349/349 [==============================] - 10s 30ms/step - loss: 0.2057 - mse: 0.2057 - val_loss: 0.2612 - val_mse: 0.2612\n",
            "Epoch 29/200\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.2021 - mse: 0.2021\n",
            "Epoch 29: val_loss did not improve from 0.26004\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.2021 - mse: 0.2021 - val_loss: 0.2602 - val_mse: 0.2602\n",
            "Epoch 30/200\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1983 - mse: 0.1983\n",
            "Epoch 30: val_loss did not improve from 0.26004\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.1983 - mse: 0.1983 - val_loss: 0.2607 - val_mse: 0.2607\n",
            "Epoch 31/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1948 - mse: 0.1948\n",
            "Epoch 31: val_loss did not improve from 0.26004\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.1948 - mse: 0.1948 - val_loss: 0.2610 - val_mse: 0.2610\n",
            "Epoch 32/200\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.1914 - mse: 0.1914\n",
            "Epoch 32: val_loss improved from 0.26004 to 0.25968, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 11s 30ms/step - loss: 0.1914 - mse: 0.1914 - val_loss: 0.2597 - val_mse: 0.2597\n",
            "Epoch 33/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1879 - mse: 0.1879\n",
            "Epoch 33: val_loss did not improve from 0.25968\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.1879 - mse: 0.1879 - val_loss: 0.2614 - val_mse: 0.2614\n",
            "Epoch 34/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1849 - mse: 0.1849\n",
            "Epoch 34: val_loss did not improve from 0.25968\n",
            "349/349 [==============================] - 11s 30ms/step - loss: 0.1849 - mse: 0.1849 - val_loss: 0.2616 - val_mse: 0.2616\n",
            "Epoch 35/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1818 - mse: 0.1818\n",
            "Epoch 35: val_loss did not improve from 0.25968\n",
            "349/349 [==============================] - 10s 30ms/step - loss: 0.1818 - mse: 0.1818 - val_loss: 0.2597 - val_mse: 0.2597\n",
            "Epoch 36/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1787 - mse: 0.1787\n",
            "Epoch 36: val_loss did not improve from 0.25968\n",
            "349/349 [==============================] - 11s 31ms/step - loss: 0.1787 - mse: 0.1787 - val_loss: 0.2604 - val_mse: 0.2604\n",
            "Epoch 37/200\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.1758 - mse: 0.1758\n",
            "Epoch 37: val_loss did not improve from 0.25968\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.1758 - mse: 0.1758 - val_loss: 0.2615 - val_mse: 0.2615\n",
            "Epoch 38/200\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.1731 - mse: 0.1731\n",
            "Epoch 38: val_loss did not improve from 0.25968\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.1731 - mse: 0.1731 - val_loss: 0.2609 - val_mse: 0.2609\n",
            "Epoch 39/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1704 - mse: 0.1704\n",
            "Epoch 39: val_loss did not improve from 0.25968\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.1704 - mse: 0.1704 - val_loss: 0.2622 - val_mse: 0.2622\n",
            "Epoch 40/200\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1678 - mse: 0.1678\n",
            "Epoch 40: val_loss did not improve from 0.25968\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.1678 - mse: 0.1678 - val_loss: 0.2606 - val_mse: 0.2606\n",
            "Epoch 41/200\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.1654 - mse: 0.1654\n",
            "Epoch 41: val_loss did not improve from 0.25968\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.1654 - mse: 0.1654 - val_loss: 0.2614 - val_mse: 0.2614\n",
            "Epoch 42/200\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.1628 - mse: 0.1628\n",
            "Epoch 42: val_loss did not improve from 0.25968\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.1628 - mse: 0.1628 - val_loss: 0.2624 - val_mse: 0.2624\n",
            "Epoch 42: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f65b22b6940>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading back the best weights and checking the layers\n",
        "hypermodel.load_weights('weights1.hdf5')\n",
        "hypermodel.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QWuY1lchrS4",
        "outputId": "ce6092ca-61a1-4db9-cc4a-f70787e220cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4860)]            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 2000)              9722000   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 2000)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 875)               1750875   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 875)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 450)               394200    \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 450)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1000)              451000    \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 2000)              2002000   \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 2000)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 4860)              9724860   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,044,935\n",
            "Trainable params: 24,044,935\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keeping only the encoder\n",
        "model2 = Model(inputs=hypermodel.input, outputs=hypermodel.layers[-7].output)\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrTwZhiyhv8Q",
        "outputId": "11f4d41e-81ed-4a5a-9493-02a3e181c933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4860)]            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 2000)              9722000   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 2000)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 875)               1750875   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 875)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 450)               394200    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,867,075\n",
            "Trainable params: 11,867,075\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sizes of the new arrays\n",
        "print('Train:', train_feat.shape) \n",
        "print('Validation:', val_feat.shape)\n",
        "print('Test:', test_feat.shape)"
      ],
      "metadata": {
        "id": "OYLjwKunQi-J",
        "outputId": "fdd1bfcf-d86a-401f-92c7-69f9a402908b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (179011, 4860)\n",
            "Validation: (59672, 4860)\n",
            "Test: (59672, 4860)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to redefine the generators for the predictions in order to retain every feature vector in every set, otherwise some of them would be left out because of the batch size.\n",
        "\n",
        "Since the memory can't fit all the data, and using 1 as batch size would be slow, we need to use prime factorization to determine the appropriate batch sizes.\n",
        "\n",
        "The factorizations are:\n",
        "- 179011 = 7 x 107 x 239\n",
        "- 59672 = 8 * 7459  "
      ],
      "metadata": {
        "id": "j25X21GObqbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = DataGenerator(train_feat, 107*239)\n",
        "val_gen = DataGenerator(val_feat, 7459)\n",
        "test_gen = DataGenerator(test_feat, 7459)\n",
        "\n",
        "# Generating the new data using the output of the bottleneck layer\n",
        "train_new = model2.predict(train_gen)\n",
        "val_new = model2.predict(val_gen)\n",
        "test_new = model2.predict(test_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqrNk9pKiiHa",
        "outputId": "d2e037ab-d1eb-4ec2-ab01-4932264a5c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 5s 739ms/step\n",
            "8/8 [==============================] - 2s 187ms/step\n",
            "8/8 [==============================] - 2s 188ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if the new sets really has the same amount of vectors\n",
        "print(train_new.shape[0] == train_feat.shape[0])\n",
        "print(val_new.shape[0] == val_feat.shape[0])\n",
        "print(test_new.shape[0] == test_feat.shape[0])"
      ],
      "metadata": {
        "id": "ODtUOd0KSBE0",
        "outputId": "9ca432b7-b10e-4701-9241-176a1687f2e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the feature-label set pairs as pickle files onto Google Drive\n",
        "import pickle\n",
        "\n",
        "with open('train.pkl', 'wb') as f:  \n",
        "    pickle.dump([train_new, train_spec], f)\n",
        "\n",
        "with open('val.pkl', 'wb') as f:  \n",
        "    pickle.dump([val_new, val_spec], f)\n",
        "\n",
        "with open('test.pkl', 'wb') as f:  \n",
        "    pickle.dump([test_new, test_spec], f)"
      ],
      "metadata": {
        "id": "UWTKZceejPgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp train.pkl drive/MyDrive/DeepLearning/train.pkl\n",
        "!cp val.pkl drive/MyDrive/DeepLearning/val.pkl\n",
        "!cp test.pkl drive/MyDrive/DeepLearning/test.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lhk7ec0pyg_",
        "outputId": "c5494f86-4208-420e-b326-c2f451114a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Option 2: Incremental PCA</h3>"
      ],
      "metadata": {
        "id": "q9cD1KX0ajBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "n_comp = 250\n",
        "\n",
        "# note: takes about 10 minutes to run\n",
        "pca = IncrementalPCA(n_components=n_comp, batch_size=1024)\n",
        "pca.fit(train_feat)\n",
        "train_feat = pca.transform(train_feat)\n",
        "val_feat = pca.transform(val_feat)\n",
        "test_feat = pca.transform(test_feat)"
      ],
      "metadata": {
        "id": "AARr0wkEaqfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the new data\n",
        "\n",
        "with open('train_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([train_feat, train_spec], f)\n",
        "\n",
        "with open('val_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([val_feat, val_spec], f)\n",
        "\n",
        "with open('test_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([test_feat, test_spec], f)\n",
        "\n",
        "!cp train_v2.pkl drive/MyDrive/DeepLearning/train_v2.pkl\n",
        "!cp val_v2.pkl drive/MyDrive/DeepLearning/val_v2.pkl\n",
        "!cp test_v2.pkl drive/MyDrive/DeepLearning/test_v2.pkl"
      ],
      "metadata": {
        "id": "QbzPyjaBdZeu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}