{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMmBhztlu0HFMs09J24pOqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v-artur/Golden_Oreos/blob/main/speaker_indep_data_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Obtaining the data</h2>"
      ],
      "metadata": {
        "id": "QRznZ-paPxRN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3F36vS9-11r",
        "outputId": "efc2451d-187b-4f02-a9c7-711df983b0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-08 16:13:54--  https://docs.google.com/uc?export=download&confirm=t&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.10.113, 142.251.10.100, 142.251.10.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.10.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/o8mg10p5mfvljobpqlevlb2ujqk6e4rq/1670516025000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=b617cba5-5688-4bdf-82ff-466c9627976a [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-08 16:13:55--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/o8mg10p5mfvljobpqlevlb2ujqk6e4rq/1670516025000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=b617cba5-5688-4bdf-82ff-466c9627976a\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 74.125.24.132, 2404:6800:4003:c03::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|74.125.24.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2141405093 (2.0G) [application/x-zip-compressed]\n",
            "Saving to: ‘features.zip’\n",
            "\n",
            "features.zip        100%[===================>]   1.99G   241MB/s    in 8.2s    \n",
            "\n",
            "2022-12-08 16:14:05 (249 MB/s) - ‘features.zip’ saved [2141405093/2141405093]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\" -O features.zip && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# Data extraction\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/features.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/features\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Preparations and needed functions</h2>"
      ],
      "metadata": {
        "id": "QdxGeq3EP2hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "#setting the path\n",
        "feat_path = r'/content/features'\n",
        "\n",
        "# Counting how many different features there are\n",
        "all_electrodes = set()\n",
        "for subject in ['01','02','03','04','05','06','07','08','09','10']:\n",
        "  list1 = set(np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy')).tolist())\n",
        "  all_electrodes = all_electrodes.union(list1)  \n",
        "\n",
        "print('Number of different features:', len(all_electrodes))\n",
        "\n",
        "#we will use this list's indexes to correspond to the feature matrices\n",
        "all_electrodes = list(all_electrodes) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CckK8PHR_BF9",
        "outputId": "7c74d7da-74a2-4003-a6fa-a012716bf451"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of different features: 4860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for making every feature vector into a 4860 dimensional vector\n",
        "\n",
        "def dim_adjust(data, feature_names):\n",
        "  #creating an empty array with appropriate length\n",
        "  feat_matrix = np.empty((0,len(all_electrodes)))\n",
        "\n",
        "  #create a new matrix with zeros, insert the values into the columns\n",
        "  #which correspond to the subject's feature names, then concatenate them\n",
        "  new_matrix = np.zeros((data.shape[0],len(all_electrodes)))\n",
        "  for column in range(data.shape[1]):\n",
        "    insert_index = all_electrodes.index(feature_names[column])\n",
        "    new_matrix[:,insert_index] = data[:,column]\n",
        "  feat_matrix = np.concatenate((feat_matrix, new_matrix), axis=0)\n",
        "    \n",
        "  return feat_matrix\n"
      ],
      "metadata": {
        "id": "d8o2edSn_CkY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Making the iterated test, validation and test sets</h2>"
      ],
      "metadata": {
        "id": "bwJu3UnzP8ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_features_and_spec():\n",
        "  #Base arrays\n",
        "  train_feat = np.empty((0, len(all_electrodes)))\n",
        "  val_feat = np.empty((0, len(all_electrodes)))\n",
        "  test_feat = np.empty((0, len(all_electrodes)))\n",
        "\n",
        "  train_spec = np.empty((0, 23))\n",
        "  val_spec = np.empty((0, 23))\n",
        "  test_spec = np.empty((0, 23))\n",
        "\n",
        "  for index, subject in enumerate(['01','02','03','04','05','06','07','08','09','10']):\n",
        "    #loading the features, feature names and mel spectrogram of the subject\n",
        "    data = np.load(os.path.join(feat_path,f'sub-{subject}_feat.npy'))\n",
        "    feature_names = np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy'))\n",
        "    spectrogram = np.load(os.path.join(feat_path,f'sub-{subject}_spec.npy'))\n",
        "\n",
        "    #splittig the features and the labels into 5 parts\n",
        "    feat_splits = np.array_split(data, 5)\n",
        "    spec_splits = np.array_split(spectrogram, 5)\n",
        "\n",
        "    #making the train, val or test arrays for the subject using a 60-20-20 ratio\n",
        "    #because of the \"index\" changing, the splitting position will iterate with each subject as well\n",
        "    subject_train_feat = np.vstack((feat_splits[index % 5],feat_splits[(index+1) % 5],feat_splits[(index+2) % 5]))\n",
        "    subject_train_spec = np.vstack((spec_splits[index % 5],spec_splits[(index+1) % 5],spec_splits[(index+2) % 5]))\n",
        "\n",
        "    subject_val_feat = feat_splits[(index+3) % 5]\n",
        "    subject_val_spec = spec_splits[(index+3) % 5]\n",
        "\n",
        "    subject_test_feat = feat_splits[(index+4) % 5]\n",
        "    subject_test_spec = spec_splits[(index+4) % 5]\n",
        "\n",
        "    #concatenating the dimensionality-adjusted features with the pre-existing feature set\n",
        "    train_feat = np.concatenate((train_feat, dim_adjust(subject_train_feat, feature_names)))\n",
        "    val_feat = np.concatenate((val_feat, dim_adjust(subject_val_feat, feature_names)))\n",
        "    test_feat = np.concatenate((test_feat, dim_adjust(subject_test_feat, feature_names)))\n",
        "\n",
        "    #concatenating the appropriate arrays with the pre-existing train, val or test labels\n",
        "    train_spec = np.concatenate((train_spec, subject_train_spec))\n",
        "    val_spec = np.concatenate((val_spec, subject_val_spec))\n",
        "    test_spec = np.concatenate((test_spec, subject_test_spec))\n",
        "\n",
        "  return train_feat, train_spec, val_feat, val_spec, test_feat, test_spec \n",
        "\n",
        "# Generating the data\n",
        "train_feat, train_spec, val_feat, val_spec, test_feat, test_spec = generate_features_and_spec()"
      ],
      "metadata": {
        "id": "3sBGVYAoc7LA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Scaling the data \n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_feat)\n",
        "train_feat = scaler.transform(train_feat)\n",
        "val_feat = scaler.transform(val_feat)\n",
        "test_feat = scaler.transform(test_feat)"
      ],
      "metadata": {
        "id": "JC2LF6YOfflo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Tuning the AutoEncoder for dimensionality reduction</h2>"
      ],
      "metadata": {
        "id": "MRV6pzVbmNWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "dVwyhAYq-4O9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the data generator\n",
        "from tensorflow.keras.utils import Sequence, set_random_seed\n",
        "\n",
        "set_random_seed(1234)\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    # Initialization\n",
        "    def __init__(self, data, batch_size=32, dim=len(all_electrodes), shuffle=True):\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.data = data\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Shows the number of batches per epoch\n",
        "        return int(np.floor(self.data.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate one batch of data\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        batch = np.empty((self.batch_size, self.dim))\n",
        "        for index1, elem in enumerate(indexes):\n",
        "          batch[index1] = self.data[index1]\n",
        "\n",
        "        return batch, batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updating the index after each epoch\n",
        "        self.indexes = np.arange(self.data.shape[0])\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "# Creating the generators\n",
        "train_gen = DataGenerator(train_feat, 512)\n",
        "val_gen = DataGenerator(val_feat, 512)\n",
        "test_gen = DataGenerator(test_feat, 512)"
      ],
      "metadata": {
        "id": "xQ8U-ZLCCYTL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization\n",
        "import keras_tuner as kt\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# Creating the hyperparameter tuner function\n",
        "# The AE has 5 Dense layers besides the output, and the 3rd one is the bottleneck layer\n",
        "def create_ae_optimal(hp):\n",
        "  input = Input(shape=(4860))\n",
        "  \n",
        "  hp_units_1 = hp.Int('units_1', min_value=1500, max_value=2500, step=125)\n",
        "  encoded = Dense(units=hp_units_1, activation=\"relu\", kernel_initializer='HeNormal')(input)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_1', min_value = 0.0, max_value = 0.5, default = 0.25, step = 0.05))(encoded)\n",
        "\n",
        "  hp_units_2 = hp.Int('units_2', min_value=750, max_value=1250, step=50)\n",
        "  encoded = Dense(units=hp_units_2, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_2', min_value = 0.0, max_value = 0.5, default = 0.25, step = 0.05))(encoded)\n",
        "\n",
        "  hp_units_3 = hp.Int('units_3', min_value=400, max_value=600, step=25)\n",
        "  encoded = Dense(units=hp_units_3, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_3', min_value = 0.0, max_value = 0.5, default = 0.25, step = 0.05))(encoded)\n",
        "\n",
        "  hp_units_4 = hp.Int('units_4', min_value=750, max_value=1250, step=50)\n",
        "  decoded = Dense(units=hp_units_4, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  decoded = Dropout(rate=hp.Float('dropout_4', min_value = 0.0, max_value = 0.5, default = 0.25, step = 0.05))(decoded)\n",
        "\n",
        "  hp_units_5 = hp.Int('units_5', min_value=1500, max_value=2500, step=125)\n",
        "  decoded = Dense(units=hp_units_5, activation=\"relu\", kernel_initializer='HeNormal')(decoded)\n",
        "  decoded = Dropout(rate=hp.Float('dropout_5', min_value = 0.0, max_value = 0.5, default = 0.25, step = 0.05))(decoded)\n",
        "\n",
        "  output = Dense(4860)(decoded)\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model = Model(input, output)\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss='mse',\n",
        "                metrics=['mse'])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "R_ZSI31I-4zT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the tuner\n",
        "tuner = kt.Hyperband(create_ae_optimal,\n",
        "                     objective='val_loss',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='/content/ae_opt',\n",
        "                     project_name='ae_opt1')"
      ],
      "metadata": {
        "id": "kdUDep51B7_T"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "#note: takes about 15-20 minutes to optimize\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=10)\n",
        "tuner.search(train_gen, epochs=100, validation_data=val_gen, verbose=0, shuffle=True, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "ISWZG5BNCJ7d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting and retraining the model with the best params\n",
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "early_stopping=EarlyStopping(patience=20, verbose=1, min_delta=1e-5)\n",
        "checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "hypermodel.fit(train_gen, epochs=500, verbose=1, validation_data=val_gen, callbacks=[checkpointer, early_stopping])"
      ],
      "metadata": {
        "id": "_lpNSFOpDDJj",
        "outputId": "be877ed7-c39d-43d9-ded6-b5d4a838c237",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.3036 - mse: 0.3036\n",
            "Epoch 1: val_loss improved from inf to 0.29726, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 14s 32ms/step - loss: 0.3030 - mse: 0.3030 - val_loss: 0.2973 - val_mse: 0.2973\n",
            "Epoch 2/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0628 - mse: 0.0628\n",
            "Epoch 2: val_loss improved from 0.29726 to 0.29106, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.0628 - mse: 0.0628 - val_loss: 0.2911 - val_mse: 0.2911\n",
            "Epoch 3/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 3: val_loss improved from 0.29106 to 0.26702, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.2670 - val_mse: 0.2670\n",
            "Epoch 4/500\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0318 - mse: 0.0318\n",
            "Epoch 4: val_loss improved from 0.26702 to 0.26269, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.2627 - val_mse: 0.2627\n",
            "Epoch 5/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0273 - mse: 0.0273\n",
            "Epoch 5: val_loss improved from 0.26269 to 0.26153, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 10s 30ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.2615 - val_mse: 0.2615\n",
            "Epoch 6/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0243 - mse: 0.0243\n",
            "Epoch 6: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.2797 - val_mse: 0.2797\n",
            "Epoch 7/500\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0220 - mse: 0.0220\n",
            "Epoch 7: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.2976 - val_mse: 0.2976\n",
            "Epoch 8/500\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0204 - mse: 0.0204\n",
            "Epoch 8: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.3074 - val_mse: 0.3074\n",
            "Epoch 9/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0190 - mse: 0.0190\n",
            "Epoch 9: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.3137 - val_mse: 0.3137\n",
            "Epoch 10/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0180 - mse: 0.0180\n",
            "Epoch 10: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.3292 - val_mse: 0.3292\n",
            "Epoch 11/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0170 - mse: 0.0170\n",
            "Epoch 11: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.3194 - val_mse: 0.3194\n",
            "Epoch 12/500\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0161 - mse: 0.0161\n",
            "Epoch 12: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 28ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.3207 - val_mse: 0.3207\n",
            "Epoch 13/500\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0152 - mse: 0.0152\n",
            "Epoch 13: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.3185 - val_mse: 0.3185\n",
            "Epoch 14/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0145 - mse: 0.0145\n",
            "Epoch 14: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.3079 - val_mse: 0.3079\n",
            "Epoch 15/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0139 - mse: 0.0139\n",
            "Epoch 15: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.3028 - val_mse: 0.3028\n",
            "Epoch 16/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0132 - mse: 0.0132\n",
            "Epoch 16: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.3021 - val_mse: 0.3021\n",
            "Epoch 17/500\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0126 - mse: 0.0126\n",
            "Epoch 17: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.2997 - val_mse: 0.2997\n",
            "Epoch 18/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0122 - mse: 0.0122\n",
            "Epoch 18: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.3009 - val_mse: 0.3009\n",
            "Epoch 19/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0117 - mse: 0.0117\n",
            "Epoch 19: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.3058 - val_mse: 0.3058\n",
            "Epoch 20/500\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0115 - mse: 0.0115\n",
            "Epoch 20: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.3156 - val_mse: 0.3156\n",
            "Epoch 21/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0114 - mse: 0.0114\n",
            "Epoch 21: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.3096 - val_mse: 0.3096\n",
            "Epoch 22/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0107 - mse: 0.0107\n",
            "Epoch 22: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.3133 - val_mse: 0.3133\n",
            "Epoch 23/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0108 - mse: 0.0108\n",
            "Epoch 23: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.3212 - val_mse: 0.3212\n",
            "Epoch 24/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0103 - mse: 0.0103\n",
            "Epoch 24: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.3303 - val_mse: 0.3303\n",
            "Epoch 25/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0102 - mse: 0.0102\n",
            "Epoch 25: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.3413 - val_mse: 0.3413\n",
            "Epoch 25: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fae439d2820>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation \n",
        "hypermodel.load_weights('weights1.hdf5')\n",
        "hypermodel.evaluate(test_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6gl1GEa6NAI",
        "outputId": "5b2c46e0-17c0-4acc-a24e-e1a7d29c6cb7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "116/116 [==============================] - 2s 14ms/step - loss: 0.2699 - mse: 0.2699\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.26994505524635315, 0.26994505524635315]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Generating and exporting the lower dimensional data</h2>"
      ],
      "metadata": {
        "id": "GYs0QLaYhisH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hypermodel.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QWuY1lchrS4",
        "outputId": "c734f743-fb07-40d6-8d0b-1bb4d60759a3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4860)]            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1625)              7899125   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 1625)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1250)              2032500   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 1250)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 575)               719325    \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 575)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 850)               489600    \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 850)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1625)              1382875   \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 1625)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 4860)              7902360   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,425,785\n",
            "Trainable params: 20,425,785\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only the encoder\n",
        "model2= Model(inputs=hypermodel.input, outputs=hypermodel.layers[-7].output)\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrTwZhiyhv8Q",
        "outputId": "0ea1587f-d307-4069-d454-6bf4c371dd3d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4860)]            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1625)              7899125   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 1625)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1250)              2032500   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 1250)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 575)               719325    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,650,950\n",
            "Trainable params: 10,650,950\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the new data using the output of the bottleneck layer\n",
        "train_new = model2.predict(train_gen)\n",
        "val_new = model2.predict(val_gen)\n",
        "test_new = model2.predict(test_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqrNk9pKiiHa",
        "outputId": "0465dffd-0ede-4a75-ba71-24753ea5ab22"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "349/349 [==============================] - 5s 14ms/step\n",
            "116/116 [==============================] - 2s 14ms/step\n",
            "116/116 [==============================] - 2s 15ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the feature-label set pairs as pickle files onto Google Drive\n",
        "import pickle\n",
        "\n",
        "with open('train.pkl', 'wb') as f:  \n",
        "    pickle.dump([train_new, train_spec], f)\n",
        "\n",
        "with open('val.pkl', 'wb') as f:  \n",
        "    pickle.dump([val_new, val_spec], f)\n",
        "\n",
        "with open('test.pkl', 'wb') as f:  \n",
        "    pickle.dump([test_new, test_spec], f)"
      ],
      "metadata": {
        "id": "UWTKZceejPgS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp train.pkl drive/MyDrive/DeepLearning/train.pkl\n",
        "!cp val.pkl drive/MyDrive/DeepLearning/val.pkl\n",
        "!cp test.pkl drive/MyDrive/DeepLearning/test.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lhk7ec0pyg_",
        "outputId": "4861f047-1c7d-4814-937e-3ca2d837febf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Option 2 for dimensionality reduction: Incremental PCA</h2>"
      ],
      "metadata": {
        "id": "q9cD1KX0ajBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "n_comp = 200\n",
        "\n",
        "# note: takes about 10 minutes to run\n",
        "pca = IncrementalPCA(n_components=n_comp, batch_size=1024)\n",
        "pca.fit(train_feat)\n",
        "train_feat = pca.transform(train_feat)\n",
        "val_feat = pca.transform(val_feat)\n",
        "test_feat = pca.transform(test_feat)"
      ],
      "metadata": {
        "id": "AARr0wkEaqfR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the new data\n",
        "\n",
        "with open('train_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([train_feat, train_spec], f)\n",
        "\n",
        "with open('val_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([val_feat, val_spec], f)\n",
        "\n",
        "with open('test_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([test_feat, test_spec], f)\n",
        "\n",
        "# Copying to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp train_v2.pkl drive/MyDrive/DeepLearning/train_v2.pkl\n",
        "!cp val_v2.pkl drive/MyDrive/DeepLearning/val_v2.pkl\n",
        "!cp test_v2.pkl drive/MyDrive/DeepLearning/test_v2.pkl"
      ],
      "metadata": {
        "id": "QbzPyjaBdZeu",
        "outputId": "d2e803c8-742c-4b1d-bb50-71eb76a20ee1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Z2sk5E2eYPA"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}