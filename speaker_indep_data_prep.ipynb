{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNspf/iwaQdTACpvvxvV63s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v-artur/Golden_Oreos/blob/main/speaker_indep_data_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtaining the data"
      ],
      "metadata": {
        "id": "QRznZ-paPxRN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3F36vS9-11r",
        "outputId": "ca8fe6b6-0270-40e0-b4d5-b4d76a6b0992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-09 07:21:10--  https://docs.google.com/uc?export=download&confirm=t&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.125.102, 108.177.125.139, 108.177.125.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.125.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/usphsnju7cn0bgk6897l018i9lc74i2u/1670570400000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=2190d4b1-aec8-4159-a55a-5ddb1b93a41a [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-09 07:21:11--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/usphsnju7cn0bgk6897l018i9lc74i2u/1670570400000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=2190d4b1-aec8-4159-a55a-5ddb1b93a41a\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 64.233.188.132, 2404:6800:4008:c06::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|64.233.188.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2141405093 (2.0G) [application/x-zip-compressed]\n",
            "Saving to: ‘features.zip’\n",
            "\n",
            "features.zip        100%[===================>]   1.99G  42.6MB/s    in 47s     \n",
            "\n",
            "2022-12-09 07:21:59 (43.2 MB/s) - ‘features.zip’ saved [2141405093/2141405093]\n",
            "\n",
            "--2022-12-09 07:22:01--  https://docs.google.com/uc?export=download&confirm=&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.125.102, 108.177.125.139, 108.177.125.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.125.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/is0fbgu6gi2br1pfv7imvhne6usro9l7/1670570475000/17895932938140350971/*/1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80?e=download&uuid=077cec6c-90f0-46ad-b4ca-e98d93fa851c [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-09 07:22:02--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/is0fbgu6gi2br1pfv7imvhne6usro9l7/1670570475000/17895932938140350971/*/1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80?e=download&uuid=077cec6c-90f0-46ad-b4ca-e98d93fa851c\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 64.233.188.132, 2404:6800:4008:c06::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|64.233.188.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9238 (9.0K) [application/x-zip-compressed]\n",
            "Saving to: ‘subject_channels.zip’\n",
            "\n",
            "subject_channels.zi 100%[===================>]   9.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-09 07:22:03 (105 MB/s) - ‘subject_channels.zip’ saved [9238/9238]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#features\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\" -O features.zip && rm -rf /tmp/cookies.txt\n",
        "#original electrode names\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80\" -O subject_channels.zip && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# Data extraction\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/features.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/features\")\n",
        "zip_ref.close()\n",
        "\n",
        "# Electrode name extraction\n",
        "zip_ref = zipfile.ZipFile(\"/content/subject_channels.zip\", 'r')\n",
        "zip_ref.extractall(\"/content\")\n",
        "zip_ref.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Preparations and needed functions</h3>"
      ],
      "metadata": {
        "id": "QdxGeq3EP2hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Our goal is to put the feature names in order to preserve the sequentiality within the feature vectors\n",
        "\n",
        "# Getting the original electrode names\n",
        "original_electrodes = set()\n",
        "\n",
        "for subject in ['01','02','03','04','05','06','07','08','09','10']:\n",
        "  table = pd.read_csv(f'/content/subject_channels/sub-{subject}_task-wordProduction_channels.tsv', sep='\\t')\n",
        "  elecs = set(table['name'])\n",
        "  original_electrodes = original_electrodes.union(elecs)\n",
        "\n",
        "# Now indexing them from -4 to 4 (9 in total)\n",
        "all_electrodes = []\n",
        "for i in range(9):\n",
        "  for elec in original_electrodes:\n",
        "    all_electrodes.append(elec + \"T\" + str(i-4))\n",
        "\n",
        "print('Number of different features:', len(all_electrodes))\n",
        "\n",
        "#we will use this list's indexes to correspond to the feature matrices\n",
        "all_electrodes = list(all_electrodes) "
      ],
      "metadata": {
        "id": "annt8G29nDNS",
        "outputId": "79362312-238c-494f-c7c3-5e63ac029b93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of different features: 4860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "print(all_electrodes[:10])"
      ],
      "metadata": {
        "id": "EKDUV4bnpLkq",
        "outputId": "3b7fce29-79d2-4abf-9994-b34aba991b00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['LH1T-4', 'LU10T-4', 'LY7T-4', 'RK8T-4', 'LK4T-4', 'RA1T-4', 'RC11T-4', 'LG13T-4', 'RM3T-4', 'LD7T-4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for making every feature vector into a 4860 dimensional vector\n",
        "\n",
        "def dim_adjust(data, feature_names):\n",
        "  #create a new matrix with zeros, insert the values into the columns\n",
        "  #which correspond to the subject's feature names, then concatenate them\n",
        "\n",
        "  new_matrix = np.zeros((data.shape[0],len(all_electrodes)))\n",
        "  for column in range(data.shape[1]):\n",
        "    insert_index = all_electrodes.index(feature_names[column])\n",
        "    new_matrix[:,insert_index] = data[:,column]\n",
        "    \n",
        "  return new_matrix\n"
      ],
      "metadata": {
        "id": "d8o2edSn_CkY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making the iterated test, validation and test sets"
      ],
      "metadata": {
        "id": "bwJu3UnzP8ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "feat_path = \"/content/features\"\n",
        "\n",
        "# Function to generate the train, val and test features and mel spectrograms\n",
        "def generate_features_and_spec():\n",
        "  #Base arrays\n",
        "  train_feat = np.empty((0, len(all_electrodes)))\n",
        "  val_feat = np.empty((0, len(all_electrodes)))\n",
        "  test_feat = np.empty((0, len(all_electrodes)))\n",
        "\n",
        "  train_spec = np.empty((0, 23))\n",
        "  val_spec = np.empty((0, 23))\n",
        "  test_spec = np.empty((0, 23))\n",
        "\n",
        "  for index, subject in enumerate(['01','02','03','04','05','06','07','08','09','10']):\n",
        "    #loading the features, feature names and mel spectrogram of the subject\n",
        "    data = np.load(os.path.join(feat_path,f'sub-{subject}_feat.npy'))\n",
        "    feature_names = np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy'))\n",
        "    spectrogram = np.load(os.path.join(feat_path,f'sub-{subject}_spec.npy'))\n",
        "\n",
        "    #splittig the features and the labels into 5 parts\n",
        "    feat_splits = np.array_split(data, 5)\n",
        "    spec_splits = np.array_split(spectrogram, 5)\n",
        "\n",
        "    #making the train, val or test arrays for the subject using a 60-20-20 ratio\n",
        "    #because of the \"index\" changing, the splitting position will iterate with each subject as well\n",
        "    subject_train_feat = np.vstack((feat_splits[index % 5],feat_splits[(index+1) % 5],feat_splits[(index+2) % 5]))\n",
        "    subject_train_spec = np.vstack((spec_splits[index % 5],spec_splits[(index+1) % 5],spec_splits[(index+2) % 5]))\n",
        "\n",
        "    subject_val_feat = feat_splits[(index+3) % 5]\n",
        "    subject_val_spec = spec_splits[(index+3) % 5]\n",
        "\n",
        "    subject_test_feat = feat_splits[(index+4) % 5]\n",
        "    subject_test_spec = spec_splits[(index+4) % 5]\n",
        "\n",
        "    #concatenating the dimensionality-adjusted features with the pre-existing feature set\n",
        "    train_feat = np.concatenate((train_feat, dim_adjust(subject_train_feat, feature_names)))\n",
        "    val_feat = np.concatenate((val_feat, dim_adjust(subject_val_feat, feature_names)))\n",
        "    test_feat = np.concatenate((test_feat, dim_adjust(subject_test_feat, feature_names)))\n",
        "\n",
        "    #concatenating the appropriate arrays with the pre-existing train, val or test labels\n",
        "    train_spec = np.concatenate((train_spec, subject_train_spec))\n",
        "    val_spec = np.concatenate((val_spec, subject_val_spec))\n",
        "    test_spec = np.concatenate((test_spec, subject_test_spec))\n",
        "\n",
        "  return train_feat, train_spec, val_feat, val_spec, test_feat, test_spec \n",
        "\n",
        "# Generating the data\n",
        "train_feat, train_spec, val_feat, val_spec, test_feat, test_spec = generate_features_and_spec()"
      ],
      "metadata": {
        "id": "3sBGVYAoc7LA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Scaling the data \n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_feat)\n",
        "train_feat = scaler.transform(train_feat)\n",
        "val_feat = scaler.transform(val_feat)\n",
        "test_feat = scaler.transform(test_feat)"
      ],
      "metadata": {
        "id": "JC2LF6YOfflo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality reduction"
      ],
      "metadata": {
        "id": "MRV6pzVbmNWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Option 1: Tuned AutoEncoder</h3>"
      ],
      "metadata": {
        "id": "NV5SeqERySxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "dVwyhAYq-4O9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the data generator\n",
        "from tensorflow.keras.utils import Sequence, set_random_seed\n",
        "\n",
        "set_random_seed(1234)\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    # Initialization\n",
        "    def __init__(self, data, batch_size=32, dim=len(all_electrodes), shuffle=True):\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.data = data\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Shows the number of batches per epoch\n",
        "        return int(np.floor(self.data.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate one batch of data\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        batch = np.empty((self.batch_size, self.dim))\n",
        "        for index1, elem in enumerate(indexes):\n",
        "          batch[index1] = self.data[index1]\n",
        "\n",
        "        return batch, batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updating the index after each epoch\n",
        "        self.indexes = np.arange(self.data.shape[0])\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "# Creating the generators\n",
        "train_gen = DataGenerator(train_feat, 256)\n",
        "val_gen = DataGenerator(val_feat, 256)\n",
        "test_gen = DataGenerator(test_feat, 256)"
      ],
      "metadata": {
        "id": "xQ8U-ZLCCYTL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "import keras_tuner as kt\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# Creating the hyperparameter tuner function\n",
        "# The AE has 5 Dense layers besides the output, and the 3rd one is the bottleneck layer\n",
        "def create_ae_optimal(hp):\n",
        "  input = Input(shape=(4860))\n",
        "  \n",
        "  hp_units_1 = hp.Int('units_1', min_value=1500, max_value=2500, step=125)\n",
        "  encoded = Dense(units=hp_units_1, activation=\"relu\", kernel_initializer='HeNormal')(input)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_1', min_value = 0.0, max_value = 0.5, default = 0.25, step = 0.05))(encoded)\n",
        "\n",
        "  hp_units_2 = hp.Int('units_2', min_value=750, max_value=1250, step=50)\n",
        "  encoded = Dense(units=hp_units_2, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_2', min_value = 0.0, max_value = 0.5, default = 0.25, step = 0.05))(encoded)\n",
        "\n",
        "  hp_units_3 = hp.Int('units_3', min_value=400, max_value=600, step=25)\n",
        "  encoded = Dense(units=hp_units_3, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_3', min_value = 0.0, max_value = 0.5, default = 0.25, step = 0.05))(encoded)\n",
        "\n",
        "  hp_units_4 = hp.Int('units_4', min_value=750, max_value=1250, step=50)\n",
        "  decoded = Dense(units=hp_units_4, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  decoded = Dropout(rate=hp.Float('dropout_4', min_value = 0.0, max_value = 0.5, default = 0.25, step = 0.05))(decoded)\n",
        "\n",
        "  hp_units_5 = hp.Int('units_5', min_value=1500, max_value=2500, step=125)\n",
        "  decoded = Dense(units=hp_units_5, activation=\"relu\", kernel_initializer='HeNormal')(decoded)\n",
        "  decoded = Dropout(rate=hp.Float('dropout_5', min_value = 0.0, max_value = 0.5, default = 0.25, step = 0.05))(decoded)\n",
        "\n",
        "  output = Dense(4860)(decoded)\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  hp_momentum = hp.Choice('momentum', values=[0.9, 0.95, 0.99])\n",
        "\n",
        "  model = Model(input, output)\n",
        "\n",
        "  # Our early experiments showed that SGD is slightly better here than ADAM\n",
        "  model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=hp_learning_rate, momentum=hp_momentum),\n",
        "                loss='mse',\n",
        "                metrics=['mse'])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "R_ZSI31I-4zT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the tuner\n",
        "tuner = kt.Hyperband(create_ae_optimal,\n",
        "                     objective='val_loss',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='/content/ae_opt',\n",
        "                     project_name='ae_opt1')"
      ],
      "metadata": {
        "id": "kdUDep51B7_T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "#note: takes about 20-25 minutes to optimize\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=10)\n",
        "tuner.search(train_gen, epochs=100, validation_data=val_gen, verbose=0, shuffle=True, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "ISWZG5BNCJ7d",
        "outputId": "245ff2a3-ebe6-45cd-9b26-4baa0439b4d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0122s vs `on_train_batch_end` time: 0.0126s). Check your callbacks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting and retraining the model with the best params\n",
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "early_stopping=EarlyStopping(patience=20, verbose=1, min_delta=1e-5)\n",
        "checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "hypermodel.fit(train_gen, epochs=500, verbose=1, validation_data=val_gen, callbacks=[checkpointer, early_stopping])"
      ],
      "metadata": {
        "id": "_lpNSFOpDDJj",
        "outputId": "584deac5-8eef-49f9-e094-c913d78e0017",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.4965 - mse: 0.4965\n",
            "Epoch 1: val_loss improved from inf to 0.56570, saving model to weights1.hdf5\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.4965 - mse: 0.4965 - val_loss: 0.5657 - val_mse: 0.5657\n",
            "Epoch 2/500\n",
            "696/699 [============================>.] - ETA: 0s - loss: 0.3386 - mse: 0.3386\n",
            "Epoch 2: val_loss improved from 0.56570 to 0.42814, saving model to weights1.hdf5\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.3384 - mse: 0.3384 - val_loss: 0.4281 - val_mse: 0.4281\n",
            "Epoch 3/500\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.2611 - mse: 0.2611\n",
            "Epoch 3: val_loss improved from 0.42814 to 0.36683, saving model to weights1.hdf5\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.3668 - val_mse: 0.3668\n",
            "Epoch 4/500\n",
            "696/699 [============================>.] - ETA: 0s - loss: 0.2248 - mse: 0.2248\n",
            "Epoch 4: val_loss improved from 0.36683 to 0.34375, saving model to weights1.hdf5\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.2247 - mse: 0.2247 - val_loss: 0.3438 - val_mse: 0.3438\n",
            "Epoch 5/500\n",
            "698/699 [============================>.] - ETA: 0s - loss: 0.2061 - mse: 0.2061\n",
            "Epoch 5: val_loss improved from 0.34375 to 0.33557, saving model to weights1.hdf5\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.2061 - mse: 0.2061 - val_loss: 0.3356 - val_mse: 0.3356\n",
            "Epoch 6/500\n",
            "696/699 [============================>.] - ETA: 0s - loss: 0.1921 - mse: 0.1921\n",
            "Epoch 6: val_loss improved from 0.33557 to 0.33057, saving model to weights1.hdf5\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.1921 - mse: 0.1921 - val_loss: 0.3306 - val_mse: 0.3306\n",
            "Epoch 7/500\n",
            "696/699 [============================>.] - ETA: 0s - loss: 0.1789 - mse: 0.1789\n",
            "Epoch 7: val_loss did not improve from 0.33057\n",
            "699/699 [==============================] - 12s 18ms/step - loss: 0.1789 - mse: 0.1789 - val_loss: 0.3343 - val_mse: 0.3343\n",
            "Epoch 8/500\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.1664 - mse: 0.1664\n",
            "Epoch 8: val_loss did not improve from 0.33057\n",
            "699/699 [==============================] - 12s 18ms/step - loss: 0.1663 - mse: 0.1663 - val_loss: 0.3309 - val_mse: 0.3309\n",
            "Epoch 9/500\n",
            "696/699 [============================>.] - ETA: 0s - loss: 0.1551 - mse: 0.1551\n",
            "Epoch 9: val_loss improved from 0.33057 to 0.32984, saving model to weights1.hdf5\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.1551 - mse: 0.1551 - val_loss: 0.3298 - val_mse: 0.3298\n",
            "Epoch 10/500\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.1453 - mse: 0.1453\n",
            "Epoch 10: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 12s 18ms/step - loss: 0.1453 - mse: 0.1453 - val_loss: 0.3342 - val_mse: 0.3342\n",
            "Epoch 11/500\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.1367 - mse: 0.1367\n",
            "Epoch 11: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 12s 18ms/step - loss: 0.1366 - mse: 0.1366 - val_loss: 0.3305 - val_mse: 0.3305\n",
            "Epoch 12/500\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.1286 - mse: 0.1286\n",
            "Epoch 12: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 12s 18ms/step - loss: 0.1286 - mse: 0.1286 - val_loss: 0.3365 - val_mse: 0.3365\n",
            "Epoch 13/500\n",
            "696/699 [============================>.] - ETA: 0s - loss: 0.1211 - mse: 0.1211\n",
            "Epoch 13: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 12s 18ms/step - loss: 0.1211 - mse: 0.1211 - val_loss: 0.3346 - val_mse: 0.3346\n",
            "Epoch 14/500\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.1142 - mse: 0.1142\n",
            "Epoch 14: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.1142 - mse: 0.1142 - val_loss: 0.3364 - val_mse: 0.3364\n",
            "Epoch 15/500\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.1078 - mse: 0.1078\n",
            "Epoch 15: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.1078 - mse: 0.1078 - val_loss: 0.3377 - val_mse: 0.3377\n",
            "Epoch 16/500\n",
            "696/699 [============================>.] - ETA: 0s - loss: 0.1021 - mse: 0.1021\n",
            "Epoch 16: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.1021 - mse: 0.1021 - val_loss: 0.3383 - val_mse: 0.3383\n",
            "Epoch 17/500\n",
            "696/699 [============================>.] - ETA: 0s - loss: 0.0971 - mse: 0.0971\n",
            "Epoch 17: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 12s 18ms/step - loss: 0.0971 - mse: 0.0971 - val_loss: 0.3417 - val_mse: 0.3417\n",
            "Epoch 18/500\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.0926 - mse: 0.0926\n",
            "Epoch 18: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 12s 18ms/step - loss: 0.0926 - mse: 0.0926 - val_loss: 0.3394 - val_mse: 0.3394\n",
            "Epoch 19/500\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.0885 - mse: 0.0885\n",
            "Epoch 19: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 12s 18ms/step - loss: 0.0885 - mse: 0.0885 - val_loss: 0.3444 - val_mse: 0.3444\n",
            "Epoch 20/500\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.0848 - mse: 0.0848\n",
            "Epoch 20: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.0848 - mse: 0.0848 - val_loss: 0.3441 - val_mse: 0.3441\n",
            "Epoch 21/500\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.0815 - mse: 0.0815\n",
            "Epoch 21: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.0815 - mse: 0.0815 - val_loss: 0.3458 - val_mse: 0.3458\n",
            "Epoch 22/500\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.0784 - mse: 0.0784\n",
            "Epoch 22: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.0784 - mse: 0.0784 - val_loss: 0.3456 - val_mse: 0.3456\n",
            "Epoch 23/500\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.0754 - mse: 0.0754\n",
            "Epoch 23: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.0754 - mse: 0.0754 - val_loss: 0.3446 - val_mse: 0.3446\n",
            "Epoch 24/500\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.0727 - mse: 0.0727\n",
            "Epoch 24: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 12s 18ms/step - loss: 0.0727 - mse: 0.0727 - val_loss: 0.3450 - val_mse: 0.3450\n",
            "Epoch 25/500\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.0701 - mse: 0.0701\n",
            "Epoch 25: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 12s 18ms/step - loss: 0.0701 - mse: 0.0701 - val_loss: 0.3484 - val_mse: 0.3484\n",
            "Epoch 26/500\n",
            "698/699 [============================>.] - ETA: 0s - loss: 0.0678 - mse: 0.0678\n",
            "Epoch 26: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 12s 18ms/step - loss: 0.0678 - mse: 0.0678 - val_loss: 0.3483 - val_mse: 0.3483\n",
            "Epoch 27/500\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.0656 - mse: 0.0656\n",
            "Epoch 27: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 12s 18ms/step - loss: 0.0656 - mse: 0.0656 - val_loss: 0.3488 - val_mse: 0.3488\n",
            "Epoch 28/500\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.0635 - mse: 0.0635\n",
            "Epoch 28: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 12s 18ms/step - loss: 0.0635 - mse: 0.0635 - val_loss: 0.3480 - val_mse: 0.3480\n",
            "Epoch 29/500\n",
            "696/699 [============================>.] - ETA: 0s - loss: 0.0616 - mse: 0.0616\n",
            "Epoch 29: val_loss did not improve from 0.32984\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.0616 - mse: 0.0616 - val_loss: 0.3466 - val_mse: 0.3466\n",
            "Epoch 29: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f696c4bf520>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading back the best weights and checking the layers\n",
        "hypermodel.load_weights('weights1.hdf5')\n",
        "hypermodel.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QWuY1lchrS4",
        "outputId": "c104a0fc-d0de-4020-e976-173c9bbf3240"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4860)]            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 2000)              9722000   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 2000)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 800)               1600800   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 800)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 575)               460575    \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 575)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1100)              633600    \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 1100)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 2250)              2477250   \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 2250)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 4860)              10939860  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,834,085\n",
            "Trainable params: 25,834,085\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keeping only the encoder\n",
        "model2 = Model(inputs=hypermodel.input, outputs=hypermodel.layers[-7].output)\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrTwZhiyhv8Q",
        "outputId": "8f5d9921-1436-4c59-c315-9053ee7bb9b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4860)]            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 2000)              9722000   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 2000)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 800)               1600800   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 800)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 575)               460575    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,783,375\n",
            "Trainable params: 11,783,375\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the new data using the output of the bottleneck layer\n",
        "train_new = model2.predict(train_gen)\n",
        "val_new = model2.predict(val_gen)\n",
        "test_new = model2.predict(test_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqrNk9pKiiHa",
        "outputId": "7141a38f-1347-4c79-cb13-9047230b0015"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "699/699 [==============================] - 5s 7ms/step\n",
            "233/233 [==============================] - 2s 8ms/step\n",
            "233/233 [==============================] - 2s 9ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the feature-label set pairs as pickle files onto Google Drive\n",
        "import pickle\n",
        "\n",
        "with open('train.pkl', 'wb') as f:  \n",
        "    pickle.dump([train_new, train_spec], f)\n",
        "\n",
        "with open('val.pkl', 'wb') as f:  \n",
        "    pickle.dump([val_new, val_spec], f)\n",
        "\n",
        "with open('test.pkl', 'wb') as f:  \n",
        "    pickle.dump([test_new, test_spec], f)"
      ],
      "metadata": {
        "id": "UWTKZceejPgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp train.pkl drive/MyDrive/DeepLearning/train.pkl\n",
        "!cp val.pkl drive/MyDrive/DeepLearning/val.pkl\n",
        "!cp test.pkl drive/MyDrive/DeepLearning/test.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lhk7ec0pyg_",
        "outputId": "4861f047-1c7d-4814-937e-3ca2d837febf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Option 2: Incremental PCA</h3>"
      ],
      "metadata": {
        "id": "q9cD1KX0ajBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "n_comp = 200\n",
        "\n",
        "# note: takes about 10 minutes to run\n",
        "pca = IncrementalPCA(n_components=n_comp, batch_size=1024)\n",
        "pca.fit(train_feat)\n",
        "train_feat = pca.transform(train_feat)\n",
        "val_feat = pca.transform(val_feat)\n",
        "test_feat = pca.transform(test_feat)"
      ],
      "metadata": {
        "id": "AARr0wkEaqfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the new data\n",
        "\n",
        "with open('train_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([train_feat, train_spec], f)\n",
        "\n",
        "with open('val_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([val_feat, val_spec], f)\n",
        "\n",
        "with open('test_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([test_feat, test_spec], f)\n",
        "\n",
        "# Copying to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp train_v2.pkl drive/MyDrive/DeepLearning/train_v2.pkl\n",
        "!cp val_v2.pkl drive/MyDrive/DeepLearning/val_v2.pkl\n",
        "!cp test_v2.pkl drive/MyDrive/DeepLearning/test_v2.pkl"
      ],
      "metadata": {
        "id": "QbzPyjaBdZeu",
        "outputId": "d2e803c8-742c-4b1d-bb50-71eb76a20ee1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}