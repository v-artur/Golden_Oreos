{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPNxcFGhZvmazpgi6Xulx1t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v-artur/Golden_Oreos/blob/main/speaker_indep_data_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Obtaining the data</h2>"
      ],
      "metadata": {
        "id": "QRznZ-paPxRN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3F36vS9-11r",
        "outputId": "f1b4f42a-7813-4815-9cb6-3e6d0916937c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-08 11:23:42--  https://docs.google.com/uc?export=download&confirm=t&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.97.139, 108.177.97.138, 108.177.97.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.97.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/32vnvbrhcbgc1ce0mqmp7qngpv1vffl1/1670498550000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=0565305a-3b47-4bc1-b251-0a252362d176 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-08 11:23:43--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/32vnvbrhcbgc1ce0mqmp7qngpv1vffl1/1670498550000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=0565305a-3b47-4bc1-b251-0a252362d176\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 74.125.204.132, 2404:6800:4008:c04::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|74.125.204.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2141405093 (2.0G) [application/x-zip-compressed]\n",
            "Saving to: ‘features.zip’\n",
            "\n",
            "features.zip        100%[===================>]   1.99G  53.5MB/s    in 47s     \n",
            "\n",
            "2022-12-08 11:24:31 (43.2 MB/s) - ‘features.zip’ saved [2141405093/2141405093]\n",
            "\n",
            "--2022-12-08 11:24:33--  https://docs.google.com/uc?export=download&confirm=t&id=1_eeG0d_r-RqazUkr-ZRPNC6L13sHYwIP\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.97.100, 108.177.97.138, 108.177.97.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.97.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-10-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qs5gqc59r5ipg86hfgj8i59mk50u1fsp/1670498625000/03710960119062529382/*/1_eeG0d_r-RqazUkr-ZRPNC6L13sHYwIP?e=download&uuid=7c53cc6a-0880-40ca-afa8-9aa72cd1398a [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-08 11:24:34--  https://doc-10-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qs5gqc59r5ipg86hfgj8i59mk50u1fsp/1670498625000/03710960119062529382/*/1_eeG0d_r-RqazUkr-ZRPNC6L13sHYwIP?e=download&uuid=7c53cc6a-0880-40ca-afa8-9aa72cd1398a\n",
            "Resolving doc-10-18-docs.googleusercontent.com (doc-10-18-docs.googleusercontent.com)... 74.125.204.132, 2404:6800:4008:c04::84\n",
            "Connecting to doc-10-18-docs.googleusercontent.com (doc-10-18-docs.googleusercontent.com)|74.125.204.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3022 (3.0K) [text/x-python]\n",
            "Saving to: ‘reconstructWave.py’\n",
            "\n",
            "reconstructWave.py  100%[===================>]   2.95K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-08 11:24:34 (125 MB/s) - ‘reconstructWave.py’ saved [3022/3022]\n",
            "\n",
            "--2022-12-08 11:24:36--  https://docs.google.com/uc?export=download&confirm=t&id=1Bjf3ncRe8CcWHl3i0HxRo4unRYkz2fog\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.97.100, 108.177.97.138, 108.177.97.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.97.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-10-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tj2h47kvddjvh5s9akd5gfcjmr13078m/1670498625000/03710960119062529382/*/1Bjf3ncRe8CcWHl3i0HxRo4unRYkz2fog?e=download&uuid=76d9f9ad-a2e6-45ac-8e92-e9cb9098911b [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-08 11:24:37--  https://doc-10-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tj2h47kvddjvh5s9akd5gfcjmr13078m/1670498625000/03710960119062529382/*/1Bjf3ncRe8CcWHl3i0HxRo4unRYkz2fog?e=download&uuid=76d9f9ad-a2e6-45ac-8e92-e9cb9098911b\n",
            "Resolving doc-10-18-docs.googleusercontent.com (doc-10-18-docs.googleusercontent.com)... 74.125.204.132, 2404:6800:4008:c04::84\n",
            "Connecting to doc-10-18-docs.googleusercontent.com (doc-10-18-docs.googleusercontent.com)|74.125.204.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2867 (2.8K) [text/x-python]\n",
            "Saving to: ‘MelFilterBank.py’\n",
            "\n",
            "MelFilterBank.py    100%[===================>]   2.80K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-08 11:24:38 (145 MB/s) - ‘MelFilterBank.py’ saved [2867/2867]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#the data\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\" -O features.zip && rm -rf /tmp/cookies.txt\n",
        "#reconstruction module\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1_eeG0d_r-RqazUkr-ZRPNC6L13sHYwIP' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1_eeG0d_r-RqazUkr-ZRPNC6L13sHYwIP\" -O reconstructWave.py && rm -rf /tmp/cookies.txt\n",
        "#Melfiltebank applier\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Bjf3ncRe8CcWHl3i0HxRo4unRYkz2fog' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Bjf3ncRe8CcWHl3i0HxRo4unRYkz2fog\" -O MelFilterBank.py && rm -rf /tmp/cookies.txt\n",
        "\n",
        "\n",
        "#extracting it\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/features.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/features\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Preparations and needed functions</h2>"
      ],
      "metadata": {
        "id": "QdxGeq3EP2hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "#setting the path\n",
        "feat_path = r'/content/features'\n",
        "\n",
        "# Counting how many different features there are\n",
        "all_electrodes = set()\n",
        "for subject in ['01','02','03','04','05','06','07','08','09','10']:\n",
        "  list1 = set(np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy')).tolist())\n",
        "  all_electrodes = all_electrodes.union(list1)  \n",
        "\n",
        "print('Number of different features:', len(all_electrodes))\n",
        "\n",
        "#we will use this list's indexes to correspond to the feature matrices\n",
        "all_electrodes = list(all_electrodes) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CckK8PHR_BF9",
        "outputId": "1130364b-fd7f-4769-8a2e-abadbed4335a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of different features: 4860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for making every feature vector into a 4860 dimensional vector\n",
        "\n",
        "def dim_adjust(data, feature_names):\n",
        "  #creating an empty array with appropriate length\n",
        "  feat_matrix = np.empty((0,len(all_electrodes)))\n",
        "\n",
        "  #create a new matrix with zeros, insert the values into the columns\n",
        "  #which correspond to the subject's feature names, then concatenate them\n",
        "  new_matrix = np.zeros((data.shape[0],len(all_electrodes)))\n",
        "  for column in range(data.shape[1]):\n",
        "    insert_index = all_electrodes.index(feature_names[column])\n",
        "    new_matrix[:,insert_index] = data[:,column]\n",
        "  feat_matrix = np.concatenate((feat_matrix, new_matrix), axis=0)\n",
        "    \n",
        "  return feat_matrix\n"
      ],
      "metadata": {
        "id": "d8o2edSn_CkY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Making the iterated test, validation and test sets</h2>"
      ],
      "metadata": {
        "id": "bwJu3UnzP8ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_features_and_spec():\n",
        "  #Base arrays\n",
        "  train_feat = np.empty((0, len(all_electrodes)))\n",
        "  val_feat = np.empty((0, len(all_electrodes)))\n",
        "  test_feat = np.empty((0, len(all_electrodes)))\n",
        "\n",
        "  train_spec = np.empty((0, 23))\n",
        "  val_spec = np.empty((0, 23))\n",
        "  test_spec = np.empty((0, 23))\n",
        "\n",
        "  for index, subject in enumerate(['01','02','03','04','05','06','07','08','09','10']):\n",
        "    #loading the features, feature names and mel spectrogram of the subject\n",
        "    data = np.load(os.path.join(feat_path,f'sub-{subject}_feat.npy'))\n",
        "    feature_names = np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy'))\n",
        "    spectrogram = np.load(os.path.join(feat_path,f'sub-{subject}_spec.npy'))\n",
        "\n",
        "    #splittig the features and the labels into 5 parts\n",
        "    feat_splits = np.array_split(data, 5)\n",
        "    spec_splits = np.array_split(spectrogram, 5)\n",
        "\n",
        "    #making the train, val or test arrays for the subject using a 60-20-20 ratio\n",
        "    #because of the \"index\" changing, the splitting position will iterate with each subject as well\n",
        "    subject_train_feat = np.vstack((feat_splits[index % 5],feat_splits[(index+1) % 5],feat_splits[(index+2) % 5]))\n",
        "    subject_train_spec = np.vstack((spec_splits[index % 5],spec_splits[(index+1) % 5],spec_splits[(index+2) % 5]))\n",
        "\n",
        "    subject_val_feat = feat_splits[(index+3) % 5]\n",
        "    subject_val_spec = spec_splits[(index+3) % 5]\n",
        "\n",
        "    subject_test_feat = feat_splits[(index+4) % 5]\n",
        "    subject_test_spec = spec_splits[(index+4) % 5]\n",
        "\n",
        "    #concatenating the dimensionality-adjusted features with the pre-existing feature set\n",
        "    train_feat = np.concatenate((train_feat, dim_adjust(subject_train_feat, feature_names)))\n",
        "    val_feat = np.concatenate((val_feat, dim_adjust(subject_val_feat, feature_names)))\n",
        "    test_feat = np.concatenate((test_feat, dim_adjust(subject_test_feat, feature_names)))\n",
        "\n",
        "    #concatenating the appropriate arrays with the pre-existing train, val or test labels\n",
        "    train_spec = np.concatenate((train_spec, subject_train_spec))\n",
        "    val_spec = np.concatenate((val_spec, subject_val_spec))\n",
        "    test_spec = np.concatenate((test_spec, subject_test_spec))\n",
        "\n",
        "  return train_feat, train_spec, val_feat, val_spec, test_feat, test_spec \n",
        "\n",
        "# Generating the data\n",
        "train_feat, train_spec, val_feat, val_spec, test_feat, test_spec = generate_features_and_spec()"
      ],
      "metadata": {
        "id": "3sBGVYAoc7LA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Scaling the data \n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_feat)\n",
        "train_feat = scaler.transform(train_feat)\n",
        "val_feat = scaler.transform(val_feat)\n",
        "test_feat = scaler.transform(test_feat)"
      ],
      "metadata": {
        "id": "JC2LF6YOfflo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Tuning the AutoEncoder for dimensionality reduction</h2>"
      ],
      "metadata": {
        "id": "MRV6pzVbmNWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization\n",
        "\n",
        "def create_ae_model(inputsize):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Input(shape=(inputsize)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2000, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1000, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(500, activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1000, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2000, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(inputsize))\n",
        "    return model"
      ],
      "metadata": {
        "id": "iHBC8y1bmSVV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the data generator\n",
        "from tensorflow.keras.utils import Sequence, set_random_seed\n",
        "\n",
        "set_random_seed(1234)\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    # Initialization\n",
        "    def __init__(self, data, batch_size=32, dim=len(all_electrodes), shuffle=True):\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.data = data\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Shows the number of batches per epoch\n",
        "        return int(np.floor(self.data.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate one batch of data\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        batch = np.empty((self.batch_size, self.dim))\n",
        "        for index1, elem in enumerate(indexes):\n",
        "          batch[index1] = self.data[index1]\n",
        "\n",
        "        return batch, batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updating the index after each epoch\n",
        "        self.indexes = np.arange(self.data.shape[0])\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "# Creating the generators\n",
        "train_gen = DataGenerator(train_feat, 256)\n",
        "val_gen = DataGenerator(val_feat, 256)\n",
        "test_gen = DataGenerator(test_feat, 256)"
      ],
      "metadata": {
        "id": "CwZCX1ZYvUhk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Creating and training the model\n",
        "model = create_ae_model(train_feat.shape[1])\n",
        "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
        "\n",
        "early_stopping=EarlyStopping(patience=20, verbose=1, min_delta=1e-5)\n",
        "checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "model.fit(train_gen, epochs=500, verbose=1, validation_data=val_gen, callbacks=[checkpointer, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYoorXA_nQWb",
        "outputId": "bbadb690-c9c6-4add-ab6f-4a256507da64"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "698/699 [============================>.] - ETA: 0s - loss: 0.3043 - mse: 0.3043\n",
            "Epoch 1: val_loss improved from inf to 0.37256, saving model to weights1.hdf5\n",
            "699/699 [==============================] - 18s 20ms/step - loss: 0.3041 - mse: 0.3041 - val_loss: 0.3726 - val_mse: 0.3726\n",
            "Epoch 2/500\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.1428 - mse: 0.1428\n",
            "Epoch 2: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.1427 - mse: 0.1427 - val_loss: 0.3890 - val_mse: 0.3890\n",
            "Epoch 3/500\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.1063 - mse: 0.1063\n",
            "Epoch 3: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 12s 18ms/step - loss: 0.1063 - mse: 0.1063 - val_loss: 0.3913 - val_mse: 0.3913\n",
            "Epoch 4/500\n",
            "696/699 [============================>.] - ETA: 0s - loss: 0.0863 - mse: 0.0863\n",
            "Epoch 4: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 18ms/step - loss: 0.0863 - mse: 0.0863 - val_loss: 0.3966 - val_mse: 0.3966\n",
            "Epoch 5/500\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.0734 - mse: 0.0734\n",
            "Epoch 5: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0734 - mse: 0.0734 - val_loss: 0.3975 - val_mse: 0.3975\n",
            "Epoch 6/500\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.0654 - mse: 0.0654\n",
            "Epoch 6: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0653 - mse: 0.0653 - val_loss: 0.4062 - val_mse: 0.4062\n",
            "Epoch 7/500\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.0593 - mse: 0.0593\n",
            "Epoch 7: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0593 - mse: 0.0593 - val_loss: 0.4062 - val_mse: 0.4062\n",
            "Epoch 8/500\n",
            "698/699 [============================>.] - ETA: 0s - loss: 0.0548 - mse: 0.0548\n",
            "Epoch 8: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0548 - mse: 0.0548 - val_loss: 0.4072 - val_mse: 0.4072\n",
            "Epoch 9/500\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.0509 - mse: 0.0509\n",
            "Epoch 9: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0509 - mse: 0.0509 - val_loss: 0.4128 - val_mse: 0.4128\n",
            "Epoch 10/500\n",
            "696/699 [============================>.] - ETA: 0s - loss: 0.0477 - mse: 0.0477\n",
            "Epoch 10: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0477 - mse: 0.0477 - val_loss: 0.4179 - val_mse: 0.4179\n",
            "Epoch 11/500\n",
            "698/699 [============================>.] - ETA: 0s - loss: 0.0450 - mse: 0.0450\n",
            "Epoch 11: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0450 - mse: 0.0450 - val_loss: 0.4185 - val_mse: 0.4185\n",
            "Epoch 12/500\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
            "Epoch 12: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.4223 - val_mse: 0.4223\n",
            "Epoch 13/500\n",
            "696/699 [============================>.] - ETA: 0s - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 13: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.4130 - val_mse: 0.4130\n",
            "Epoch 14/500\n",
            "696/699 [============================>.] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 14: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.4170 - val_mse: 0.4170\n",
            "Epoch 15/500\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.0367 - mse: 0.0367\n",
            "Epoch 15: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.4223 - val_mse: 0.4223\n",
            "Epoch 16/500\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.0353 - mse: 0.0353\n",
            "Epoch 16: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.4201 - val_mse: 0.4201\n",
            "Epoch 17/500\n",
            "696/699 [============================>.] - ETA: 0s - loss: 0.0341 - mse: 0.0341\n",
            "Epoch 17: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.4158 - val_mse: 0.4158\n",
            "Epoch 18/500\n",
            "698/699 [============================>.] - ETA: 0s - loss: 0.0331 - mse: 0.0331\n",
            "Epoch 18: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0331 - mse: 0.0331 - val_loss: 0.4175 - val_mse: 0.4175\n",
            "Epoch 19/500\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.0322 - mse: 0.0322\n",
            "Epoch 19: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.4172 - val_mse: 0.4172\n",
            "Epoch 20/500\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 20: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.4164 - val_mse: 0.4164\n",
            "Epoch 21/500\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.0307 - mse: 0.0307\n",
            "Epoch 21: val_loss did not improve from 0.37256\n",
            "699/699 [==============================] - 13s 19ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.4246 - val_mse: 0.4246\n",
            "Epoch 21: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f500a12ea60>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation \n",
        "model.load_weights('weights1.hdf5')\n",
        "model.evaluate(test_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6gl1GEa6NAI",
        "outputId": "9772045c-1a22-4a6a-ac5e-31d8922a08ae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "233/233 [==============================] - 2s 10ms/step - loss: 0.3897 - mse: 0.3897\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3897421658039093, 0.3897421658039093]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Generating and exporting the lower dimensional data</h2>"
      ],
      "metadata": {
        "id": "GYs0QLaYhisH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QWuY1lchrS4",
        "outputId": "17f6c513-7dc2-4246-d283-f8ca4a79374a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dropout (Dropout)           (None, 4860)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2000)              9722000   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 2000)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1000)              2001000   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 500)               500500    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 500)              2000      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 500)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1000)              501000    \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 2000)              2002000   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 2000)              0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4860)              9724860   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,453,360\n",
            "Trainable params: 24,452,360\n",
            "Non-trainable params: 1,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "model2= Model(inputs=model.input, outputs=model.layers[-7].output)\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrTwZhiyhv8Q",
        "outputId": "de719550-ea0f-47fb-ea89-cb2a39875841"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 4860)]            0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4860)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2000)              9722000   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 2000)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1000)              2001000   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 500)               500500    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 500)              2000      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,225,500\n",
            "Trainable params: 12,224,500\n",
            "Non-trainable params: 1,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the new data using the output of the bottleneck layer\n",
        "train_new = model2.predict(train_gen)\n",
        "val_new = model2.predict(val_gen)\n",
        "test_new = model2.predict(test_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqrNk9pKiiHa",
        "outputId": "57433f98-1280-47cf-b8b6-46dcbec3740b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "699/699 [==============================] - 5s 7ms/step\n",
            "233/233 [==============================] - 1s 6ms/step\n",
            "233/233 [==============================] - 1s 6ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the feature-label set pairs as pickles onto Google Drive\n",
        "import pickle\n",
        "\n",
        "with open('train.pkl', 'wb') as f:  \n",
        "    pickle.dump([train_new, train_spec], f)\n",
        "\n",
        "with open('val.pkl', 'wb') as f:  \n",
        "    pickle.dump([val_new, val_spec], f)\n",
        "\n",
        "with open('test.pkl', 'wb') as f:  \n",
        "    pickle.dump([test_new, test_spec], f)"
      ],
      "metadata": {
        "id": "UWTKZceejPgS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp train.pkl drive/MyDrive/DeepLearning/train.pkl\n",
        "!cp val.pkl drive/MyDrive/DeepLearning/val.pkl\n",
        "!cp test.pkl drive/MyDrive/DeepLearning/test.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lhk7ec0pyg_",
        "outputId": "b8f6ad19-7ed8-44be-98fe-2289173e38c4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fu7fDKfXsanv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}