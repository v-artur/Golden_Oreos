{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOLlNpCgdnRaTYFNjKEXzCk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v-artur/Golden_Oreos/blob/main/speaker_indep_data_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Obtaining the data</h2>"
      ],
      "metadata": {
        "id": "QRznZ-paPxRN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3F36vS9-11r",
        "outputId": "07c3055f-02f0-4bcc-ca56-9085b034ad09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-08 21:19:23--  https://docs.google.com/uc?export=download&confirm=t&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.145.138, 142.250.145.102, 142.250.145.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.145.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/pc97h5l1b56hjeak02mqmtu2fil61e1b/1670534325000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=681b46c5-04dc-4575-b187-31f48c196a40 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-08 21:19:23--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/pc97h5l1b56hjeak02mqmtu2fil61e1b/1670534325000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=681b46c5-04dc-4575-b187-31f48c196a40\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 172.217.218.132, 2a00:1450:4013:c08::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|172.217.218.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2141405093 (2.0G) [application/x-zip-compressed]\n",
            "Saving to: ‘features.zip’\n",
            "\n",
            "features.zip        100%[===================>]   1.99G   231MB/s    in 7.8s    \n",
            "\n",
            "2022-12-08 21:19:31 (261 MB/s) - ‘features.zip’ saved [2141405093/2141405093]\n",
            "\n",
            "--2022-12-08 21:19:33--  https://docs.google.com/uc?export=download&confirm=&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.145.138, 142.250.145.102, 142.250.145.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.145.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/t8i9fj9tjojg3hov5707nqifarfa8gvc/1670534325000/17895932938140350971/*/1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80?e=download&uuid=7d84e18a-89ee-45ba-8047-12889915b56f [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-08 21:19:34--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/t8i9fj9tjojg3hov5707nqifarfa8gvc/1670534325000/17895932938140350971/*/1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80?e=download&uuid=7d84e18a-89ee-45ba-8047-12889915b56f\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 172.217.218.132, 2a00:1450:4013:c08::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|172.217.218.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9238 (9.0K) [application/x-zip-compressed]\n",
            "Saving to: ‘subject_channels.zip’\n",
            "\n",
            "subject_channels.zi 100%[===================>]   9.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-08 21:19:34 (107 MB/s) - ‘subject_channels.zip’ saved [9238/9238]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#features\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\" -O features.zip && rm -rf /tmp/cookies.txt\n",
        "#original electrode names\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80\" -O subject_channels.zip && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# Data extraction\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/features.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/features\")\n",
        "zip_ref.close()\n",
        "\n",
        "# Electrode name extraction\n",
        "zip_ref = zipfile.ZipFile(\"/content/subject_channels.zip\", 'r')\n",
        "zip_ref.extractall(\"/content\")\n",
        "zip_ref.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Preparations and needed functions</h2>"
      ],
      "metadata": {
        "id": "QdxGeq3EP2hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We put the feature names in order \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Getting the original electrode names\n",
        "original_electrodes = set()\n",
        "\n",
        "for subject in ['01','02','03','04','05','06','07','08','09','10']:\n",
        "  table = pd.read_csv(f'/content/subject_channels/sub-{subject}_task-wordProduction_channels.tsv', sep='\\t')\n",
        "  elecs = set(table['name'])\n",
        "  original_electrodes = original_electrodes.union(elecs)\n",
        "\n",
        "# Now indexing them from -4 to 4 (9 in total)\n",
        "all_electrodes = []\n",
        "for i in range(9):\n",
        "  for elec in original_electrodes:\n",
        "    all_electrodes.append(elec + \"T\" + str(i-4))\n",
        "\n",
        "print('Number of different features:', len(all_electrodes))\n",
        "\n",
        "#we will use this list's indexes to correspond to the feature matrices\n",
        "all_electrodes = list(all_electrodes) "
      ],
      "metadata": {
        "id": "annt8G29nDNS",
        "outputId": "d5a4fee2-d1a9-43c1-e2d3-bde26c289f76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of different features: 4860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "print(all_electrodes[:10])"
      ],
      "metadata": {
        "id": "EKDUV4bnpLkq",
        "outputId": "0ec765c5-b65f-499a-a784-2917a87023bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['RT1T-4', 'LF5T-4', 'LFC3T-4', 'LG7T-4', 'RW10T-4', 'LS2T-4', 'LA11T-4', 'LL4T-4', 'RW7T-4', 'RF11T-4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for making every feature vector into a 4860 dimensional vector\n",
        "\n",
        "def dim_adjust(data, feature_names):\n",
        "  #create a new matrix with zeros, insert the values into the columns\n",
        "  #which correspond to the subject's feature names, then concatenate them\n",
        "\n",
        "  new_matrix = np.zeros((data.shape[0],len(all_electrodes)))\n",
        "  for column in range(data.shape[1]):\n",
        "    insert_index = all_electrodes.index(feature_names[column])\n",
        "    new_matrix[:,insert_index] = data[:,column]\n",
        "    \n",
        "  return new_matrix\n"
      ],
      "metadata": {
        "id": "d8o2edSn_CkY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Making the iterated test, validation and test sets</h2>"
      ],
      "metadata": {
        "id": "bwJu3UnzP8ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "feat_path = \"/content/features\"\n",
        "\n",
        "# Function to generate the train, val and test features and mel spectrograms\n",
        "def generate_features_and_spec():\n",
        "  #Base arrays\n",
        "  train_feat = np.empty((0, len(all_electrodes)))\n",
        "  val_feat = np.empty((0, len(all_electrodes)))\n",
        "  test_feat = np.empty((0, len(all_electrodes)))\n",
        "\n",
        "  train_spec = np.empty((0, 23))\n",
        "  val_spec = np.empty((0, 23))\n",
        "  test_spec = np.empty((0, 23))\n",
        "\n",
        "  for index, subject in enumerate(['01','02','03','04','05','06','07','08','09','10']):\n",
        "    #loading the features, feature names and mel spectrogram of the subject\n",
        "    data = np.load(os.path.join(feat_path,f'sub-{subject}_feat.npy'))\n",
        "    feature_names = np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy'))\n",
        "    spectrogram = np.load(os.path.join(feat_path,f'sub-{subject}_spec.npy'))\n",
        "\n",
        "    #splittig the features and the labels into 5 parts\n",
        "    feat_splits = np.array_split(data, 5)\n",
        "    spec_splits = np.array_split(spectrogram, 5)\n",
        "\n",
        "    #making the train, val or test arrays for the subject using a 60-20-20 ratio\n",
        "    #because of the \"index\" changing, the splitting position will iterate with each subject as well\n",
        "    subject_train_feat = np.vstack((feat_splits[index % 5],feat_splits[(index+1) % 5],feat_splits[(index+2) % 5]))\n",
        "    subject_train_spec = np.vstack((spec_splits[index % 5],spec_splits[(index+1) % 5],spec_splits[(index+2) % 5]))\n",
        "\n",
        "    subject_val_feat = feat_splits[(index+3) % 5]\n",
        "    subject_val_spec = spec_splits[(index+3) % 5]\n",
        "\n",
        "    subject_test_feat = feat_splits[(index+4) % 5]\n",
        "    subject_test_spec = spec_splits[(index+4) % 5]\n",
        "\n",
        "    #concatenating the dimensionality-adjusted features with the pre-existing feature set\n",
        "    train_feat = np.concatenate((train_feat, dim_adjust(subject_train_feat, feature_names)))\n",
        "    val_feat = np.concatenate((val_feat, dim_adjust(subject_val_feat, feature_names)))\n",
        "    test_feat = np.concatenate((test_feat, dim_adjust(subject_test_feat, feature_names)))\n",
        "\n",
        "    #concatenating the appropriate arrays with the pre-existing train, val or test labels\n",
        "    train_spec = np.concatenate((train_spec, subject_train_spec))\n",
        "    val_spec = np.concatenate((val_spec, subject_val_spec))\n",
        "    test_spec = np.concatenate((test_spec, subject_test_spec))\n",
        "\n",
        "  return train_feat, train_spec, val_feat, val_spec, test_feat, test_spec \n",
        "\n",
        "# Generating the data\n",
        "train_feat, train_spec, val_feat, val_spec, test_feat, test_spec = generate_features_and_spec()"
      ],
      "metadata": {
        "id": "3sBGVYAoc7LA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Scaling the data \n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_feat)\n",
        "train_feat = scaler.transform(train_feat)\n",
        "val_feat = scaler.transform(val_feat)\n",
        "test_feat = scaler.transform(test_feat)"
      ],
      "metadata": {
        "id": "JC2LF6YOfflo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping and exporting the data for the BiGRU models\n",
        "\n",
        "train_reshaped = np.reshape(train_feat, (train_feat.shape[0], 9, int(train_feat.shape[1]/9)))\n",
        "val_reshaped = np.reshape(val_feat, (val_feat.shape[0], 9, int(val_feat.shape[1]/9)))\n",
        "test_reshaped = np.reshape(test_feat, (test_feat.shape[0], 9, int(test_feat.shape[1]/9)))"
      ],
      "metadata": {
        "id": "qPONHuL8q9dl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Testing GRU</h2>"
      ],
      "metadata": {
        "id": "s1wd448mx7lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, Bidirectional, GRU, Flatten\n",
        "\n",
        "def create_bigru_model(channels, outputsize):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(Input(shape=(9, channels)))\n",
        "  model.add(Bidirectional(GRU(units=256, return_sequences=True, dropout=0.2)))\n",
        "  model.add(Bidirectional(GRU(units=128, return_sequences=True, dropout=0.2)))\n",
        "  model.add(Bidirectional(GRU(units=64, return_sequences=True, dropout=0.2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(outputsize))\n",
        "  return model"
      ],
      "metadata": {
        "id": "om4tn_0Ox_yd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the data generator\n",
        "from tensorflow.keras.utils import Sequence, set_random_seed\n",
        "\n",
        "set_random_seed(1234)\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    # Initialization\n",
        "    def __init__(self, data, spec, batch_size=32, dim=(9, 540), shuffle=True):\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.data = data\n",
        "        self.shuffle = shuffle\n",
        "        self.spec = spec\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Shows the number of batches per epoch\n",
        "        return int(np.floor(self.data.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate one batch of data\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        X_batch = np.empty((self.batch_size, self.dim[0], self.dim[1]))\n",
        "        y_batch = np.empty((self.batch_size, 23))\n",
        "        for index1, elem in enumerate(indexes):\n",
        "          X_batch[index1] = self.data[index1]\n",
        "          y_batch[index1] = self.spec[index1]\n",
        "\n",
        "        return X_batch, y_batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updating the index after each epoch\n",
        "        self.indexes = np.arange(self.data.shape[0])\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "# Creating the generators\n",
        "train_gen = DataGenerator(train_reshaped, train_spec, 256)\n",
        "val_gen = DataGenerator(val_reshaped, train_spec, 256)\n",
        "test_gen = DataGenerator(test_reshaped, train_spec, 256)"
      ],
      "metadata": {
        "id": "NDT_2IfWy49Z"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "early_stopping=EarlyStopping(patience=20, verbose=1, min_delta=1e-5)\n",
        "checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "model = create_bigru_model(540, 23)\n",
        "model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(learning_rate=1e-4, momentum=0.9), metrics=['mse'])\n",
        "\n",
        "model.fit(train_gen, epochs=500, verbose=1, validation_data=val_gen, callbacks=[checkpointer, early_stopping])"
      ],
      "metadata": {
        "id": "Btv9wGJwzviK",
        "outputId": "2b406aeb-7a0d-4c3c-a6f3-d930822f64da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1396/1398 [============================>.] - ETA: 0s - loss: 2.6654 - mse: 2.6654\n",
            "Epoch 1: val_loss improved from inf to 3.76635, saving model to weights1.hdf5\n",
            "1398/1398 [==============================] - 35s 19ms/step - loss: 2.6631 - mse: 2.6631 - val_loss: 3.7663 - val_mse: 3.7663\n",
            "Epoch 2/500\n",
            "1397/1398 [============================>.] - ETA: 0s - loss: 0.7730 - mse: 0.7730\n",
            "Epoch 2: val_loss improved from 3.76635 to 3.66678, saving model to weights1.hdf5\n",
            "1398/1398 [==============================] - 23s 16ms/step - loss: 0.7729 - mse: 0.7729 - val_loss: 3.6668 - val_mse: 3.6668\n",
            "Epoch 3/500\n",
            "1398/1398 [==============================] - ETA: 0s - loss: 0.5352 - mse: 0.5352\n",
            "Epoch 3: val_loss improved from 3.66678 to 3.56214, saving model to weights1.hdf5\n",
            "1398/1398 [==============================] - 23s 17ms/step - loss: 0.5352 - mse: 0.5352 - val_loss: 3.5621 - val_mse: 3.5621\n",
            "Epoch 4/500\n",
            "1398/1398 [==============================] - ETA: 0s - loss: 0.4178 - mse: 0.4178\n",
            "Epoch 4: val_loss did not improve from 3.56214\n",
            "1398/1398 [==============================] - 23s 16ms/step - loss: 0.4178 - mse: 0.4178 - val_loss: 3.5812 - val_mse: 3.5812\n",
            "Epoch 5/500\n",
            "1396/1398 [============================>.] - ETA: 0s - loss: 0.3508 - mse: 0.3508\n",
            "Epoch 5: val_loss improved from 3.56214 to 3.53116, saving model to weights1.hdf5\n",
            "1398/1398 [==============================] - 25s 18ms/step - loss: 0.3507 - mse: 0.3507 - val_loss: 3.5312 - val_mse: 3.5312\n",
            "Epoch 6/500\n",
            "1397/1398 [============================>.] - ETA: 0s - loss: 0.3069 - mse: 0.3069\n",
            "Epoch 6: val_loss did not improve from 3.53116\n",
            "1398/1398 [==============================] - 26s 18ms/step - loss: 0.3069 - mse: 0.3069 - val_loss: 3.5702 - val_mse: 3.5702\n",
            "Epoch 7/500\n",
            "1395/1398 [============================>.] - ETA: 0s - loss: 0.2777 - mse: 0.2777\n",
            "Epoch 7: val_loss did not improve from 3.53116\n",
            "1398/1398 [==============================] - 23s 16ms/step - loss: 0.2777 - mse: 0.2777 - val_loss: 3.6260 - val_mse: 3.6260\n",
            "Epoch 8/500\n",
            "1395/1398 [============================>.] - ETA: 0s - loss: 0.2554 - mse: 0.2554"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-d7a5da11a170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1443\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1445\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1446\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1754\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1756\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1757\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('weights1.hdf5')\n",
        "model.evaluate(test_gen)"
      ],
      "metadata": {
        "id": "YGGfabMt61Tz",
        "outputId": "1c300f88-f202-4ec2-82dd-d1e3761b1c83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "466/466 [==============================] - 3s 7ms/step - loss: 5.4040 - mse: 5.4040\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.404033184051514, 5.404033184051514]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Tuning the AutoEncoder for dimensionality reduction</h2>"
      ],
      "metadata": {
        "id": "MRV6pzVbmNWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "dVwyhAYq-4O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the data generator\n",
        "from tensorflow.keras.utils import Sequence, set_random_seed\n",
        "\n",
        "set_random_seed(1234)\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    # Initialization\n",
        "    def __init__(self, data, batch_size=32, dim=len(all_electrodes), shuffle=True):\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.data = data\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Shows the number of batches per epoch\n",
        "        return int(np.floor(self.data.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate one batch of data\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        batch = np.empty((self.batch_size, self.dim))\n",
        "        for index1, elem in enumerate(indexes):\n",
        "          batch[index1] = self.data[index1]\n",
        "\n",
        "        return batch, batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updating the index after each epoch\n",
        "        self.indexes = np.arange(self.data.shape[0])\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "# Creating the generators\n",
        "train_gen = DataGenerator(train_feat, 256)\n",
        "val_gen = DataGenerator(val_feat, 256)\n",
        "test_gen = DataGenerator(test_feat, 256)"
      ],
      "metadata": {
        "id": "xQ8U-ZLCCYTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "import keras_tuner as kt\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# Creating the hyperparameter tuner function\n",
        "# The AE has 5 Dense layers besides the output, and the 3rd one is the bottleneck layer\n",
        "def create_ae_optimal(hp):\n",
        "  input = Input(shape=(4860))\n",
        "  \n",
        "  hp_units_1 = hp.Int('units_1', min_value=1500, max_value=2500, step=125)\n",
        "  encoded = Dense(units=hp_units_1, activation=\"relu\", kernel_initializer='HeNormal')(input)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_1', min_value = 0.0, max_value = 0.5, default = 0.25, step = 0.05))(encoded)\n",
        "\n",
        "  hp_units_2 = hp.Int('units_2', min_value=750, max_value=1250, step=50)\n",
        "  encoded = Dense(units=hp_units_2, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_2', min_value = 0.0, max_value = 0.5, default = 0.25, step = 0.05))(encoded)\n",
        "\n",
        "  hp_units_3 = hp.Int('units_3', min_value=400, max_value=600, step=25)\n",
        "  encoded = Dense(units=hp_units_3, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  encoded = Dropout(rate=hp.Float('dropout_3', min_value = 0.0, max_value = 0.5, default = 0.25, step = 0.05))(encoded)\n",
        "\n",
        "  hp_units_4 = hp.Int('units_4', min_value=750, max_value=1250, step=50)\n",
        "  decoded = Dense(units=hp_units_4, activation=\"relu\", kernel_initializer='HeNormal')(encoded)\n",
        "  decoded = Dropout(rate=hp.Float('dropout_4', min_value = 0.0, max_value = 0.5, default = 0.25, step = 0.05))(decoded)\n",
        "\n",
        "  hp_units_5 = hp.Int('units_5', min_value=1500, max_value=2500, step=125)\n",
        "  decoded = Dense(units=hp_units_5, activation=\"relu\", kernel_initializer='HeNormal')(decoded)\n",
        "  decoded = Dropout(rate=hp.Float('dropout_5', min_value = 0.0, max_value = 0.5, default = 0.25, step = 0.05))(decoded)\n",
        "\n",
        "  output = Dense(4860)(decoded)\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  hp_momentum = hp.Choice('momentum', values=[0.9, 0.95, 0.99])\n",
        "\n",
        "  model = Model(input, output)\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=hp_learning_rate, momentum=hp_momentum),\n",
        "                loss='mse',\n",
        "                metrics=['mse'])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "R_ZSI31I-4zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the tuner\n",
        "tuner = kt.Hyperband(create_ae_optimal,\n",
        "                     objective='val_loss',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='/content/ae_opt',\n",
        "                     project_name='ae_opt1')"
      ],
      "metadata": {
        "id": "kdUDep51B7_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "#note: takes about 15-20 minutes to optimize\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=10)\n",
        "tuner.search(train_gen, epochs=100, validation_data=val_gen, verbose=0, shuffle=True, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "ISWZG5BNCJ7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting and retraining the model with the best params\n",
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "early_stopping=EarlyStopping(patience=20, verbose=1, min_delta=1e-5)\n",
        "checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "hypermodel.fit(train_gen, epochs=500, verbose=1, validation_data=val_gen, callbacks=[checkpointer, early_stopping])"
      ],
      "metadata": {
        "id": "_lpNSFOpDDJj",
        "outputId": "be877ed7-c39d-43d9-ded6-b5d4a838c237",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.3036 - mse: 0.3036\n",
            "Epoch 1: val_loss improved from inf to 0.29726, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 14s 32ms/step - loss: 0.3030 - mse: 0.3030 - val_loss: 0.2973 - val_mse: 0.2973\n",
            "Epoch 2/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0628 - mse: 0.0628\n",
            "Epoch 2: val_loss improved from 0.29726 to 0.29106, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.0628 - mse: 0.0628 - val_loss: 0.2911 - val_mse: 0.2911\n",
            "Epoch 3/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 3: val_loss improved from 0.29106 to 0.26702, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.2670 - val_mse: 0.2670\n",
            "Epoch 4/500\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0318 - mse: 0.0318\n",
            "Epoch 4: val_loss improved from 0.26702 to 0.26269, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 10s 29ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.2627 - val_mse: 0.2627\n",
            "Epoch 5/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0273 - mse: 0.0273\n",
            "Epoch 5: val_loss improved from 0.26269 to 0.26153, saving model to weights1.hdf5\n",
            "349/349 [==============================] - 10s 30ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.2615 - val_mse: 0.2615\n",
            "Epoch 6/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0243 - mse: 0.0243\n",
            "Epoch 6: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.2797 - val_mse: 0.2797\n",
            "Epoch 7/500\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0220 - mse: 0.0220\n",
            "Epoch 7: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.2976 - val_mse: 0.2976\n",
            "Epoch 8/500\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0204 - mse: 0.0204\n",
            "Epoch 8: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.3074 - val_mse: 0.3074\n",
            "Epoch 9/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0190 - mse: 0.0190\n",
            "Epoch 9: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.3137 - val_mse: 0.3137\n",
            "Epoch 10/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0180 - mse: 0.0180\n",
            "Epoch 10: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.3292 - val_mse: 0.3292\n",
            "Epoch 11/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0170 - mse: 0.0170\n",
            "Epoch 11: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.3194 - val_mse: 0.3194\n",
            "Epoch 12/500\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0161 - mse: 0.0161\n",
            "Epoch 12: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 28ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.3207 - val_mse: 0.3207\n",
            "Epoch 13/500\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0152 - mse: 0.0152\n",
            "Epoch 13: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.3185 - val_mse: 0.3185\n",
            "Epoch 14/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0145 - mse: 0.0145\n",
            "Epoch 14: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.3079 - val_mse: 0.3079\n",
            "Epoch 15/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0139 - mse: 0.0139\n",
            "Epoch 15: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.3028 - val_mse: 0.3028\n",
            "Epoch 16/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0132 - mse: 0.0132\n",
            "Epoch 16: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.3021 - val_mse: 0.3021\n",
            "Epoch 17/500\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0126 - mse: 0.0126\n",
            "Epoch 17: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.2997 - val_mse: 0.2997\n",
            "Epoch 18/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0122 - mse: 0.0122\n",
            "Epoch 18: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.3009 - val_mse: 0.3009\n",
            "Epoch 19/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0117 - mse: 0.0117\n",
            "Epoch 19: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.3058 - val_mse: 0.3058\n",
            "Epoch 20/500\n",
            "349/349 [==============================] - ETA: 0s - loss: 0.0115 - mse: 0.0115\n",
            "Epoch 20: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.3156 - val_mse: 0.3156\n",
            "Epoch 21/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0114 - mse: 0.0114\n",
            "Epoch 21: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.3096 - val_mse: 0.3096\n",
            "Epoch 22/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0107 - mse: 0.0107\n",
            "Epoch 22: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.3133 - val_mse: 0.3133\n",
            "Epoch 23/500\n",
            "348/349 [============================>.] - ETA: 0s - loss: 0.0108 - mse: 0.0108\n",
            "Epoch 23: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.3212 - val_mse: 0.3212\n",
            "Epoch 24/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0103 - mse: 0.0103\n",
            "Epoch 24: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 10s 27ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.3303 - val_mse: 0.3303\n",
            "Epoch 25/500\n",
            "347/349 [============================>.] - ETA: 0s - loss: 0.0102 - mse: 0.0102\n",
            "Epoch 25: val_loss did not improve from 0.26153\n",
            "349/349 [==============================] - 9s 27ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.3413 - val_mse: 0.3413\n",
            "Epoch 25: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fae439d2820>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation \n",
        "hypermodel.load_weights('weights1.hdf5')\n",
        "hypermodel.evaluate(test_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6gl1GEa6NAI",
        "outputId": "5b2c46e0-17c0-4acc-a24e-e1a7d29c6cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "116/116 [==============================] - 2s 14ms/step - loss: 0.2699 - mse: 0.2699\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.26994505524635315, 0.26994505524635315]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Generating and exporting the lower dimensional data</h2>"
      ],
      "metadata": {
        "id": "GYs0QLaYhisH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hypermodel.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QWuY1lchrS4",
        "outputId": "c734f743-fb07-40d6-8d0b-1bb4d60759a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4860)]            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1625)              7899125   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 1625)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1250)              2032500   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 1250)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 575)               719325    \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 575)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 850)               489600    \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 850)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1625)              1382875   \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 1625)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 4860)              7902360   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,425,785\n",
            "Trainable params: 20,425,785\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only the encoder\n",
        "model2= Model(inputs=hypermodel.input, outputs=hypermodel.layers[-7].output)\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrTwZhiyhv8Q",
        "outputId": "0ea1587f-d307-4069-d454-6bf4c371dd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4860)]            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1625)              7899125   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 1625)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1250)              2032500   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 1250)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 575)               719325    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,650,950\n",
            "Trainable params: 10,650,950\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the new data using the output of the bottleneck layer\n",
        "train_new = model2.predict(train_gen)\n",
        "val_new = model2.predict(val_gen)\n",
        "test_new = model2.predict(test_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqrNk9pKiiHa",
        "outputId": "0465dffd-0ede-4a75-ba71-24753ea5ab22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "349/349 [==============================] - 5s 14ms/step\n",
            "116/116 [==============================] - 2s 14ms/step\n",
            "116/116 [==============================] - 2s 15ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the feature-label set pairs as pickle files onto Google Drive\n",
        "import pickle\n",
        "\n",
        "with open('train.pkl', 'wb') as f:  \n",
        "    pickle.dump([train_new, train_spec], f)\n",
        "\n",
        "with open('val.pkl', 'wb') as f:  \n",
        "    pickle.dump([val_new, val_spec], f)\n",
        "\n",
        "with open('test.pkl', 'wb') as f:  \n",
        "    pickle.dump([test_new, test_spec], f)"
      ],
      "metadata": {
        "id": "UWTKZceejPgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp train.pkl drive/MyDrive/DeepLearning/train.pkl\n",
        "!cp val.pkl drive/MyDrive/DeepLearning/val.pkl\n",
        "!cp test.pkl drive/MyDrive/DeepLearning/test.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lhk7ec0pyg_",
        "outputId": "4861f047-1c7d-4814-937e-3ca2d837febf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Option 2 for dimensionality reduction: Incremental PCA</h2>"
      ],
      "metadata": {
        "id": "q9cD1KX0ajBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "n_comp = 200\n",
        "\n",
        "# note: takes about 10 minutes to run\n",
        "pca = IncrementalPCA(n_components=n_comp, batch_size=1024)\n",
        "pca.fit(train_feat)\n",
        "train_feat = pca.transform(train_feat)\n",
        "val_feat = pca.transform(val_feat)\n",
        "test_feat = pca.transform(test_feat)"
      ],
      "metadata": {
        "id": "AARr0wkEaqfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the new data\n",
        "\n",
        "with open('train_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([train_feat, train_spec], f)\n",
        "\n",
        "with open('val_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([val_feat, val_spec], f)\n",
        "\n",
        "with open('test_v2.pkl', 'wb') as f:  \n",
        "    pickle.dump([test_feat, test_spec], f)\n",
        "\n",
        "# Copying to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp train_v2.pkl drive/MyDrive/DeepLearning/train_v2.pkl\n",
        "!cp val_v2.pkl drive/MyDrive/DeepLearning/val_v2.pkl\n",
        "!cp test_v2.pkl drive/MyDrive/DeepLearning/test_v2.pkl"
      ],
      "metadata": {
        "id": "QbzPyjaBdZeu",
        "outputId": "d2e803c8-742c-4b1d-bb50-71eb76a20ee1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Z2sk5E2eYPA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}