{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/v-artur/Golden_Oreos/blob/main/Modeling.ipynb)"
      ],
      "metadata": {
        "id": "-YwF9Hzt2euf"
      },
      "id": "-YwF9Hzt2euf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import files from drive"
      ],
      "metadata": {
        "id": "gVCNOaaoDSbO"
      },
      "id": "gVCNOaaoDSbO"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a2005cbc",
      "metadata": {
        "id": "a2005cbc",
        "outputId": "518ce42b-bc83-4f6f-cc91-1cabff12cedd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spectrogram reconstruction "
      ],
      "metadata": {
        "id": "QUch8ZTsGET3"
      },
      "id": "QUch8ZTsGET3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.) One person model"
      ],
      "metadata": {
        "id": "SFlhVaHrGjo-"
      },
      "id": "SFlhVaHrGjo-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for modeling and predicting"
      ],
      "metadata": {
        "id": "HPnNLUrkzd3J"
      },
      "id": "HPnNLUrkzd3J"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM, GRU, Conv1D, MaxPooling1D, Flatten, TimeDistributed\n",
        "import numpy as np\n",
        "\n",
        "# Bottleneck FC-DNN\n",
        "\n",
        "def create_ae_model(inputsize, outputsize):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(inputsize)))\n",
        "    model.add(tf.keras.layers.Dense(128, activation=\"relu\", kernel_initializer='HeNormal'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(64, activation=\"relu\", kernel_initializer='HeNormal'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(16, activation=\"relu\", kernel_initializer='HeNormal'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(4, activation=\"relu\", kernel_initializer='HeNormal'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(16, activation=\"relu\", kernel_initializer='HeNormal'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(outputsize))\n",
        "    return model\n",
        "\n",
        "\n",
        "# LSTM/GRU Network\n",
        "\n",
        "def create_LSTM_model(inputsize, outputsize):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(GRU(units=300, dropout=0.2, return_sequences=True, input_shape=(inputsize,1)))\n",
        "  model.add(GRU(units=300, dropout=0.2, return_sequences=True, input_shape=(inputsize,1)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(outputsize))\n",
        "  return(model)\n",
        "\n",
        "\n",
        "# CNN network\n",
        "def create_cnn_model(rows_per_feature, cols_per_feature, outputsize):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(Conv1D(10, 8, activation=\"relu\", input_shape=(rows_per_feature, cols_per_feature,)))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Conv1D(10, 20, activation=\"relu\"))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units=outputsize, activation='linear'))\n",
        "  return(model)\n",
        "\n",
        "# to make a 2D object out of the EEG featurevector with sliding window\n",
        "# windowsize is minimum 10 and multiple of 2.\n",
        "def cnnize(data, window):\n",
        "  data = np.asarray(data)\n",
        "  #the new array, the first coordinate is the number of samples,\n",
        "  #the second and the third are the new features\n",
        "  X = np.zeros((data.shape[0], int((data.shape[1]-window)/2)+1 , window))\n",
        "  for index, elem in enumerate(data):\n",
        "    # creating the feature-parts\n",
        "    new_elems = np.array([elem[i:i+window] for i in range(0, data.shape[1] - window + 1, 2)])\n",
        "    X[index] = new_elems\n",
        "  return X\n"
      ],
      "metadata": {
        "id": "Wk9elaV4zgv3"
      },
      "id": "Wk9elaV4zgv3",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting seed\n",
        "tf.keras.utils.set_random_seed(1234)"
      ],
      "metadata": {
        "id": "u2bzTs6zEeXd"
      },
      "id": "u2bzTs6zEeXd",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizing the FC-DNN structure for one person\n",
        "\n"
      ],
      "metadata": {
        "id": "wZ5bO6U_yM5h"
      },
      "id": "wZ5bO6U_yM5h"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "data = np.load(r'/content/drive/MyDrive/DeepLearning/features/sub-01_feat.npy')\n",
        "spectrogram = np.load(r'/content/drive/MyDrive/DeepLearning/features/sub-01_spec.npy')\n",
        "\n",
        "#Inital parameters\n",
        "nfolds = 10\n",
        "kf = KFold(nfolds,shuffle=False)\n",
        "pca = PCA()\n",
        "numComps = 200\n",
        "explainedVariance = np.zeros(nfolds)\n",
        "val_split = 0.2\n",
        "\n",
        "\n",
        "#Initialize an empty spectrogram to save the reconstruction to\n",
        "rec_spec = np.zeros(spectrogram.shape)\n",
        "#Save the correlation coefficients for each fold\n",
        "rs = np.zeros((nfolds,spectrogram.shape[1]))\n",
        "for k,(train, test) in enumerate(kf.split(data)):\n",
        "          \n",
        "    #Train, validation and test data\n",
        "    X_train_temp = data[train,:]\n",
        "    y_train_temp = spectrogram[train,:]\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=val_split, random_state=0)\n",
        "    X_test = data[test,:]\n",
        "    y_test = spectrogram[test,:] # this one might not be needed\n",
        "    \n",
        "    #Normalization\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train[:] = scaler.transform(X_train)\n",
        "    X_val[:] = scaler.transform(X_val)\n",
        "    X_test[:] = scaler.transform(X_test)\n",
        "\n",
        "    #Fit PCA to training data\n",
        "    pca.fit(X_train)\n",
        "    #Get percentage of explained variance by selected components\n",
        "    explainedVariance[k] = np.sum(pca.explained_variance_ratio_[:numComps])\n",
        "    #Tranform data \n",
        "    X_train = np.dot(X_train, pca.components_[:numComps,:].T)\n",
        "    X_val = np.dot(X_val, pca.components_[:numComps,:].T)\n",
        "    X_test = np.dot(X_test, pca.components_[:numComps,:].T)\n",
        "\n",
        "    # Bottleneck model\n",
        "    early_stopping=EarlyStopping(patience=25, verbose=1, min_delta=1e-5)\n",
        "    checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "    model = create_ae_model(numComps, spectrogram.shape[1])\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
        "    model.fit(X_train, y_train, batch_size=64, \n",
        "              epochs=500, verbose=0, validation_data=(X_val, y_val), shuffle=True,\n",
        "              callbacks=[checkpointer, early_stopping])\n",
        "            \n",
        "    #predict with the Autoencoder\n",
        "    model.load_weights('weights1.hdf5')\n",
        "    rec_spec[test, :] = model.predict(X_test, verbose=0)\n",
        "\n",
        "    #Evaluate reconstruction of this fold\n",
        "    for specBin in range(spectrogram.shape[1]):\n",
        "         r, p = pearsonr(spectrogram[test, specBin], rec_spec[test, specBin])\n",
        "         rs[k,specBin] = r\n",
        "\n",
        "\n",
        "#Show evaluation result\n",
        "print('mean correlation', np.mean(rs))"
      ],
      "metadata": {
        "id": "ZHp3r8AIyUfI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a195712e-bd99-4709-ba03-377f574d8a54"
      },
      "id": "ZHp3r8AIyUfI",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 5.08743, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss improved from 5.08743 to 3.29884, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 3: val_loss improved from 3.29884 to 2.55712, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 4: val_loss improved from 2.55712 to 2.20429, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 5: val_loss improved from 2.20429 to 2.04845, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 6: val_loss improved from 2.04845 to 1.91544, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 7: val_loss improved from 1.91544 to 1.71705, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 8: val_loss improved from 1.71705 to 1.50765, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 9: val_loss improved from 1.50765 to 1.50368, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.50368\n",
            "\n",
            "Epoch 11: val_loss did not improve from 1.50368\n",
            "\n",
            "Epoch 12: val_loss improved from 1.50368 to 1.47402, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 13: val_loss did not improve from 1.47402\n",
            "\n",
            "Epoch 14: val_loss improved from 1.47402 to 1.46418, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 15: val_loss improved from 1.46418 to 1.38421, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 16: val_loss did not improve from 1.38421\n",
            "\n",
            "Epoch 17: val_loss did not improve from 1.38421\n",
            "\n",
            "Epoch 18: val_loss improved from 1.38421 to 1.19393, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 19: val_loss did not improve from 1.19393\n",
            "\n",
            "Epoch 20: val_loss did not improve from 1.19393\n",
            "\n",
            "Epoch 21: val_loss did not improve from 1.19393\n",
            "\n",
            "Epoch 22: val_loss improved from 1.19393 to 1.18906, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 23: val_loss did not improve from 1.18906\n",
            "\n",
            "Epoch 24: val_loss did not improve from 1.18906\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ab1b2aa19223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     model.fit(X_train, y_train, batch_size=64, \n\u001b[1;32m     57\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m               callbacks=[checkpointer, early_stopping])\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m#predict with the Autoencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizing the LSTM structure for one person"
      ],
      "metadata": {
        "id": "YuDJaqHQoNR3"
      },
      "id": "YuDJaqHQoNR3"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "data = np.load(r'/content/drive/MyDrive/DeepLearning/features/sub-01_feat.npy')\n",
        "spectrogram = np.load(r'/content/drive/MyDrive/DeepLearning/features/sub-01_spec.npy')\n",
        "\n",
        "#Inital parameters\n",
        "nfolds = 10\n",
        "kf = KFold(nfolds,shuffle=False)\n",
        "pca = PCA()\n",
        "numComps = 300\n",
        "explainedVariance = np.zeros(nfolds)\n",
        "val_split = 0.2\n",
        "\n",
        "\n",
        "#Initialize an empty spectrogram to save the reconstruction to\n",
        "rec_spec = np.zeros(spectrogram.shape)\n",
        "#Save the correlation coefficients for each fold\n",
        "rs = np.zeros((nfolds,spectrogram.shape[1]))\n",
        "for k,(train, test) in enumerate(kf.split(data)):         \n",
        "    #Train, validation and test data\n",
        "    X_train_temp = data[train,:]\n",
        "    y_train_temp = spectrogram[train,:]\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=val_split, random_state=0, shuffle=False)\n",
        "    X_test = data[test,:]\n",
        "    y_test = spectrogram[test,:] # this one might not be needed\n",
        "    \n",
        "    #Normalization\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train[:] = scaler.transform(X_train)\n",
        "    X_val[:] = scaler.transform(X_val)\n",
        "    X_test[:] = scaler.transform(X_test)\n",
        "\n",
        "    #Fit PCA to training data\n",
        "    pca.fit(X_train)\n",
        "    #Get percentage of explained variance by selected components\n",
        "    explainedVariance[k] = np.sum(pca.explained_variance_ratio_[:numComps])\n",
        "    #Tranform data \n",
        "    X_train = np.dot(X_train, pca.components_[:numComps,:].T)\n",
        "    X_val = np.dot(X_val, pca.components_[:numComps,:].T)\n",
        "    X_test = np.dot(X_test, pca.components_[:numComps,:].T)\n",
        "\n",
        "    # LSTM model\n",
        "    early_stopping=EarlyStopping(patience=25, verbose=1, min_delta=1e-5)\n",
        "    checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "    model = create_LSTM_model(numComps, spectrogram.shape[1])\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
        "    model.fit(X_train, y_train, batch_size=64, \n",
        "              epochs=150, verbose=0, validation_data=(X_val, y_val), shuffle=True,\n",
        "              callbacks=[checkpointer, early_stopping])\n",
        "            \n",
        "    #predict with the Autoencoder\n",
        "    model.load_weights('weights1.hdf5')\n",
        "    rec_spec[test, :] = model.predict(X_test, verbose=0)\n",
        "\n",
        "    #Evaluate reconstruction of this fold\n",
        "    for specBin in range(spectrogram.shape[1]):\n",
        "         r, p = pearsonr(spectrogram[test, specBin], rec_spec[test, specBin])\n",
        "         rs[k,specBin] = r\n",
        "\n",
        "\n",
        "#Show evaluation result\n",
        "print('mean correlation', np.mean(rs))"
      ],
      "metadata": {
        "id": "ifhJLOucoM3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9694a797-55a9-423e-cd9e-54388613b7ca"
      },
      "id": "ifhJLOucoM3d",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 2.36893, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 3: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 4: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 5: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 6: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 7: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 11: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 14: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.36893\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.36893\n",
            "Epoch 26: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 2.37300, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 3: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 4: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 5: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 6: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 7: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 11: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 14: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.37300\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.37300\n",
            "Epoch 26: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 2.33568, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 3: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 4: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 5: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 6: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 7: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 11: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 14: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.33568\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.33568\n",
            "Epoch 26: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 2.40489, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 3: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 4: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 5: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 6: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 7: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 11: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 14: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.40489\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.40489\n",
            "Epoch 26: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 2.47018, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss did not improve from 2.47018\n",
            "\n",
            "Epoch 3: val_loss improved from 2.47018 to 2.41638, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 4: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 5: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 6: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 7: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 11: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 14: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 27: val_loss did not improve from 2.41638\n",
            "\n",
            "Epoch 28: val_loss did not improve from 2.41638\n",
            "Epoch 28: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 2.37603, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 3: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 4: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 5: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 6: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 7: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 11: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 14: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.37603\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.37603\n",
            "Epoch 26: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 2.41114, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 3: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 4: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 5: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 6: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 7: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 11: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 14: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.41114\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.41114\n",
            "Epoch 26: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 2.42622, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 3: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 4: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 5: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 6: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 7: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 11: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 14: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.42622\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.42622\n",
            "Epoch 26: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 2.60844, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 3: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 4: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 5: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 6: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 7: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 11: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 14: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.60844\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.60844\n",
            "Epoch 26: early stopping\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 2.11427, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 3: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 4: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 5: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 6: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 7: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 11: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 14: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.11427\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.11427\n",
            "Epoch 26: early stopping\n",
            "mean correlation 0.5629516076353095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizing the CNN structure for one person"
      ],
      "metadata": {
        "id": "JASkuUZGO6Di"
      },
      "id": "JASkuUZGO6Di"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "data = np.load(r'/content/drive/MyDrive/DeepLearning/features/sub-01_feat.npy')\n",
        "spectrogram = np.load(r'/content/drive/MyDrive/DeepLearning/features/sub-01_spec.npy')\n",
        "\n",
        "#Inital parameters\n",
        "nfolds = 10\n",
        "kf = KFold(nfolds,shuffle=False)\n",
        "pca = PCA()\n",
        "numComps = 200\n",
        "explainedVariance = np.zeros(nfolds)\n",
        "val_split = 0.2\n",
        "window = 20\n",
        "\n",
        "\n",
        "#Initialize an empty spectrogram to save the reconstruction to\n",
        "rec_spec = np.zeros(spectrogram.shape)\n",
        "#Save the correlation coefficients for each fold\n",
        "rs = np.zeros((nfolds,spectrogram.shape[1]))\n",
        "for k,(train, test) in enumerate(kf.split(data)):         \n",
        "    #Train, validation and test data\n",
        "    X_train_temp = data[train,:]\n",
        "    y_train_temp = spectrogram[train,:]\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=val_split, random_state=0, shuffle=False)\n",
        "    X_test = data[test,:]\n",
        "    y_test = spectrogram[test,:] # this one might not be needed\n",
        "    \n",
        "    #Normalization\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train[:] = scaler.transform(X_train)\n",
        "    X_val[:] = scaler.transform(X_val)\n",
        "    X_test[:] = scaler.transform(X_test)\n",
        "\n",
        "    #Fit PCA to training data\n",
        "    pca.fit(X_train)\n",
        "    #Get percentage of explained variance by selected components\n",
        "    explainedVariance[k] = np.sum(pca.explained_variance_ratio_[:numComps])\n",
        "    #Tranform data \n",
        "    X_train = np.dot(X_train, pca.components_[:numComps,:].T)\n",
        "    X_val = np.dot(X_val, pca.components_[:numComps,:].T)\n",
        "    X_test = np.dot(X_test, pca.components_[:numComps,:].T)\n",
        "\n",
        "    #reshaping the data\n",
        "    X_train = cnnize(X_train, window)\n",
        "    X_val = cnnize(X_val, window)\n",
        "    X_test = cnnize(X_test, window)\n",
        "\n",
        "    # CNN model\n",
        "    early_stopping=EarlyStopping(patience=25, verbose=1, min_delta=1e-5)\n",
        "    checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "    model = create_cnn_model(int((numComps-window)/2)+1, window, spectrogram.shape[1])\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
        "    model.fit(X_train, y_train, batch_size=64, \n",
        "              epochs=150, verbose=0, validation_data=(X_val, y_val), shuffle=True,\n",
        "              callbacks=[checkpointer, early_stopping])\n",
        "            \n",
        "    #predict with the Autoencoder\n",
        "    model.load_weights('weights1.hdf5')\n",
        "    rec_spec[test, :] = model.predict(X_test, verbose=0)\n",
        "\n",
        "    #Evaluate reconstruction of this fold\n",
        "    for specBin in range(spectrogram.shape[1]):\n",
        "         r, p = pearsonr(spectrogram[test, specBin], rec_spec[test, specBin])\n",
        "         rs[k,specBin] = r\n",
        "\n",
        "\n",
        "#Show evaluation result\n",
        "print('mean correlation', np.mean(rs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JmcptaXXO1Sj",
        "outputId": "4fddf724-e61b-4041-e957-a3e67bd975e9"
      },
      "id": "JmcptaXXO1Sj",
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 3.65074, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 2: val_loss improved from 3.65074 to 3.44781, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 3: val_loss improved from 3.44781 to 3.12265, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 4: val_loss did not improve from 3.12265\n",
            "\n",
            "Epoch 5: val_loss improved from 3.12265 to 3.09337, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 6: val_loss improved from 3.09337 to 2.97692, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 7: val_loss improved from 2.97692 to 2.92251, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.92251\n",
            "\n",
            "Epoch 9: val_loss did not improve from 2.92251\n",
            "\n",
            "Epoch 10: val_loss did not improve from 2.92251\n",
            "\n",
            "Epoch 11: val_loss improved from 2.92251 to 2.87664, saving model to weights1.hdf5\n",
            "\n",
            "Epoch 12: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 13: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 14: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 15: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 16: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 17: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 18: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 19: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 20: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 21: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 22: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 23: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 24: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 25: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 26: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 27: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 28: val_loss did not improve from 2.87664\n",
            "\n",
            "Epoch 29: val_loss did not improve from 2.87664\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-dfc07efd962f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m     model.fit(X_train, y_train, batch_size=64, \n\u001b[1;32m     62\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m               callbacks=[checkpointer, early_stopping])\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m#predict with the Autoencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.) Trying it out for every subject"
      ],
      "metadata": {
        "id": "8puXSrNvPks0"
      },
      "id": "8puXSrNvPks0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c0796a",
      "metadata": {
        "id": "31c0796a",
        "outputId": "fe1e224e-354d-4f60-ed10-dfef7396dffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a44b39bb2b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m             model.fit(trainData, spectrogram[train, :], batch_size=64, \n\u001b[1;32m     93\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                       callbacks=[checkpointer, early_stopping])\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m#predict with the Autoencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wavfile\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#import reconstructWave as rW\n",
        "#import MelFilterBank as mel\n",
        "\n",
        "# #Optional function\n",
        "# def createAudio(spectrogram, audiosr=16000, winLength=0.05, frameshift=0.01):\n",
        "#     mfb = mel.MelFilterBank(int((audiosr*winLength)/2+1), spectrogram.shape[1], audiosr)\n",
        "#     nfolds = 10\n",
        "#     hop = int(spectrogram.shape[0]/nfolds)\n",
        "#     rec_audio = np.array([])\n",
        "#     for_reconstruction = mfb.fromLogMels(spectrogram)\n",
        "#     for w in range(0,spectrogram.shape[0],hop):\n",
        "#         spec = for_reconstruction[w:min(w+hop,for_reconstruction.shape[0]),:]\n",
        "#         rec = rW.reconstructWavFromSpectrogram(spec,spec.shape[0]*spec.shape[1],fftsize=int(audiosr*winLength),overlap=int(winLength/frameshift))\n",
        "#         rec_audio = np.append(rec_audio,rec)\n",
        "#     scaled = np.int16(rec_audio/np.max(np.abs(rec_audio)) * 32767)\n",
        "#     return scaled\n",
        "\n",
        "\n",
        "##### MODELING ########\n",
        "\n",
        "model = True\n",
        "\n",
        "scores= []\n",
        "\n",
        "if model == True:\n",
        "    feat_path = r'/content/drive/MyDrive/DeepLearning/features'\n",
        "    result_path = r'/content/drive/MyDrive/DeepLearning/features'\n",
        "    pts = ['sub-%02d'%i for i in range(1,11)]\n",
        "\n",
        "    winLength = 0.05\n",
        "    frameshift = 0.01\n",
        "    audiosr = 16000\n",
        "\n",
        "    nfolds = 15\n",
        "    kf = KFold(nfolds,shuffle=False)\n",
        "    pca = PCA()\n",
        "    numComps = 400\n",
        "    \n",
        "    #Initialize empty matrices for correlation results, randomized controls and amount of explained variance\n",
        "    allRes = np.zeros((len(pts),nfolds,23))\n",
        "    explainedVariance = np.zeros((len(pts),nfolds))\n",
        "    numRands = 1000\n",
        "    randomControl = np.zeros((len(pts),numRands, 23))\n",
        "\n",
        "    for pNr, pt in enumerate(pts):\n",
        "        \n",
        "        \n",
        "        #Load the data\n",
        "        #Dimensions of these data vary depending on the subject\n",
        "        spectrogram = np.load(os.path.join(feat_path,f'{pt}_spec.npy'))  \n",
        "        data = np.load(os.path.join(feat_path,f'{pt}_feat.npy'))\n",
        "        labels = np.load(os.path.join(feat_path,f'{pt}_procWords.npy'))\n",
        "        featName = np.load(os.path.join(feat_path,f'{pt}_feat_names.npy'))\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        #Initialize an empty spectrogram to save the reconstruction to\n",
        "        rec_spec = np.zeros(spectrogram.shape)\n",
        "        #Save the correlation coefficients for each fold\n",
        "        rs = np.zeros((nfolds,spectrogram.shape[1]))\n",
        "        for k,(train, test) in enumerate(kf.split(data)):\n",
        "          \n",
        "            #Normalization\n",
        "            mu=np.mean(data[train,:],axis=0)\n",
        "            std=np.std(data[train,:],axis=0)\n",
        "            trainData=(data[train,:]-mu)/std\n",
        "            testData=(data[test,:]-mu)/std\n",
        "\n",
        "            #Fit PCA to training data\n",
        "            pca.fit(trainData)\n",
        "            #Get percentage of explained variance by selected components\n",
        "            explainedVariance[pNr,k] = np.sum(pca.explained_variance_ratio_[:numComps])\n",
        "            #Tranform data into component space\n",
        "            trainData = np.dot(trainData, pca.components_[:numComps,:].T)\n",
        "            testData = np.dot(testData, pca.components_[:numComps,:].T)\n",
        "\n",
        "            early_stopping=EarlyStopping(patience=25, verbose=0, min_delta=1e-5)\n",
        "            checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=0)\n",
        "\n",
        "            #Autoencoder\n",
        "            model = create_ae_model(trainData.shape[1], spectrogram.shape[1])\n",
        "            model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
        "            model.fit(trainData, spectrogram[train, :], batch_size=64, \n",
        "                      epochs=500, verbose=0, validation_split=1/9, shuffle=True,\n",
        "                      callbacks=[checkpointer, early_stopping])\n",
        "            \n",
        "            #predict with the Autoencoder\n",
        "            rec_spec[test, :] = model.predict(testData, verbose=0)\n",
        "\n",
        "            #Evaluate reconstruction of this fold\n",
        "            for specBin in range(spectrogram.shape[1]):\n",
        "                if np.any(np.isnan(rec_spec)):\n",
        "                    print('%s has %d broken samples in reconstruction' % (pt, np.sum(np.isnan(rec_spec))))\n",
        "                r, p = pearsonr(spectrogram[test, specBin], rec_spec[test, specBin])\n",
        "                rs[k,specBin] = r\n",
        "\n",
        "        #Show evaluation result\n",
        "        print('%s has mean correlation of %f' % (pt, np.mean(rs)))\n",
        "        allRes[pNr,:,:]=rs\n",
        "        scores.append(np.mean(rs))\n",
        "\n",
        "#         #Estimate random baseline\n",
        "#         for randRound in range(numRands):\n",
        "#             #Choose a random splitting point at least 10% of the dataset size away\n",
        "#             splitPoint = np.random.choice(np.arange(int(spectrogram.shape[0]*0.1),int(spectrogram.shape[0]*0.9)))\n",
        "#             #Swap the dataset on the splitting point \n",
        "#             shuffled = np.concatenate((spectrogram[splitPoint:,:],spectrogram[:splitPoint,:]))\n",
        "#             #Calculate the correlations\n",
        "#             for specBin in range(spectrogram.shape[1]):\n",
        "#                 if np.any(np.isnan(rec_spec)):\n",
        "#                     print('%s has %d broken samples in reconstruction' % (pt, np.sum(np.isnan(rec_spec))))\n",
        "#                 r, p = pearsonr(spectrogram[:,specBin], shuffled[:,specBin])\n",
        "#                 randomControl[pNr, randRound,specBin]=r\n",
        "\n",
        "\n",
        "#         #Save reconstructed spectrogram\n",
        "#         os.makedirs(os.path.join(result_path), exist_ok=True)\n",
        "#         np.save(os.path.join(result_path,f'{pt}_predicted_spec.npy'), rec_spec)\n",
        "        \n",
        "#         #Synthesize waveform from spectrogram using Griffin-Lim\n",
        "#         reconstructedWav = createAudio(rec_spec,audiosr=audiosr,winLength=winLength,frameshift=frameshift)\n",
        "#         wavfile.write(os.path.join(result_path,f'{pt}_predicted.wav'),int(audiosr),reconstructedWav)\n",
        "\n",
        "#         #For comparison synthesize the original spectrogram with Griffin-Lim\n",
        "#         origWav = createAudio(spectrogram,audiosr=audiosr,winLength=winLength,frameshift=frameshift)\n",
        "#         wavfile.write(os.path.join(result_path,f'{pt}_orig_synthesized.wav'),int(audiosr),origWav)\n",
        "\n",
        "#     #Save results in numpy arrays          \n",
        "#     np.save(os.path.join(result_path,'linearResults.npy'),allRes)\n",
        "#     np.save(os.path.join(result_path,'randomResults.npy'),randomControl)\n",
        "#     np.save(os.path.join(result_path,'explainedVariance.npy'),explainedVariance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a6e6b40",
      "metadata": {
        "id": "0a6e6b40",
        "outputId": "4e3b10f6-a443-47ac-e2b6-c6eba396472d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5629875535958715,\n",
              " 0.615317710223286,\n",
              " 0.8574981748428402,\n",
              " 0.816490556136459,\n",
              " 0.5712122543230624,\n",
              " 0.8819938942481294,\n",
              " 0.7771201475020989,\n",
              " 0.6878816278290169,\n",
              " 0.7386449014724298,\n",
              " 0.7146618900721041]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R6FcMJWfW_99"
      },
      "id": "R6FcMJWfW_99",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}