{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOevkE3Pszo1k8Yci+s6J8K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v-artur/Golden_Oreos/blob/main/speaker_indep_bigru_conv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtaining the data"
      ],
      "metadata": {
        "id": "QRznZ-paPxRN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3F36vS9-11r",
        "outputId": "ca8fe6b6-0270-40e0-b4d5-b4d76a6b0992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-09 07:21:10--  https://docs.google.com/uc?export=download&confirm=t&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.125.102, 108.177.125.139, 108.177.125.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.125.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/usphsnju7cn0bgk6897l018i9lc74i2u/1670570400000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=2190d4b1-aec8-4159-a55a-5ddb1b93a41a [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-09 07:21:11--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/usphsnju7cn0bgk6897l018i9lc74i2u/1670570400000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=2190d4b1-aec8-4159-a55a-5ddb1b93a41a\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 64.233.188.132, 2404:6800:4008:c06::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|64.233.188.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2141405093 (2.0G) [application/x-zip-compressed]\n",
            "Saving to: ‘features.zip’\n",
            "\n",
            "features.zip        100%[===================>]   1.99G  42.6MB/s    in 47s     \n",
            "\n",
            "2022-12-09 07:21:59 (43.2 MB/s) - ‘features.zip’ saved [2141405093/2141405093]\n",
            "\n",
            "--2022-12-09 07:22:01--  https://docs.google.com/uc?export=download&confirm=&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.125.102, 108.177.125.139, 108.177.125.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.125.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/is0fbgu6gi2br1pfv7imvhne6usro9l7/1670570475000/17895932938140350971/*/1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80?e=download&uuid=077cec6c-90f0-46ad-b4ca-e98d93fa851c [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-09 07:22:02--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/is0fbgu6gi2br1pfv7imvhne6usro9l7/1670570475000/17895932938140350971/*/1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80?e=download&uuid=077cec6c-90f0-46ad-b4ca-e98d93fa851c\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 64.233.188.132, 2404:6800:4008:c06::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|64.233.188.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9238 (9.0K) [application/x-zip-compressed]\n",
            "Saving to: ‘subject_channels.zip’\n",
            "\n",
            "subject_channels.zi 100%[===================>]   9.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-09 07:22:03 (105 MB/s) - ‘subject_channels.zip’ saved [9238/9238]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#features\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\" -O features.zip && rm -rf /tmp/cookies.txt\n",
        "#original electrode names\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80\" -O subject_channels.zip && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# Data extraction\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/features.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/features\")\n",
        "zip_ref.close()\n",
        "\n",
        "# Electrode name extraction\n",
        "zip_ref = zipfile.ZipFile(\"/content/subject_channels.zip\", 'r')\n",
        "zip_ref.extractall(\"/content\")\n",
        "zip_ref.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Preparations and needed functions</h3>"
      ],
      "metadata": {
        "id": "QdxGeq3EP2hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our plan is to transform every feature vector into a larger dimensional feature vector, which contains all the different electrode names across all the subjects. In the original features, every feature vector consisted of 9 smaller feature vectors which corresponded to the transformed iEEG signals across 9 consecutive timesteps, so every feature vector had some sort of sequentiality within itself. So in order to use BiGRU and Convolutional models effectively, we need to be specific about the ordering of our features.\n"
      ],
      "metadata": {
        "id": "SCvHnABvbIco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Getting the original electrode names\n",
        "original_electrodes = set()\n",
        "\n",
        "for subject in ['01','02','03','04','05','06','07','08','09','10']:\n",
        "  table = pd.read_csv(f'/content/subject_channels/sub-{subject}_task-wordProduction_channels.tsv', sep='\\t')\n",
        "  elecs = set(table['name'])\n",
        "  original_electrodes = original_electrodes.union(elecs)\n",
        "\n",
        "\n",
        "# Now indexing them from -4 to 4 in order, so all features vectors will consist of 9 smaller feature vectors which will correspond to the\n",
        "# 9 consecutive timesteps in the larger dimensional vectors (every step will have 540 channels)\n",
        "all_electrodes = []\n",
        "for i in range(9):\n",
        "  for elec in original_electrodes:\n",
        "    all_electrodes.append(elec + \"T\" + str(i-4))\n",
        "\n",
        "\n",
        "print('Number of different features:', len(all_electrodes))\n",
        "\n",
        "#we will use this list's indexes to correspond to the feature matrices\n",
        "all_electrodes = list(all_electrodes) "
      ],
      "metadata": {
        "id": "annt8G29nDNS",
        "outputId": "79362312-238c-494f-c7c3-5e63ac029b93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of different features: 4860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for making every feature vector into a 4860 dimensional vector\n",
        "\n",
        "def dim_adjust(data, feature_names):\n",
        "  #create a new matrix with zeros, insert the values into the columns\n",
        "  #which correspond to the subject's feature names, then concatenate them\n",
        "\n",
        "  new_matrix = np.zeros((data.shape[0],len(all_electrodes)))\n",
        "  for column in range(data.shape[1]):\n",
        "    insert_index = all_electrodes.index(feature_names[column])\n",
        "    new_matrix[:,insert_index] = data[:,column]\n",
        "    \n",
        "  return new_matrix\n"
      ],
      "metadata": {
        "id": "d8o2edSn_CkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making the iterated test, validation and test sets"
      ],
      "metadata": {
        "id": "bwJu3UnzP8ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "feat_path = \"/content/features\"\n",
        "\n",
        "# Function to generate the train, val and test features and mel spectrograms\n",
        "def generate_features_and_spec():\n",
        "  #Base arrays\n",
        "  train_feat = np.empty((0, len(all_electrodes)))\n",
        "  val_feat = np.empty((0, len(all_electrodes)))\n",
        "  test_feat = np.empty((0, len(all_electrodes)))\n",
        "\n",
        "  train_spec = np.empty((0, 23))\n",
        "  val_spec = np.empty((0, 23))\n",
        "  test_spec = np.empty((0, 23))\n",
        "\n",
        "  for index, subject in enumerate(['01','02','03','04','05','06','07','08','09','10']):\n",
        "    #loading the features, feature names and mel spectrogram of the subject\n",
        "    data = np.load(os.path.join(feat_path,f'sub-{subject}_feat.npy'))\n",
        "    feature_names = np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy'))\n",
        "    spectrogram = np.load(os.path.join(feat_path,f'sub-{subject}_spec.npy'))\n",
        "\n",
        "    #splittig the features and the labels into 5 parts\n",
        "    feat_splits = np.array_split(data, 5)\n",
        "    spec_splits = np.array_split(spectrogram, 5)\n",
        "\n",
        "    #making the train, val or test arrays for the subject using a 60-20-20 ratio\n",
        "    #because of the \"index\" changing, the splitting position will iterate with each subject as well\n",
        "    subject_train_feat = np.vstack((feat_splits[index % 5],feat_splits[(index+1) % 5],feat_splits[(index+2) % 5]))\n",
        "    subject_train_spec = np.vstack((spec_splits[index % 5],spec_splits[(index+1) % 5],spec_splits[(index+2) % 5]))\n",
        "\n",
        "    subject_val_feat = feat_splits[(index+3) % 5]\n",
        "    subject_val_spec = spec_splits[(index+3) % 5]\n",
        "\n",
        "    subject_test_feat = feat_splits[(index+4) % 5]\n",
        "    subject_test_spec = spec_splits[(index+4) % 5]\n",
        "\n",
        "    #concatenating the dimensionality-adjusted features with the pre-existing feature set\n",
        "    train_feat = np.concatenate((train_feat, dim_adjust(subject_train_feat, feature_names)))\n",
        "    val_feat = np.concatenate((val_feat, dim_adjust(subject_val_feat, feature_names)))\n",
        "    test_feat = np.concatenate((test_feat, dim_adjust(subject_test_feat, feature_names)))\n",
        "\n",
        "    #concatenating the appropriate arrays with the pre-existing train, val or test labels\n",
        "    train_spec = np.concatenate((train_spec, subject_train_spec))\n",
        "    val_spec = np.concatenate((val_spec, subject_val_spec))\n",
        "    test_spec = np.concatenate((test_spec, subject_test_spec))\n",
        "\n",
        "  return train_feat, train_spec, val_feat, val_spec, test_feat, test_spec \n",
        "\n",
        "# Generating the data\n",
        "train_feat, train_spec, val_feat, val_spec, test_feat, test_spec = generate_features_and_spec()"
      ],
      "metadata": {
        "id": "3sBGVYAoc7LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Scaling the data \n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_feat)\n",
        "train_feat = scaler.transform(train_feat)\n",
        "val_feat = scaler.transform(val_feat)\n",
        "test_feat = scaler.transform(test_feat)"
      ],
      "metadata": {
        "id": "JC2LF6YOfflo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping the data for the sequential models\n",
        "\n",
        "train_feat = np.reshape(train_feat, (train_feat.shape[0], 9, int(train_feat.shape[1]/9)))\n",
        "val_feat = np.reshape(val_feat, (val_feat.shape[0], 9, int(val_feat.shape[1]/9)))\n",
        "test_feat = np.reshape(test_feat, (test_feat.shape[0], 9, int(test_feat.shape[1]/9)))"
      ],
      "metadata": {
        "id": "eHiMZ2J61NdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "hWaGwiqz1a8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Defining the data generators</h3>"
      ],
      "metadata": {
        "id": "MHh3ZQQc57ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence, set_random_seed\n",
        "\n",
        "set_random_seed(1234)\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    # Initialization\n",
        "    def __init__(self, data, spec, batch_size=32, dim=(9, 540), shuffle=True):\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.data = data\n",
        "        self.shuffle = shuffle\n",
        "        self.spec = spec\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Shows the number of batches per epoch\n",
        "        return int(np.floor(self.data.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate one batch of data\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        X_batch = np.empty((self.batch_size, self.dim[0], self.dim[1]))\n",
        "        y_batch = np.empty((self.batch_size, 23))\n",
        "        for index1, elem in enumerate(indexes):\n",
        "          X_batch[index1] = self.data[index1]\n",
        "          y_batch[index1] = self.spec[index1]\n",
        "\n",
        "        return X_batch, y_batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updating the index after each epoch\n",
        "        self.indexes = np.arange(self.data.shape[0])\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "# Creating the generators\n",
        "train_gen = DataGenerator(train_feat, train_spec, 256)\n",
        "val_gen = DataGenerator(val_feat, train_spec, 256)\n",
        "test_gen = DataGenerator(test_feat, train_spec, 256)"
      ],
      "metadata": {
        "id": "NDT_2IfWy49Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Tuning the GRU model</h3>"
      ],
      "metadata": {
        "id": "xQNXj3MWyB1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, Bidirectional, GRU, Flatten, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import keras_tuner as kt\n",
        "\n",
        "def create_bigru_optimal(hp):\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(Input(shape=(9, 540)))\n",
        "\n",
        "  # Recurrent layer 1\n",
        "  hp_units_1 = hp.Int('units_1', min_value=128, max_value=512, step=64)\n",
        "  hp_dropout_1 = hp.Float('dropout_1', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1)\n",
        "  model.add(Bidirectional(GRU(units=hp_units_1, return_sequences=True, dropout=hp_dropout_1)))\n",
        "\n",
        "  # Recurrent layer 2\n",
        "  hp_units_2 = hp.Int('units_2', min_value=64, max_value=256, step=32)\n",
        "  hp_dropout_2 = hp.Float('dropout_2', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1)\n",
        "  model.add(Bidirectional(GRU(units=hp_units_2, return_sequences=True, dropout=hp_dropout_2)))\n",
        "\n",
        "  # Recurrent layer 3\n",
        "  hp_units_3 = hp.Int('units_3', min_value=16, max_value=64, step=8)\n",
        "  hp_dropout_3 = hp.Float('dropout_3', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1)\n",
        "  model.add(Bidirectional(GRU(units=hp_units_3, return_sequences=True, dropout=hp_dropout_3)))\n",
        "\n",
        "  # Final layers\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(23, activation = 'linear'))\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss='mse',\n",
        "                metrics=['mse'])\n",
        "  \n",
        "  return model\n"
      ],
      "metadata": {
        "id": "xQ8xsb4w07uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the tuner for the BiGRU model\n",
        "tuner_bigru = kt.Hyperband(create_bigru_optimal,\n",
        "                     objective='val_loss',\n",
        "                     max_epochs=20,\n",
        "                     factor = 3,\n",
        "                     seed = 42,\n",
        "                     directory='/content/bigru_opt',\n",
        "                     project_name='bigru_opt1')"
      ],
      "metadata": {
        "id": "b-AzazaF4lxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Note: takes about 20 minutes to optimize\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=10)\n",
        "tuner_bigru.search(train_gen, epochs=100, validation_data=val_gen, verbose=0, shuffle=True, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps_bigru=tuner_bigru.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "waIjq-SD5NGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Tuning the Convolutional model</h3>"
      ],
      "metadata": {
        "id": "mGBC9Bfm7gb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_conv_optimal(hp):\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # 1st block\n",
        "  hp_filter_1 = hp.Int('filter_1', min_value=16, max_value=64, step=8)\n",
        "  hp_kernel_1 = hp.Choice('kernel_1', values=[3,4,5])\n",
        "  hp_activation_1 = hp.Choice('activation_1', values=['relu', 'leakyrelu', 'swish'])\n",
        "  model.add(Conv1D(filters=hp_filter_1, kernel_size=hp_kernel_1, activation=hp_activation_1, input_shape=(9, 540)))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "  # 2nd block\n",
        "  hp_filter_2 = hp.Int('filter_2', min_value=32, max_value=128, step=16)\n",
        "  hp_kernel_2 = hp.Choice('kernel_2', values=[3,4,5])\n",
        "  hp_activation_2 = hp.Choice('activation_1', values=['relu', 'leakyrelu', 'swish'])\n",
        "  model.add(Conv1D(filters=hp_filter_2, kernel_size=hp_kernel_2, activation=hp_activation_2))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units=23, activation='linear'))\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss='mse',\n",
        "                metrics=['mse'])\n",
        "  \n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "0Ljz6Ks379LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the tuner for the convolutional model\n",
        "tuner_conv = kt.Hyperband(create_conv_optimal,\n",
        "                     objective='val_loss',\n",
        "                     max_epochs=20,\n",
        "                     factor = 3,\n",
        "                     seed = 42,\n",
        "                     directory='/content/conv_opt',\n",
        "                     project_name='conv_opt1')"
      ],
      "metadata": {
        "id": "Gy22NmMM_Ztg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Note: takes about 20 minutes to optimize\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=10)\n",
        "tuner_conv.search(train_gen, epochs=100, validation_data=val_gen, verbose=0, shuffle=True, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps_conv=tuner_conv.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "YkSAjrdy_Ztn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}