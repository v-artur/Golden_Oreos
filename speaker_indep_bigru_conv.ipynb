{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOO3fpAR+/0EbmQ0U+QHJd8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v-artur/Golden_Oreos/blob/main/speaker_indep_bigru_conv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install fastdtw\n",
        "!pip install pysptk      \n",
        "!pip install pyworld"
      ],
      "metadata": {
        "id": "CXHLzfkF1We0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtaining the data"
      ],
      "metadata": {
        "id": "QRznZ-paPxRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "# IMPORTANT: In the case that wget throws a '429 error', please contact us, and we will grant you access to the needed files\n",
        "# Then by setting the 'drive' variable to true, you can obtain the files via our drive\n",
        "drive = False\n",
        "\n",
        "if drive == True:\n",
        "  drive.mount('/content/drive')\n",
        "  # Data extraction\n",
        "  \n",
        "  zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/DeepLearning/features.zip\", 'r')\n",
        "  zip_ref.extractall(\"/content/features\")\n",
        "  zip_ref.close()\n",
        "\n",
        "  # Electrode name extraction\n",
        "  zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/DeepLearning/subject_channels.zip\", 'r')\n",
        "  zip_ref.extractall(\"/content\")\n",
        "  zip_ref.close()\n",
        "\n",
        "  # Copying the modules into the base directory\n",
        "  shutil.copyfile(\"/content/drive/MyDrive/DeepLearning/MelFilterBank.py\", \"/content/MelFilterBank.py\")\n",
        "  shutil.copyfile(\"/content/drive/MyDrive/DeepLearning/reconstructWave.py\", \"/content/reconstructWave.py\")\n"
      ],
      "metadata": {
        "id": "sVGMjkjagaKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "b3F36vS9-11r",
        "outputId": "61583378-e261-4f54-ffdb-c745de09dd07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-10 08:26:05--  https://docs.google.com/uc?export=download&confirm=&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.219.101, 172.217.219.138, 172.217.219.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.219.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6tht3df23fglhscdumjj79bbldbcprpf/1670660700000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=64830157-7344-494e-871c-1f9758982ac2 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-10 08:26:12--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6tht3df23fglhscdumjj79bbldbcprpf/1670660700000/17895932938140350971/*/1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp?e=download&uuid=64830157-7344-494e-871c-1f9758982ac2\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 172.217.212.132, 2607:f8b0:4001:c03::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|172.217.212.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 429 Too Many Requests\n",
            "2022-12-10 08:26:12 ERROR 429: Too Many Requests.\n",
            "\n",
            "--2022-12-10 08:26:13--  https://docs.google.com/uc?export=download&confirm=&id=1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.219.101, 172.217.219.138, 172.217.219.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.219.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9i4e0p287e8vsmv624eh8vot3760eq8o/1670660700000/17895932938140350971/*/1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80?e=download&uuid=28ad5fea-21cc-4ba9-9eb6-25b0114f7aaf [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-10 08:26:13--  https://doc-08-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9i4e0p287e8vsmv624eh8vot3760eq8o/1670660700000/17895932938140350971/*/1A2CMLYAMOjET7Bdwt8bjRt8YLQeoVP80?e=download&uuid=28ad5fea-21cc-4ba9-9eb6-25b0114f7aaf\n",
            "Resolving doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)... 172.217.212.132, 2607:f8b0:4001:c03::84\n",
            "Connecting to doc-08-9o-docs.googleusercontent.com (doc-08-9o-docs.googleusercontent.com)|172.217.212.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9238 (9.0K) [application/x-zip-compressed]\n",
            "Saving to: ‘subject_channels.zip’\n",
            "\n",
            "subject_channels.zi 100%[===================>]   9.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-10 08:26:14 (47.6 MB/s) - ‘subject_channels.zip’ saved [9238/9238]\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadZipFile",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5898ac92f193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Data extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mzip_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/features.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ],
      "source": [
        "#features\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1vtZchVzl424pSQBXQ8EBxvVOzcEQPKIp\" -O features.zip && rm -rf /tmp/cookies.txt\n",
        "#original electrode names\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1hBr7KNrNNl9udDB8OQ0nKt2LD0WfxJ7K' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1hBr7KNrNNl9udDB8OQ0nKt2LD0WfxJ7K\" -O subject_channels.zip && rm -rf /tmp/cookies.txt\n",
        "#reconstruction module\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1_eeG0d_r-RqazUkr-ZRPNC6L13sHYwIP' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1_eeG0d_r-RqazUkr-ZRPNC6L13sHYwIP\" -O reconstructWave.py && rm -rf /tmp/cookies.txt\n",
        "#Melfiltebank applier\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Bjf3ncRe8CcWHl3i0HxRo4unRYkz2fog' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Bjf3ncRe8CcWHl3i0HxRo4unRYkz2fog\" -O MelFilterBank.py && rm -rf /tmp/cookies.txt\n",
        "\n",
        "\n",
        "# Data extraction\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/features.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/features\")\n",
        "zip_ref.close()\n",
        "\n",
        "# Electrode name extraction\n",
        "zip_ref = zipfile.ZipFile(\"/content/subject_channels.zip\", 'r')\n",
        "zip_ref.extractall(\"/content\")\n",
        "zip_ref.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Preparations and needed functions</h3>"
      ],
      "metadata": {
        "id": "QdxGeq3EP2hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our plan is to transform every feature vector into a larger dimensional feature vector, which contains all the different electrode names across all the subjects. In the original features, every feature vector consisted of 9 smaller feature vectors which corresponded to the transformed iEEG signals across 9 consecutive timesteps, so every feature vector had some sort of sequentiality within itself. So in order to use BiGRU and Convolutional models effectively, we need to be specific about the ordering of our features.\n"
      ],
      "metadata": {
        "id": "SCvHnABvbIco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Getting the original electrode names\n",
        "original_electrodes = set()\n",
        "\n",
        "for subject in ['01','02','03','04','05','06','07','08','09','10']:\n",
        "  table = pd.read_csv(f'/content/subject_channels/sub-{subject}_task-wordProduction_channels.tsv', sep='\\t')\n",
        "  elecs = set(table['name'])\n",
        "  original_electrodes = original_electrodes.union(elecs)\n",
        "\n",
        "\n",
        "# Now indexing them from -4 to 4 in order, so all features vectors will consist of 9 smaller feature vectors which will correspond to the\n",
        "# 9 consecutive timesteps in the larger dimensional vectors (every step will have 540 channels)\n",
        "all_electrodes = []\n",
        "for i in range(9):\n",
        "  for elec in original_electrodes:\n",
        "    all_electrodes.append(elec + \"T\" + str(i-4))\n",
        "\n",
        "\n",
        "print('Number of different features:', len(all_electrodes))\n",
        "\n",
        "#we will use this list's indexes to correspond to the feature matrices\n",
        "all_electrodes = list(all_electrodes) "
      ],
      "metadata": {
        "id": "annt8G29nDNS",
        "outputId": "29830fba-8529-4ea2-f4ac-4fd8c3024c5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of different features: 4860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for making every feature vector into a 4860 dimensional vector\n",
        "\n",
        "def dim_adjust(data, feature_names):\n",
        "  #create a new matrix with zeros, insert the values into the columns\n",
        "  #which correspond to the subject's feature names, then concatenate them\n",
        "\n",
        "  new_matrix = np.zeros((data.shape[0],len(all_electrodes)))\n",
        "  for column in range(data.shape[1]):\n",
        "    insert_index = all_electrodes.index(feature_names[column])\n",
        "    new_matrix[:,insert_index] = data[:,column]\n",
        "    \n",
        "  return new_matrix\n"
      ],
      "metadata": {
        "id": "d8o2edSn_CkY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making the iterated test, validation and test sets"
      ],
      "metadata": {
        "id": "bwJu3UnzP8ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "feat_path = \"/content/features\"\n",
        "\n",
        "# Function to generate the train, val and test features and mel spectrograms\n",
        "def generate_features_and_spec():\n",
        "  #Base arrays\n",
        "  train_feat = np.empty((0, len(all_electrodes)))\n",
        "  val_feat = np.empty((0, len(all_electrodes)))\n",
        "  test_feat = np.empty((0, len(all_electrodes)))\n",
        "\n",
        "  train_spec = np.empty((0, 23))\n",
        "  val_spec = np.empty((0, 23))\n",
        "  test_spec = np.empty((0, 23))\n",
        "\n",
        "  for index, subject in enumerate(['01','02','03','04','05','06','07','08','09','10']):\n",
        "    #loading the features, feature names and mel spectrogram of the subject\n",
        "    data = np.load(os.path.join(feat_path,f'sub-{subject}_feat.npy'))\n",
        "    feature_names = np.load(os.path.join(feat_path,f'sub-{subject}_feat_names.npy'))\n",
        "    spectrogram = np.load(os.path.join(feat_path,f'sub-{subject}_spec.npy'))\n",
        "\n",
        "    #splittig the features and the labels into 5 parts\n",
        "    feat_splits = np.array_split(data, 5)\n",
        "    spec_splits = np.array_split(spectrogram, 5)\n",
        "\n",
        "    #making the train, val or test arrays for the subject using a 60-20-20 ratio\n",
        "    #because of the \"index\" changing, the splitting position will iterate with each subject as well\n",
        "    subject_train_feat = np.vstack((feat_splits[index % 5],feat_splits[(index+1) % 5],feat_splits[(index+2) % 5]))\n",
        "    subject_train_spec = np.vstack((spec_splits[index % 5],spec_splits[(index+1) % 5],spec_splits[(index+2) % 5]))\n",
        "\n",
        "    subject_val_feat = feat_splits[(index+3) % 5]\n",
        "    subject_val_spec = spec_splits[(index+3) % 5]\n",
        "\n",
        "    subject_test_feat = feat_splits[(index+4) % 5]\n",
        "    subject_test_spec = spec_splits[(index+4) % 5]\n",
        "\n",
        "    #concatenating the dimensionality-adjusted features with the pre-existing feature set\n",
        "    train_feat = np.concatenate((train_feat, dim_adjust(subject_train_feat, feature_names)))\n",
        "    val_feat = np.concatenate((val_feat, dim_adjust(subject_val_feat, feature_names)))\n",
        "    test_feat = np.concatenate((test_feat, dim_adjust(subject_test_feat, feature_names)))\n",
        "\n",
        "    #concatenating the appropriate arrays with the pre-existing train, val or test labels\n",
        "    train_spec = np.concatenate((train_spec, subject_train_spec))\n",
        "    val_spec = np.concatenate((val_spec, subject_val_spec))\n",
        "    test_spec = np.concatenate((test_spec, subject_test_spec))\n",
        "\n",
        "  return train_feat, train_spec, val_feat, val_spec, test_feat, test_spec \n",
        "\n",
        "# Generating the data\n",
        "train_feat, train_spec, val_feat, val_spec, test_feat, test_spec = generate_features_and_spec()"
      ],
      "metadata": {
        "id": "3sBGVYAoc7LA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Scaling the data \n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_feat)\n",
        "train_feat = scaler.transform(train_feat)\n",
        "val_feat = scaler.transform(val_feat)\n",
        "test_feat = scaler.transform(test_feat)"
      ],
      "metadata": {
        "id": "JC2LF6YOfflo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping the data for the sequential models\n",
        "\n",
        "train_feat = np.reshape(train_feat, (train_feat.shape[0], 9, int(train_feat.shape[1]/9)))\n",
        "val_feat = np.reshape(val_feat, (val_feat.shape[0], 9, int(val_feat.shape[1]/9)))\n",
        "test_feat = np.reshape(test_feat, (test_feat.shape[0], 9, int(test_feat.shape[1]/9)))"
      ],
      "metadata": {
        "id": "eHiMZ2J61NdC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning the models"
      ],
      "metadata": {
        "id": "hWaGwiqz1a8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Defining the data generators</h3>"
      ],
      "metadata": {
        "id": "MHh3ZQQc57ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence, set_random_seed\n",
        "\n",
        "set_random_seed(1234)\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    # Initialization\n",
        "    def __init__(self, data, spec, batch_size=32, dim=(9, 540), shuffle=True):\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.data = data\n",
        "        self.shuffle = shuffle\n",
        "        self.spec = spec\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Shows the number of batches per epoch\n",
        "        return int(np.floor(self.data.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate one batch of data\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        X_batch = np.empty((self.batch_size, self.dim[0], self.dim[1]))\n",
        "        y_batch = np.empty((self.batch_size, 23))\n",
        "        for index1, elem in enumerate(indexes):\n",
        "          X_batch[index1] = self.data[index1]\n",
        "          y_batch[index1] = self.spec[index1]\n",
        "\n",
        "        return X_batch, y_batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updating the index after each epoch\n",
        "        self.indexes = np.arange(self.data.shape[0])\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "# Creating the generators\n",
        "train_gen = DataGenerator(train_feat, train_spec, 256)\n",
        "val_gen = DataGenerator(val_feat, train_spec, 256)"
      ],
      "metadata": {
        "id": "NDT_2IfWy49Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Tuning the BiGRU model</h3>"
      ],
      "metadata": {
        "id": "xQNXj3MWyB1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "mDM2dsRCMt9H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, Bidirectional, GRU, Flatten, Conv1D, MaxPooling1D, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import keras_tuner as kt\n",
        "\n",
        "\n",
        "def create_bigru_optimal(hp):\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(Input(shape=(9, 540)))\n",
        "\n",
        "  # Recurrent layer 1\n",
        "  hp_units_1 = hp.Int('units_1', min_value=128, max_value=512, step=64)\n",
        "  hp_dropout_1 = hp.Float('dropout_1', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1)\n",
        "  model.add(Bidirectional(GRU(units=hp_units_1, return_sequences=True, dropout=hp_dropout_1)))\n",
        "\n",
        "  # Recurrent layer 2\n",
        "  hp_units_2 = hp.Int('units_2', min_value=64, max_value=256, step=32)\n",
        "  hp_dropout_2 = hp.Float('dropout_2', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1)\n",
        "  model.add(Bidirectional(GRU(units=hp_units_2, return_sequences=True, dropout=hp_dropout_2)))\n",
        "\n",
        "  # Recurrent layer 3\n",
        "  hp_units_3 = hp.Int('units_3', min_value=16, max_value=64, step=8)\n",
        "  hp_dropout_3 = hp.Float('dropout_3', min_value = 0.0, max_value = 0.5, default = 0.2, step = 0.1)\n",
        "  model.add(Bidirectional(GRU(units=hp_units_3, return_sequences=True, dropout=hp_dropout_3)))\n",
        "\n",
        "  # Final layers\n",
        "  model.add(Flatten())\n",
        "  #hp_dense_units = hp.Int('units_dense', min_value=128, max_value=512, step=64)\n",
        "  #hp_activation = hp.Choice('activation', values=['relu', 'swish'], kernel_initializer='HeNormal')\n",
        "\n",
        "  #model.add(Dense(hp_dense_units, activation = hp_activation))\n",
        "  model.add(Dense(23, activation = 'linear'))\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss='mse',\n",
        "                metrics=['mse'])\n",
        "  \n",
        "  return model\n"
      ],
      "metadata": {
        "id": "xQ8xsb4w07uo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the tuner for the BiGRU model\n",
        "tuner_bigru = kt.Hyperband(create_bigru_optimal,\n",
        "                     objective='val_loss',\n",
        "                     max_epochs=10,\n",
        "                     factor = 2,\n",
        "                     seed = 42,\n",
        "                     directory='/content/bigru_opt',\n",
        "                     project_name='bigru_opt1')"
      ],
      "metadata": {
        "id": "b-AzazaF4lxE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Note: takes about 20 minutes to optimize\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
        "tuner_bigru.search(train_gen, epochs=100, validation_data=val_gen, verbose=0, shuffle=True, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps_bigru=tuner_bigru.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "waIjq-SD5NGR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Tuning the Convolutional model</h3>"
      ],
      "metadata": {
        "id": "mGBC9Bfm7gb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_conv_optimal(hp):\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # 1st conv\n",
        "  hp_filter_1 = hp.Int('filter_1', min_value=16, max_value=64, step=8)\n",
        "  hp_kernel_1 = hp.Choice('kernel_1', values=[3,4,5])\n",
        "  hp_activation_1 = hp.Choice('activation_1', values=['relu', 'swish'])\n",
        "  model.add(Conv1D(filters=hp_filter_1, kernel_size=hp_kernel_1, activation=hp_activation_1, kernel_initializer='HeNormal', input_shape=(9, 540)))\n",
        "\n",
        "  # 2nd conv\n",
        "  hp_filter_2 = hp.Int('filter_2', min_value=32, max_value=128, step=16)\n",
        "  hp_kernel_2 = hp.Choice('kernel_2', values=[3,4,5])\n",
        "  hp_activation_2 = hp.Choice('activation_1', values=['relu', 'swish'])\n",
        "  model.add(Conv1D(filters=hp_filter_2, kernel_size=hp_kernel_2, activation=hp_activation_2, kernel_initializer='HeNormal'))\n",
        "\n",
        "  # Last layer\n",
        "  model.add(Flatten())\n",
        "  hp_dense_units = hp.Int('units_dense', min_value=32, max_value=64, step=8)\n",
        "  hp_activation = hp.Choice('activation', values=['relu', 'swish'])\n",
        "  \n",
        "  model.add(Dense(hp_dense_units, activation = hp_activation, kernel_initializer='HeNormal'))\n",
        "  model.add(Dense(units=23, activation='linear'))\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss='mse',\n",
        "                metrics=['mse'])\n",
        "  \n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "0Ljz6Ks379LA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the tuner for the convolutional model\n",
        "tuner_conv = kt.Hyperband(create_conv_optimal,\n",
        "                     objective='val_loss',\n",
        "                     max_epochs = 10,\n",
        "                     factor = 2,\n",
        "                     seed = 42,\n",
        "                     directory='/content/conv_opt',\n",
        "                     project_name='conv_opt1')"
      ],
      "metadata": {
        "id": "Gy22NmMM_Ztg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Note: takes about 20 minutes to optimize\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
        "tuner_conv.search(train_gen, epochs=100, validation_data=val_gen, verbose=0, shuffle=True, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps_conv=tuner_conv.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "YkSAjrdy_Ztn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the tuned models"
      ],
      "metadata": {
        "id": "1S1BLR0NbfKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>BiGRU</h3>"
      ],
      "metadata": {
        "id": "d7kU6fLTbjVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting and retraining the model with the best params\n",
        "model_bigru = tuner_bigru.hypermodel.build(best_hps_bigru)\n",
        "\n",
        "early_stopping=EarlyStopping(patience=5, verbose=1, min_delta=1e-5)\n",
        "checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "model_bigru.fit(train_gen, epochs=100, verbose=1, validation_data=val_gen, callbacks=[checkpointer, early_stopping])"
      ],
      "metadata": {
        "id": "JsSMs5h_biZ1",
        "outputId": "d85d961d-8dbb-4775-af60-e77df567b894",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "698/699 [============================>.] - ETA: 0s - loss: 3.7658 - mse: 3.7658\n",
            "Epoch 1: val_loss improved from inf to 2.67326, saving model to weights1.hdf5\n",
            "699/699 [==============================] - 24s 24ms/step - loss: 3.7616 - mse: 3.7616 - val_loss: 2.6733 - val_mse: 2.6733\n",
            "Epoch 2/100\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.4712 - mse: 0.4712\n",
            "Epoch 2: val_loss did not improve from 2.67326\n",
            "699/699 [==============================] - 15s 21ms/step - loss: 0.4707 - mse: 0.4707 - val_loss: 2.8344 - val_mse: 2.8344\n",
            "Epoch 3/100\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.1950 - mse: 0.1950\n",
            "Epoch 3: val_loss did not improve from 2.67326\n",
            "699/699 [==============================] - 14s 21ms/step - loss: 0.1949 - mse: 0.1949 - val_loss: 2.8197 - val_mse: 2.8197\n",
            "Epoch 4/100\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.1166 - mse: 0.1166\n",
            "Epoch 4: val_loss did not improve from 2.67326\n",
            "699/699 [==============================] - 14s 21ms/step - loss: 0.1166 - mse: 0.1166 - val_loss: 2.9563 - val_mse: 2.9563\n",
            "Epoch 5/100\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.0845 - mse: 0.0845\n",
            "Epoch 5: val_loss did not improve from 2.67326\n",
            "699/699 [==============================] - 15s 21ms/step - loss: 0.0845 - mse: 0.0845 - val_loss: 3.1688 - val_mse: 3.1688\n",
            "Epoch 6/100\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.0660 - mse: 0.0660\n",
            "Epoch 6: val_loss did not improve from 2.67326\n",
            "699/699 [==============================] - 15s 21ms/step - loss: 0.0660 - mse: 0.0660 - val_loss: 3.1461 - val_mse: 3.1461\n",
            "Epoch 6: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9c16689c70>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on the test set\n",
        "test_gen = DataGenerator(test_feat, train_spec, 7459)\n",
        "pred_bigru = model_bigru.predict(test_gen)"
      ],
      "metadata": {
        "id": "gZnARq5ZdnhE",
        "outputId": "128b458e-2e77-458f-ee3f-718cf08d4315",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 3s 127ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Convolutional network</h2>"
      ],
      "metadata": {
        "id": "jnUsf5UHd8Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting and retraining the model with the best params\n",
        "model_conv = tuner_conv.hypermodel.build(best_hps_conv)\n",
        "\n",
        "early_stopping=EarlyStopping(patience=5, verbose=1, min_delta=1e-5)\n",
        "checkpointer=ModelCheckpoint(filepath='weights1.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "model_conv.fit(train_gen, epochs=100, verbose=1, validation_data=val_gen, callbacks=[checkpointer, early_stopping])"
      ],
      "metadata": {
        "id": "29b1CT_5rGoo",
        "outputId": "b3ee9209-b614-42f8-ac07-bf8047b14426",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "693/699 [============================>.] - ETA: 0s - loss: 1.8280 - mse: 1.8280\n",
            "Epoch 1: val_loss improved from inf to 5.15667, saving model to weights1.hdf5\n",
            "699/699 [==============================] - 6s 8ms/step - loss: 1.8149 - mse: 1.8149 - val_loss: 5.1567 - val_mse: 5.1567\n",
            "Epoch 2/100\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.2272 - mse: 0.2272\n",
            "Epoch 2: val_loss improved from 5.15667 to 3.88152, saving model to weights1.hdf5\n",
            "699/699 [==============================] - 5s 8ms/step - loss: 0.2270 - mse: 0.2270 - val_loss: 3.8815 - val_mse: 3.8815\n",
            "Epoch 3/100\n",
            "691/699 [============================>.] - ETA: 0s - loss: 0.1347 - mse: 0.1347\n",
            "Epoch 3: val_loss did not improve from 3.88152\n",
            "699/699 [==============================] - 5s 8ms/step - loss: 0.1343 - mse: 0.1343 - val_loss: 4.1032 - val_mse: 4.1032\n",
            "Epoch 4/100\n",
            "697/699 [============================>.] - ETA: 0s - loss: 0.0976 - mse: 0.0976\n",
            "Epoch 4: val_loss did not improve from 3.88152\n",
            "699/699 [==============================] - 6s 8ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 4.1879 - val_mse: 4.1879\n",
            "Epoch 5/100\n",
            "692/699 [============================>.] - ETA: 0s - loss: 0.0719 - mse: 0.0719\n",
            "Epoch 5: val_loss improved from 3.88152 to 3.69421, saving model to weights1.hdf5\n",
            "699/699 [==============================] - 6s 8ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 3.6942 - val_mse: 3.6942\n",
            "Epoch 6/100\n",
            "699/699 [==============================] - ETA: 0s - loss: 0.0574 - mse: 0.0574\n",
            "Epoch 6: val_loss improved from 3.69421 to 3.40086, saving model to weights1.hdf5\n",
            "699/699 [==============================] - 6s 8ms/step - loss: 0.0574 - mse: 0.0574 - val_loss: 3.4009 - val_mse: 3.4009\n",
            "Epoch 7/100\n",
            "696/699 [============================>.] - ETA: 0s - loss: 0.0500 - mse: 0.0500\n",
            "Epoch 7: val_loss did not improve from 3.40086\n",
            "699/699 [==============================] - 5s 8ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 3.4584 - val_mse: 3.4584\n",
            "Epoch 8/100\n",
            "692/699 [============================>.] - ETA: 0s - loss: 0.0416 - mse: 0.0416\n",
            "Epoch 8: val_loss did not improve from 3.40086\n",
            "699/699 [==============================] - 5s 8ms/step - loss: 0.0416 - mse: 0.0416 - val_loss: 3.4738 - val_mse: 3.4738\n",
            "Epoch 9/100\n",
            "699/699 [==============================] - ETA: 0s - loss: 2.5189 - mse: 2.5189\n",
            "Epoch 9: val_loss improved from 3.40086 to 3.05280, saving model to weights1.hdf5\n",
            "699/699 [==============================] - 5s 8ms/step - loss: 2.5189 - mse: 2.5189 - val_loss: 3.0528 - val_mse: 3.0528\n",
            "Epoch 10/100\n",
            "697/699 [============================>.] - ETA: 0s - loss: 3.0528 - mse: 3.0528\n",
            "Epoch 10: val_loss did not improve from 3.05280\n",
            "699/699 [==============================] - 5s 8ms/step - loss: 3.0528 - mse: 3.0528 - val_loss: 3.0528 - val_mse: 3.0528\n",
            "Epoch 11/100\n",
            "694/699 [============================>.] - ETA: 0s - loss: 3.0531 - mse: 3.0531\n",
            "Epoch 11: val_loss did not improve from 3.05280\n",
            "699/699 [==============================] - 5s 8ms/step - loss: 3.0531 - mse: 3.0531 - val_loss: 3.0528 - val_mse: 3.0528\n",
            "Epoch 12/100\n",
            "698/699 [============================>.] - ETA: 0s - loss: 3.0530 - mse: 3.0530\n",
            "Epoch 12: val_loss did not improve from 3.05280\n",
            "699/699 [==============================] - 5s 8ms/step - loss: 3.0530 - mse: 3.0530 - val_loss: 3.0528 - val_mse: 3.0528\n",
            "Epoch 13/100\n",
            "692/699 [============================>.] - ETA: 0s - loss: 3.0531 - mse: 3.0531\n",
            "Epoch 13: val_loss did not improve from 3.05280\n",
            "699/699 [==============================] - 7s 10ms/step - loss: 3.0531 - mse: 3.0531 - val_loss: 3.0528 - val_mse: 3.0528\n",
            "Epoch 14/100\n",
            "699/699 [==============================] - ETA: 0s - loss: 3.0531 - mse: 3.0531\n",
            "Epoch 14: val_loss did not improve from 3.05280\n",
            "699/699 [==============================] - 6s 8ms/step - loss: 3.0531 - mse: 3.0531 - val_loss: 3.0528 - val_mse: 3.0528\n",
            "Epoch 15/100\n",
            "696/699 [============================>.] - ETA: 0s - loss: 3.0531 - mse: 3.0531\n",
            "Epoch 15: val_loss did not improve from 3.05280\n",
            "699/699 [==============================] - 6s 8ms/step - loss: 3.0531 - mse: 3.0531 - val_loss: 3.0528 - val_mse: 3.0528\n",
            "Epoch 16/100\n",
            "693/699 [============================>.] - ETA: 0s - loss: 3.0531 - mse: 3.0531\n",
            "Epoch 16: val_loss did not improve from 3.05280\n",
            "699/699 [==============================] - 5s 8ms/step - loss: 3.0531 - mse: 3.0531 - val_loss: 3.0528 - val_mse: 3.0528\n",
            "Epoch 17/100\n",
            "696/699 [============================>.] - ETA: 0s - loss: 3.0531 - mse: 3.0531\n",
            "Epoch 17: val_loss did not improve from 3.05280\n",
            "699/699 [==============================] - 5s 7ms/step - loss: 3.0531 - mse: 3.0531 - val_loss: 3.0528 - val_mse: 3.0528\n",
            "Epoch 18/100\n",
            "694/699 [============================>.] - ETA: 0s - loss: 3.0531 - mse: 3.0531\n",
            "Epoch 18: val_loss did not improve from 3.05280\n",
            "699/699 [==============================] - 5s 8ms/step - loss: 3.0531 - mse: 3.0531 - val_loss: 3.0528 - val_mse: 3.0528\n",
            "Epoch 19/100\n",
            "698/699 [============================>.] - ETA: 0s - loss: 3.0531 - mse: 3.0531\n",
            "Epoch 19: val_loss did not improve from 3.05280\n",
            "699/699 [==============================] - 5s 8ms/step - loss: 3.0531 - mse: 3.0531 - val_loss: 3.0528 - val_mse: 3.0528\n",
            "Epoch 19: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9bfa3f1af0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on the test set\n",
        "test_gen = DataGenerator(test_feat, train_spec, 7459)\n",
        "pred_conv = model_conv.predict(test_gen)"
      ],
      "metadata": {
        "id": "-mheXyMZznOb",
        "outputId": "664466ff-1ec2-4471-bb89-584b8b4e458e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 138ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the models"
      ],
      "metadata": {
        "id": "Fow35RuyzzFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Functions for evaluating the models<h3>"
      ],
      "metadata": {
        "id": "ItvgBLC42mnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For audio reconstruction and MCD measure \n",
        "#dependencies\n",
        "import reconstructWave as rW\n",
        "import MelFilterBank as mel\n",
        "import librosa\n",
        "from scipy.io import wavfile\n",
        "import pysptk\n",
        "from scipy.spatial.distance import euclidean\n",
        "from fastdtw import fastdtw\n",
        "\n",
        "# functions for MCD calculations \n",
        "# (source: https://github.com/ttslr/python-MCD?fbclid=IwAR2OFaz3-8kTfhJXC7F-cmTTHkY-egEzZdSYHsC0agwPw58N2G3hqhfdVNY)\n",
        "\n",
        "\n",
        "#The following function converts a wav file into a format which can be used to evaluate the MCD score\n",
        "def readmgc(filename):\n",
        "    sr, x = wavfile.read(filename)\n",
        "    assert sr == 16000\n",
        "    x = x.astype(np.float64)\n",
        "    frame_length = 1024\n",
        "    hop_length = 256  \n",
        "    frames = librosa.util.frame(x, frame_length=frame_length, hop_length=hop_length).astype(np.float64).T\n",
        "    frames *= pysptk.blackman(frame_length)\n",
        "    assert frames.shape[1] == frame_length \n",
        "    order = 25\n",
        "    alpha = 0.41\n",
        "    stage = 5\n",
        "    gamma = -1.0 / stage\n",
        "\n",
        "    mgc = pysptk.mgcep(frames, order, alpha, gamma)\n",
        "    mgc = mgc.reshape(-1, order + 1)\n",
        "    print(\"mgc of {} is ok!\".format(filename))\n",
        "    return mgc\n",
        "\n",
        "\n",
        "# variable for the compute_mcd function \n",
        "natural_folder = '/content/features/'\n",
        "\n",
        "\n",
        "#this function returns the average MCD score for the reconstructed WAV file by a chosen model\n",
        "def compute_mcd(synth_folder):\n",
        "  '''\n",
        "  :param synth_folder: the folder in which the reconstructed WAV files can be found\n",
        "  '''\n",
        "\n",
        "  # computing the MCD\n",
        "  files = os.listdir(synth_folder)\n",
        "  for subject in files:\n",
        "    \n",
        "    # this is necessary because of the .ipynb_checkpoints files\n",
        "    if subject.startswith('.'):\n",
        "      continue\n",
        "\n",
        "    print(\"Processing -----------{}\".format(subject))\n",
        "\n",
        "    #computational parameters\n",
        "    _logdb_const = 10.0 / np.log(10.0) * np.sqrt(2.0)\n",
        "    s = 0.0\n",
        "    framesTot = 0\n",
        "    \n",
        "\n",
        "    # obtaining the mgc features\n",
        "    subject_ID = subject[0:6]\n",
        "    filename1 = natural_folder + subject_ID + '_orig_audio.wav'\n",
        "    mgc1 = readmgc(filename1)\n",
        "    filename2 = synth_folder + subject_ID + '_predicted.wav'\n",
        "    mgc2 = readmgc(filename2)\n",
        "  \n",
        "    x = mgc1\n",
        "    y = mgc2\n",
        "    \n",
        "    # calculating the mcd\n",
        "    distance, path = fastdtw(x, y, dist=euclidean)\n",
        "  \n",
        "    distance/= (len(x) + len(y))\n",
        "    pathx = list(map(lambda l: l[0], path))\n",
        "    pathy = list(map(lambda l: l[1], path))\n",
        "    x, y = x[pathx], y[pathy]\n",
        "\n",
        "    frames = x.shape[0]\n",
        "    framesTot  += frames\n",
        "\n",
        "    z = x - y\n",
        "    s += np.sqrt((z * z).sum(-1)).sum()\n",
        "\n",
        "    MCD_value = _logdb_const * float(s) / float(framesTot)\n",
        "\n",
        "    print(f\"MCD of {subject_ID}: {MCD_value}\")\n",
        "\n",
        "\n",
        "#Reconstructing the WAV file from the predicted mel-log spectrogram (Griffin-Lim method)\n",
        "def createAudio(spectrogram, audiosr=16000, winLength=0.05, frameshift=0.01):\n",
        "    mfb = mel.MelFilterBank(int((audiosr*winLength)/2+1), spectrogram.shape[1], audiosr)\n",
        "    hop = int(spectrogram.shape[0])\n",
        "    rec_audio = np.array([])\n",
        "    for_reconstruction = mfb.fromLogMels(spectrogram)\n",
        "    for w in range(0,spectrogram.shape[0],hop):\n",
        "        spec = for_reconstruction[w:min(w+hop,for_reconstruction.shape[0]),:]\n",
        "        rec = rW.reconstructWavFromSpectrogram(spec,spec.shape[0]*spec.shape[1],fftsize=int(audiosr*winLength),overlap=int(winLength/frameshift))\n",
        "        rec_audio = np.append(rec_audio,rec)\n",
        "    scaled = np.int16(rec_audio/np.max(np.abs(rec_audio)) * 32767)\n",
        "    return scaled\n"
      ],
      "metadata": {
        "id": "F4U4FzDK1CbJ",
        "outputId": "e9ebcac9-e9ae-4679-aa35-d9ce4e70ff3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7b04eacb8739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#For audio reconstruction and MCD measure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mreconstructWave\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMelFilterBank\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'reconstructWave'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Pearson correlation"
      ],
      "metadata": {
        "id": "6YWoXAYub36k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pearson correlation\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# BiGRU\n",
        "corrs_1 = []\n",
        "for col in range(23):\n",
        "  r, p = pearsonr(test_spec[:,col], pred_bigru[:,col])\n",
        "  corrs_1.append(r)\n",
        "print('For the BiGRU Network, the mean Pearson correlation is:', np.mean(corrs_1))\n",
        "\n",
        "# Conv\n",
        "corrs_2 = []\n",
        "for col in range(23):\n",
        "  r, p = pearsonr(test_spec[:,col], pred_conv[:,col])\n",
        "  corrs_2.append(r)\n",
        "print('For the Convolutional Network, the mean Pearson correlation is:', np.mean(corrs_2))"
      ],
      "metadata": {
        "id": "gGzSi-MC1dPk",
        "outputId": "d18cdbd9-4c80-4cfc-9a8e-f8b19f9387ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the BiGRU Network, the mean Pearson correlation is: 0.016036042714715232\n",
            "For the Convolutional Network, the mean Pearson correlation is: -0.019299341227333983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSE"
      ],
      "metadata": {
        "id": "rX3KbKuGb7lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "print('For the BiGRU Network, the RMSE is:', (mse(test_spec, pred_bigru))**0.5)\n",
        "print('For the Conv Network, the RMSE is:', (mse(test_spec, pred_conv))**0.5)"
      ],
      "metadata": {
        "id": "8vgYplW84BA0",
        "outputId": "840ec274-d375-4b19-a6bb-11c5bd6d5897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the BiGRU Network, the RMSE is: 4.118048502397663\n",
            "For the Conv Network, the RMSE is: 79.00616712942556\n"
          ]
        }
      ]
    }
  ]
}